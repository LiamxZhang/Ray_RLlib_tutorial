{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f793e10",
   "metadata": {},
   "source": [
    "# AlgorithmConfig API\n",
    "首先创建 AlgorithmConfig 的一个实例，然后调用其某些方法来设置各种配置选项。RLlib 在其所有代码部分使用以下符合 black 标准的格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63a5f4d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ray.rllib.algorithms.algorithm_config.AlgorithmConfig at 0x7665c42fe3b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray.rllib.algorithms.algorithm_config import AlgorithmConfig\n",
    "\n",
    "config = (\n",
    "    # Create an `AlgorithmConfig` instance.\n",
    "    AlgorithmConfig()\n",
    "    # Change the learning rate.\n",
    "    .training(lr=0.0005)\n",
    "    # Change the number of Learner actors.\n",
    "    .learners(num_learners=2)\n",
    ")\n",
    "config.environment(env=\"CartPole-v1\")  # call the proper method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0971db",
   "metadata": {},
   "source": [
    "## 算法特定的配置类\n",
    "实践中不会直接使用基础 AlgorithmConfig 类，而总是使用其算法特定的子类，例如 PPOConfig。每个子类都有自己的一组 additional arguments 用于 training() 方法。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fbf25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06 17:06:47,400\tWARNING algorithm_config.py:5033 -- You are running IMPALA on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:520: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2025-08-06 17:06:48,839\tINFO worker.py:1927 -- Started a local Ray instance.\n",
      "[2025-08-06 17:06:50,660 E 3945021 3945021] core_worker.cc:2740: Actor with class name: 'SingleAgentEnvRunner' and ID: 'd5bac1ed88e14b9dd4eb48d901000000' has constructor arguments in the object store and max_restarts > 0. If the arguments in the object store go out of scope or are lost, the actor restart will fail. See https://github.com/ray-project/ray/issues/53727 for more details.\n",
      "[2025-08-06 17:06:50,729 E 3945021 3945021] core_worker.cc:2740: Actor with class name: 'SingleAgentEnvRunner' and ID: '5b587d5bcfc44bba6135a5f701000000' has constructor arguments in the object store and max_restarts > 0. If the arguments in the object store go out of scope or are lost, the actor restart will fail. See https://github.com/ray-project/ray/issues/53727 for more details.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3945333)\u001b[0m 2025-08-06 17:06:56,271\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3945343)\u001b[0m 2025-08-06 17:07:15,186\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "2025-08-06 17:07:15,615\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[36m(_WrappedExecutable pid=3945337)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(_WrappedExecutable pid=3945337)\u001b[0m 2025-08-06 17:07:20,255\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "2025-08-06 17:07:21,436\tINFO trainable.py:161 -- Trainable.setup took 34.009 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:520: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "[2025-08-06 17:07:21,551 E 3945021 3945021] core_worker.cc:2740: Actor with class name: 'SingleAgentEnvRunner' and ID: '2b3cf06c6c72b1450178aed901000000' has constructor arguments in the object store and max_restarts > 0. If the arguments in the object store go out of scope or are lost, the actor restart will fail. See https://github.com/ray-project/ray/issues/53727 for more details.\n",
      "[2025-08-06 17:07:21,607 E 3945021 3945021] core_worker.cc:2740: Actor with class name: 'SingleAgentEnvRunner' and ID: 'a1010255bf487b75ca23d0ca01000000' has constructor arguments in the object store and max_restarts > 0. If the arguments in the object store go out of scope or are lost, the actor restart will fail. See https://github.com/ray-project/ray/issues/53727 for more details.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3945348)\u001b[0m 2025-08-06 17:07:26,610\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:07:48,765 E 3945255 3945255] (raylet) node_manager.cc:3041: 16 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 61fd26e6f3808fe0e771dcc37032101039e669fcb13ae215856e2c04, IP: 198.18.0.1) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 198.18.0.1`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:08:48,766 E 3945255 3945255] (raylet) node_manager.cc:3041: 17 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 61fd26e6f3808fe0e771dcc37032101039e669fcb13ae215856e2c04, IP: 198.18.0.1) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 198.18.0.1`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:09:48,768 E 3945255 3945255] (raylet) node_manager.cc:3041: 14 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 61fd26e6f3808fe0e771dcc37032101039e669fcb13ae215856e2c04, IP: 198.18.0.1) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 198.18.0.1`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3956505)\u001b[0m 2025-08-06 17:10:41,754\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[36m(_WrappedExecutable pid=3956799)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(_WrappedExecutable pid=3956799)\u001b[0m 2025-08-06 17:10:47,448\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:10:48,773 E 3945255 3945255] (raylet) node_manager.cc:3041: 15 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 61fd26e6f3808fe0e771dcc37032101039e669fcb13ae215856e2c04, IP: 198.18.0.1) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 198.18.0.1`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "2025-08-06 17:10:50,624\tERROR actor_manager.py:873 -- Ray error (The actor a1010255bf487b75ca23d0ca01000000 is unavailable: The actor is temporarily unavailable: IOError: The actor is restarting.. The task may or may not have been executed on the actor.), taking actor 2 out of service.\n",
      "2025-08-06 17:10:50,625\tERROR actor_manager.py:674 -- The actor a1010255bf487b75ca23d0ca01000000 is unavailable: The actor is temporarily unavailable: IOError: The actor is restarting.. The task may or may not have been executed on the actor.\n",
      "NoneType: None\n",
      "2025-08-06 17:10:50,628\tINFO trainable.py:161 -- Trainable.setup took 209.126 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(SingleAgentEnvRunner pid=3957377)\u001b[0m 2025-08-06 17:10:56,354\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:11:48,775 E 3945255 3945255] (raylet) node_manager.cc:3041: 18 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 61fd26e6f3808fe0e771dcc37032101039e669fcb13ae215856e2c04, IP: 198.18.0.1) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 198.18.0.1`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3962319)\u001b[0m 2025-08-06 17:12:29,843\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3962998)\u001b[0m 2025-08-06 17:12:42,319\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:12:48,779 E 3945255 3945255] (raylet) node_manager.cc:3041: 14 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 61fd26e6f3808fe0e771dcc37032101039e669fcb13ae215856e2c04, IP: 198.18.0.1) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 198.18.0.1`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:14:48,786 E 3945255 3945255] (raylet) node_manager.cc:3041: 18 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 61fd26e6f3808fe0e771dcc37032101039e669fcb13ae215856e2c04, IP: 198.18.0.1) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 198.18.0.1`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:15:48,789 E 3945255 3945255] (raylet) node_manager.cc:3041: 25 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 61fd26e6f3808fe0e771dcc37032101039e669fcb13ae215856e2c04, IP: 198.18.0.1) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 198.18.0.1`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:16:48,791 E 3945255 3945255] (raylet) node_manager.cc:3041: 29 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 61fd26e6f3808fe0e771dcc37032101039e669fcb13ae215856e2c04, IP: 198.18.0.1) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 198.18.0.1`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:17:48,792 E 3945255 3945255] (raylet) node_manager.cc:3041: 26 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 61fd26e6f3808fe0e771dcc37032101039e669fcb13ae215856e2c04, IP: 198.18.0.1) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 198.18.0.1`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3980212)\u001b[0m 2025-08-06 17:17:50,225\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3980952)\u001b[0m 2025-08-06 17:18:05,390\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:18:49,324 E 3945255 3945255] (raylet) node_manager.cc:3041: 14 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 61fd26e6f3808fe0e771dcc37032101039e669fcb13ae215856e2c04, IP: 198.18.0.1) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 198.18.0.1`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3983682)\u001b[0m 2025-08-06 17:19:05,184\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:19:49,327 E 3945255 3945255] (raylet) node_manager.cc:3041: 3 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 61fd26e6f3808fe0e771dcc37032101039e669fcb13ae215856e2c04, IP: 198.18.0.1) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 198.18.0.1`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[36m(IMPALA pid=3984719)\u001b[0m 2025-08-06 17:20:22,512\tWARNING algorithm_config.py:5033 -- You are running IMPALA on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "\u001b[36m(IMPALA pid=3984719)\u001b[0m [2025-08-06 17:20:23,040 E 3984719 3984719] core_worker.cc:2740: Actor with class name: 'SingleAgentEnvRunner' and ID: '0d5ccee779076028dd0c6a5101000000' has constructor arguments in the object store and max_restarts > 0. If the arguments in the object store go out of scope or are lost, the actor restart will fail. See https://github.com/ray-project/ray/issues/53727 for more details.\n",
      "\u001b[36m(IMPALA pid=3984719)\u001b[0m [2025-08-06 17:20:23,113 E 3984719 3984719] core_worker.cc:2740: Actor with class name: 'SingleAgentEnvRunner' and ID: '54ded27a4b2994bdab573dcb01000000' has constructor arguments in the object store and max_restarts > 0. If the arguments in the object store go out of scope or are lost, the actor restart will fail. See https://github.com/ray-project/ray/issues/53727 for more details.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3985383)\u001b[0m 2025-08-06 17:20:31,135\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[36m(_WrappedExecutable pid=3986326)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(IMPALA pid=3984719)\u001b[0m 2025-08-06 17:20:31,588\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(IMPALA pid=3984719)\u001b[0m Trainable.setup took 20.392 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:20:49,796 E 3945255 3945255] (raylet) node_manager.cc:3041: 28 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 61fd26e6f3808fe0e771dcc37032101039e669fcb13ae215856e2c04, IP: 198.18.0.1) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 198.18.0.1`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[36m(_WrappedExecutable pid=3986326)\u001b[0m 2025-08-06 17:20:40,872\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[36m(IMPALA(env=CartPole-v1; env-runners=2; learners=1; multi-agent=False) pid=3984719)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/robotarm/ray_results/IMPALA_2025-08-06_17-20-14/IMPALA_CartPole-v1_8f71d_00000_0_2025-08-06_17-20-14/checkpoint_000000)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3989061)\u001b[0m 2025-08-06 17:20:59,734\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:21:49,798 E 3945255 3945255] (raylet) node_manager.cc:3041: 25 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 61fd26e6f3808fe0e771dcc37032101039e669fcb13ae215856e2c04, IP: 198.18.0.1) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 198.18.0.1`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3993313)\u001b[0m 2025-08-06 17:21:58,326\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3995979)\u001b[0m 2025-08-06 17:22:43,396\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:22:50,156 E 3945255 3945255] (raylet) node_manager.cc:3041: 18 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 61fd26e6f3808fe0e771dcc37032101039e669fcb13ae215856e2c04, IP: 198.18.0.1) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 198.18.0.1`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:23:50,161 E 3945255 3945255] (raylet) node_manager.cc:3041: 22 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 61fd26e6f3808fe0e771dcc37032101039e669fcb13ae215856e2c04, IP: 198.18.0.1) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 198.18.0.1`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=4000189)\u001b[0m 2025-08-06 17:23:56,447\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:24:50,165 E 3945255 3945255] (raylet) node_manager.cc:3041: 13 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 61fd26e6f3808fe0e771dcc37032101039e669fcb13ae215856e2c04, IP: 198.18.0.1) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 198.18.0.1`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=4002536)\u001b[0m 2025-08-06 17:24:51,467\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[36m(SingleAgentEnvRunner pid=4004033)\u001b[0m 2025-08-06 17:25:41,455\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:25:50,169 E 3945255 3945255] (raylet) node_manager.cc:3041: 7 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 61fd26e6f3808fe0e771dcc37032101039e669fcb13ae215856e2c04, IP: 198.18.0.1) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 198.18.0.1`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=4005764)\u001b[0m 2025-08-06 17:26:34,720\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[36m(SingleAgentEnvRunner pid=4006358)\u001b[0m 2025-08-06 17:26:48,928\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:26:50,172 E 3945255 3945255] (raylet) node_manager.cc:3041: 10 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 61fd26e6f3808fe0e771dcc37032101039e669fcb13ae215856e2c04, IP: 198.18.0.1) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 198.18.0.1`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=4006607)\u001b[0m 2025-08-06 17:26:58,453\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:27:50,174 E 3945255 3945255] (raylet) node_manager.cc:3041: 2 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 61fd26e6f3808fe0e771dcc37032101039e669fcb13ae215856e2c04, IP: 198.18.0.1) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 198.18.0.1`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=4008493)\u001b[0m 2025-08-06 17:28:19,781\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:28:50,179 E 3945255 3945255] (raylet) node_manager.cc:3041: 13 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 61fd26e6f3808fe0e771dcc37032101039e669fcb13ae215856e2c04, IP: 198.18.0.1) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 198.18.0.1`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=4009983)\u001b[0m 2025-08-06 17:28:56,243\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:29:50,183 E 3945255 3945255] (raylet) node_manager.cc:3041: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 61fd26e6f3808fe0e771dcc37032101039e669fcb13ae215856e2c04, IP: 198.18.0.1) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 198.18.0.1`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:30:50,185 E 3945255 3945255] (raylet) node_manager.cc:3041: 2 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 61fd26e6f3808fe0e771dcc37032101039e669fcb13ae215856e2c04, IP: 198.18.0.1) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 198.18.0.1`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=4011440)\u001b[0m 2025-08-06 17:30:53,840\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[36m(SingleAgentEnvRunner pid=4013010)\u001b[0m 2025-08-06 17:31:31,772\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[36m(SingleAgentEnvRunner pid=4013220)\u001b[0m 2025-08-06 17:31:37,717\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:31:50,370 E 3945255 3945255] (raylet) node_manager.cc:3041: 14 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 61fd26e6f3808fe0e771dcc37032101039e669fcb13ae215856e2c04, IP: 198.18.0.1) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 198.18.0.1`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=4015233)\u001b[0m 2025-08-06 17:32:12,700\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[36m(SingleAgentEnvRunner pid=4017152)\u001b[0m 2025-08-06 17:32:46,161\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:32:50,373 E 3945255 3945255] (raylet) node_manager.cc:3041: 22 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 61fd26e6f3808fe0e771dcc37032101039e669fcb13ae215856e2c04, IP: 198.18.0.1) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 198.18.0.1`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:33:50,800 E 3945255 3945255] (raylet) node_manager.cc:3041: 32 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 61fd26e6f3808fe0e771dcc37032101039e669fcb13ae215856e2c04, IP: 198.18.0.1) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 198.18.0.1`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:34:50,803 E 3945255 3945255] (raylet) node_manager.cc:3041: 28 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 61fd26e6f3808fe0e771dcc37032101039e669fcb13ae215856e2c04, IP: 198.18.0.1) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 198.18.0.1`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=4029308)\u001b[0m 2025-08-06 17:35:21,520\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[36m(SingleAgentEnvRunner pid=4030713)\u001b[0m 2025-08-06 17:35:48,345\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:35:50,806 E 3945255 3945255] (raylet) node_manager.cc:3041: 18 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 61fd26e6f3808fe0e771dcc37032101039e669fcb13ae215856e2c04, IP: 198.18.0.1) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 198.18.0.1`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=4031617)\u001b[0m 2025-08-06 17:36:47,306\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:36:50,810 E 3945255 3945255] (raylet) node_manager.cc:3041: 4 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 61fd26e6f3808fe0e771dcc37032101039e669fcb13ae215856e2c04, IP: 198.18.0.1) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 198.18.0.1`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=4033627)\u001b[0m 2025-08-06 17:37:17,990\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:37:51,091 E 3945255 3945255] (raylet) node_manager.cc:3041: 44 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 61fd26e6f3808fe0e771dcc37032101039e669fcb13ae215856e2c04, IP: 198.18.0.1) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 198.18.0.1`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:38:51,564 E 3945255 3945255] (raylet) node_manager.cc:3041: 54 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 61fd26e6f3808fe0e771dcc37032101039e669fcb13ae215856e2c04, IP: 198.18.0.1) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 198.18.0.1`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:39:51,567 E 3945255 3945255] (raylet) node_manager.cc:3041: 50 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 61fd26e6f3808fe0e771dcc37032101039e669fcb13ae215856e2c04, IP: 198.18.0.1) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 198.18.0.1`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:40:51,721 E 3945255 3945255] (raylet) node_manager.cc:3041: 48 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 61fd26e6f3808fe0e771dcc37032101039e669fcb13ae215856e2c04, IP: 198.18.0.1) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 198.18.0.1`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:41:52,297 E 3945255 3945255] (raylet) node_manager.cc:3041: 47 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 61fd26e6f3808fe0e771dcc37032101039e669fcb13ae215856e2c04, IP: 198.18.0.1) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 198.18.0.1`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:42:52,775 E 3945255 3945255] (raylet) node_manager.cc:3041: 53 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 61fd26e6f3808fe0e771dcc37032101039e669fcb13ae215856e2c04, IP: 198.18.0.1) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 198.18.0.1`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:43:52,778 E 3945255 3945255] (raylet) node_manager.cc:3041: 52 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 61fd26e6f3808fe0e771dcc37032101039e669fcb13ae215856e2c04, IP: 198.18.0.1) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 198.18.0.1`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:44:52,937 E 3945255 3945255] (raylet) node_manager.cc:3041: 54 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 61fd26e6f3808fe0e771dcc37032101039e669fcb13ae215856e2c04, IP: 198.18.0.1) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 198.18.0.1`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:45:52,941 E 3945255 3945255] (raylet) node_manager.cc:3041: 55 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 61fd26e6f3808fe0e771dcc37032101039e669fcb13ae215856e2c04, IP: 198.18.0.1) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 198.18.0.1`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:46:53,051 E 3945255 3945255] (raylet) node_manager.cc:3041: 57 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 61fd26e6f3808fe0e771dcc37032101039e669fcb13ae215856e2c04, IP: 198.18.0.1) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 198.18.0.1`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:47:53,137 E 3945255 3945255] (raylet) node_manager.cc:3041: 57 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 61fd26e6f3808fe0e771dcc37032101039e669fcb13ae215856e2c04, IP: 198.18.0.1) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 198.18.0.1`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:48:53,140 E 3945255 3945255] (raylet) node_manager.cc:3041: 51 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 61fd26e6f3808fe0e771dcc37032101039e669fcb13ae215856e2c04, IP: 198.18.0.1) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 198.18.0.1`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:49:53,144 E 3945255 3945255] (raylet) node_manager.cc:3041: 53 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 61fd26e6f3808fe0e771dcc37032101039e669fcb13ae215856e2c04, IP: 198.18.0.1) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 198.18.0.1`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:50:53,150 E 3945255 3945255] (raylet) node_manager.cc:3041: 59 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 61fd26e6f3808fe0e771dcc37032101039e669fcb13ae215856e2c04, IP: 198.18.0.1) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 198.18.0.1`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:51:53,305 E 3945255 3945255] (raylet) node_manager.cc:3041: 59 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 61fd26e6f3808fe0e771dcc37032101039e669fcb13ae215856e2c04, IP: 198.18.0.1) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 198.18.0.1`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:52:53,896 E 3945255 3945255] (raylet) node_manager.cc:3041: 58 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 61fd26e6f3808fe0e771dcc37032101039e669fcb13ae215856e2c04, IP: 198.18.0.1) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 198.18.0.1`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:53:53,946 E 3945255 3945255] (raylet) node_manager.cc:3041: 47 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 61fd26e6f3808fe0e771dcc37032101039e669fcb13ae215856e2c04, IP: 198.18.0.1) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 198.18.0.1`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:54:53,951 E 3945255 3945255] (raylet) node_manager.cc:3041: 50 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 61fd26e6f3808fe0e771dcc37032101039e669fcb13ae215856e2c04, IP: 198.18.0.1) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 198.18.0.1`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.impala import IMPALAConfig\n",
    "\n",
    "config = (\n",
    "    # Create an `IMPALAConfig` instance.\n",
    "    IMPALAConfig()\n",
    "    # Specify the RL environment.\n",
    "    .environment(\"CartPole-v1\")\n",
    "    # Change the learning rate.\n",
    "    .training(lr=0.0004)\n",
    ")\n",
    "\n",
    "# Change an IMPALA-specific setting (the entropy coefficient).\n",
    "config.training(entropy_coeff=0.01)\n",
    "\n",
    "# Build the algorithm instance.\n",
    "impala = config.build_algo()\n",
    "\n",
    "# Further alter the config without affecting the previously built IMPALA object ...\n",
    "config.training(lr=0.00123)\n",
    "# ... and build a new IMPALA from it.\n",
    "another_impala = config.build_algo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d898c819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-08-06 17:20:54</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:39.63        </td></tr>\n",
       "<tr><td>Memory:      </td><td>14.8/15.3 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 4.0/16 CPUs, 0/2 GPUs (0.0/1.0 accelerator_type:P1000)\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  : ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
       "  \n",
       "  \n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status    </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">           mean_num_learner_gro\n",
       "up_update_called</th><th style=\"text-align: right;\">   ...calls_since_last_\n",
       "synch_worker_weights</th><th style=\"text-align: right;\">     num_training_step_ca\n",
       "lls_per_iteration</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>IMPALA_CartPole-v1_8f71d_00000</td><td>TERMINATED</td><td>198.18.0.1:3984719</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         10.0061</td><td style=\"text-align: right;\">0.00743095</td><td style=\"text-align: right;\">35</td><td style=\"text-align: right;\">3184</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06 17:20:54,176\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/robotarm/ray_results/IMPALA_2025-08-06_17-20-14' in 0.0252s.\n",
      "2025-08-06 17:20:54,214\tINFO tune.py:1041 -- Total run time: 39.70 seconds (39.60 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "from ray import tune\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    \"IMPALA\",\n",
    "    param_space=config,  # <- your RLlib AlgorithmConfig object\n",
    "    run_config=tune.RunConfig(stop={\"num_env_steps_sampled_lifetime\": 4000}),\n",
    ")\n",
    "# Run the experiment with Ray Tune.\n",
    "results = tuner.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b868cd71",
   "metadata": {},
   "source": [
    "## 通用配置设置\n",
    "大多数配置设置是通用的，适用于 RLlib 的所有 Algorithm 类。以下部分将引导您了解用户在深入研究其他配置设置和开始超参数微调之前应密切关注的最重要的配置设置。\n",
    "### RL 环境\n",
    "config.environment(\"Humanoid-v5\")\n",
    "### 学习率 lr\n",
    "config.training(lr=0.0001)\n",
    "### 训练批次大小\n",
    "config.training(train_batch_size_per_learner=256)\n",
    "### 折扣因子 gamma\n",
    "config.training(gamma=0.995)\n",
    "### 使用 num_env_runners 和 num_learners 进行扩展\n",
    "config.env_runners(num_env_runners=4)\n",
    "\n",
    "'''Also use `num_envs_per_env_runner` to vectorize your environment on each EnvRunner actor.\n",
    "Note that this option is only available in single-agent setups.\n",
    "The Ray Team is working on a solution for this restriction. '''\n",
    "\n",
    "config.env_runners(num_envs_per_env_runner=10)\n",
    "\n",
    "config.learners(num_learners=2)\n",
    "### 禁用 explore 行为\n",
    "'''Disable exploration behavior.\n",
    "When False, the EnvRunner calls `forward_inference()` on the RLModule to compute\n",
    "actions instead of `forward_exploration()`. '''\n",
    "\n",
    "config.env_runners(explore=False)\n",
    "### Rollout 长度\n",
    "config.env_runners(rollout_fragment_length=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ray_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "422c3bae",
   "metadata": {},
   "source": [
    "# 环境\n",
    "单智能体设置：一个智能体生活在环境中，并执行由单个策略计算的行动。智能体到策略的映射是固定的（“default_agent” 映射到 “default_policy”）。有关此设置在多智能体情况下的泛化方式，请参阅多智能体环境。\n",
    "\n",
    "## Farama Gymnasium\n",
    "RLlib 依赖 Farama 的 Gymnasium API 作为其主要的 RL 环境接口，用于进行单智能体训练（多智能体请参见此处）。要使用 gymnasium 实现自定义逻辑并将其集成到 RLlib 配置中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba610237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-08-05 20:26:00</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:10.11        </td></tr>\n",
       "<tr><td>Memory:      </td><td>13.5/15.3 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 0/16 CPUs, 0/2 GPUs (0.0/1.0 accelerator_type:P1000)\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  \n",
       "  Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                                                             </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_corridor_env_52b8c_00000</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2025-08-05_20-25-48_528425_3440243/artifacts/2025-08-05_20-25-50/PPO_2025-08-05_20-25-50/driver_artifacts/PPO_corridor_env_52b8c_00000_0_2025-08-05_20-25-50/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_corridor_env_52b8c_00000</td><td>ERROR   </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(PPO pid=3450135)\u001b[0m 2025-08-05 20:25:55,477\tWARNING algorithm_config.py:5033 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m [2025-08-05 20:25:55,871 E 3450135 3450135] core_worker.cc:2740: Actor with class name: 'SingleAgentEnvRunner' and ID: '2030e8858c269b01fd8b382c01000000' has constructor arguments in the object store and max_restarts > 0. If the arguments in the object store go out of scope or are lost, the actor restart will fail. See https://github.com/ray-project/ray/issues/53727 for more details.\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m [2025-08-05 20:25:55,912 E 3450135 3450135] core_worker.cc:2740: Actor with class name: 'SingleAgentEnvRunner' and ID: '693d2a6815d375afd5e75f6701000000' has constructor arguments in the object store and max_restarts > 0. If the arguments in the object store go out of scope or are lost, the actor restart will fail. See https://github.com/ray-project/ray/issues/53727 for more details.\n",
      "2025-08-05 20:26:00,827\tERROR tune_controller.py:1331 -- Trial task failed for trial PPO_corridor_env_52b8c_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/_private/worker.py\", line 2858, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/_private/worker.py\", line 960, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died because of an error raised in its creation task, \u001b[36mray::PPO.__init__()\u001b[39m (pid=3450135, ip=10.110.34.88, actor_id=228c0a947ad0e6ba50ac8e0601000000, repr=PPO(env=corridor_env; env-runners=2; learners=0; multi-agent=False))\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 536, in __init__\n",
      "    super().__init__(\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 158, in __init__\n",
      "    self.setup(copy.deepcopy(self.config))\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 644, in setup\n",
      "    self.env_runner_group = EnvRunnerGroup(\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/env_runner_group.py\", line 198, in __init__\n",
      "    self._setup(\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/env_runner_group.py\", line 286, in _setup\n",
      "    spaces = self.get_spaces()\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/env_runner_group.py\", line 314, in get_spaces\n",
      "    spaces = self.foreach_env_runner(\n",
      "IndexError: list index out of range\n",
      "2025-08-05 20:26:00,836\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/robotarm/ray_results/PPO_2025-08-05_20-25-50' in 0.0057s.\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m 2025-08-05 20:26:00,803\tERROR actor_manager.py:873 -- Ray error (The actor died because of an error raised in its creation task, \u001b[36mray::SingleAgentEnvRunner.__init__()\u001b[39m (pid=3450239, ip=10.110.34.88, actor_id=2030e8858c269b01fd8b382c01000000, repr=<ray.rllib.env.single_agent_env_runner.SingleAgentEnvRunner object at 0x72e635a23ee0>)\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 687, in make\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m     env_spec = _find_spec(id)\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 531, in _find_spec\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m     _check_version_exists(ns, name, version)\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 397, in _check_version_exists\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m     _check_name_exists(ns, name)\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 374, in _check_name_exists\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m     raise error.NameNotFound(\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m gymnasium.error.NameNotFound: Environment `corridor_env` doesn't exist.\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m \n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m \n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m \u001b[36mray::SingleAgentEnvRunner.__init__()\u001b[39m (pid=3450239, ip=10.110.34.88, actor_id=2030e8858c269b01fd8b382c01000000, repr=<ray.rllib.env.single_agent_env_runner.SingleAgentEnvRunner object at 0x72e635a23ee0>)\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 104, in __init__\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m     self.make_env()\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 675, in make_env\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m     gym.make_vec(\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 918, in make_vec\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m     env = gym.vector.SyncVectorEnv(\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/vector/sync_vector_env.py\", line 86, in __init__\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m     self.envs = [env_fn() for env_fn in env_fns]\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/vector/sync_vector_env.py\", line 86, in <listcomp>\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m     self.envs = [env_fn() for env_fn in env_fns]\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 903, in create_single_env\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m     single_env = make(env_spec, **env_spec_kwargs.copy())\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 740, in make\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m     env = env_creator(**env_spec_kwargs)\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/utils/__init__.py\", line 122, in _gym_env_creator\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m     raise EnvError(ERR_MSG_INVALID_ENV_DESCRIPTOR.format(env_descriptor))\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m ray.rllib.utils.error.EnvError: The env string you provided ('corridor_env') is:\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m a) Not a supported or -installed environment.\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m b) Not a tune-registered environment creator.\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m c) Not a valid env class string.\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m \n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m Try one of the following:\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m a) For Atari support: `pip install gym[atari] autorom[accept-rom-license]`.\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m    For PyBullet support: `pip install pybullet`.\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m b) To register your custom env, do `from ray import tune;\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m    tune.register_env('[name]', lambda cfg: [return env obj from here using cfg])`.\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m    Then in your config, do `config.environment(env='[name]').\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m c) Make sure you provide a fully qualified classpath, e.g.:\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m    `ray.rllib.examples.envs.classes.repeat_after_me_env.RepeatAfterMeEnv`), taking actor 1 out of service.\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m 2025-08-05 20:26:00,804\tERROR actor_manager.py:873 -- Ray error (The actor died because of an error raised in its creation task, \u001b[36mray::SingleAgentEnvRunner.__init__()\u001b[39m (pid=3450240, ip=10.110.34.88, actor_id=693d2a6815d375afd5e75f6701000000, repr=<ray.rllib.env.single_agent_env_runner.SingleAgentEnvRunner object at 0x70d158b9fee0>)\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 687, in make\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m     env_spec = _find_spec(id)\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 531, in _find_spec\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m     _check_version_exists(ns, name, version)\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 397, in _check_version_exists\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m     _check_name_exists(ns, name)\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 374, in _check_name_exists\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m     raise error.NameNotFound(\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m gymnasium.error.NameNotFound: Environment `corridor_env` doesn't exist.\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m \n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m \n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m \u001b[36mray::SingleAgentEnvRunner.__init__()\u001b[39m (pid=3450240, ip=10.110.34.88, actor_id=693d2a6815d375afd5e75f6701000000, repr=<ray.rllib.env.single_agent_env_runner.SingleAgentEnvRunner object at 0x70d158b9fee0>)\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 104, in __init__\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m     self.make_env()\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 675, in make_env\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m     gym.make_vec(\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 918, in make_vec\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m     env = gym.vector.SyncVectorEnv(\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/vector/sync_vector_env.py\", line 86, in __init__\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m     self.envs = [env_fn() for env_fn in env_fns]\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/vector/sync_vector_env.py\", line 86, in <listcomp>\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m     self.envs = [env_fn() for env_fn in env_fns]\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 903, in create_single_env\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m     single_env = make(env_spec, **env_spec_kwargs.copy())\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 740, in make\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m     env = env_creator(**env_spec_kwargs)\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/utils/__init__.py\", line 122, in _gym_env_creator\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m     raise EnvError(ERR_MSG_INVALID_ENV_DESCRIPTOR.format(env_descriptor))\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m ray.rllib.utils.error.EnvError: The env string you provided ('corridor_env') is:\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m a) Not a supported or -installed environment.\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m b) Not a tune-registered environment creator.\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m c) Not a valid env class string.\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m \n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m Try one of the following:\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m a) For Atari support: `pip install gym[atari] autorom[accept-rom-license]`.\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m    For PyBullet support: `pip install pybullet`.\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m b) To register your custom env, do `from ray import tune;\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m    tune.register_env('[name]', lambda cfg: [return env obj from here using cfg])`.\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m    Then in your config, do `config.environment(env='[name]').\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m c) Make sure you provide a fully qualified classpath, e.g.:\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m    `ray.rllib.examples.envs.classes.repeat_after_me_env.RepeatAfterMeEnv`), taking actor 2 out of service.\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m 2025-08-05 20:26:00,804\tERROR env_runner_group.py:758 -- Validation of EnvRunner failed! Error=The actor died because of an error raised in its creation task, \u001b[36mray::SingleAgentEnvRunner.__init__()\u001b[39m (pid=3450239, ip=10.110.34.88, actor_id=2030e8858c269b01fd8b382c01000000, repr=<ray.rllib.env.single_agent_env_runner.SingleAgentEnvRunner object at 0x72e635a23ee0>)\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 687, in make\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m     env_spec = _find_spec(id)\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 531, in _find_spec\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m     _check_version_exists(ns, name, version)\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 397, in _check_version_exists\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m     _check_name_exists(ns, name)\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 374, in _check_name_exists\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m     raise error.NameNotFound(\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m gymnasium.error.NameNotFound: Environment `corridor_env` doesn't exist.\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m \n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m \n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m \u001b[36mray::SingleAgentEnvRunner.__init__()\u001b[39m (pid=3450239, ip=10.110.34.88, actor_id=2030e8858c269b01fd8b382c01000000, repr=<ray.rllib.env.single_agent_env_runner.SingleAgentEnvRunner object at 0x72e635a23ee0>)\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 104, in __init__\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m     self.make_env()\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 675, in make_env\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m     gym.make_vec(\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 918, in make_vec\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m     env = gym.vector.SyncVectorEnv(\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/vector/sync_vector_env.py\", line 86, in __init__\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m     self.envs = [env_fn() for env_fn in env_fns]\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/vector/sync_vector_env.py\", line 86, in <listcomp>\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m     self.envs = [env_fn() for env_fn in env_fns]\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 903, in create_single_env\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m     single_env = make(env_spec, **env_spec_kwargs.copy())\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 740, in make\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m     env = env_creator(**env_spec_kwargs)\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/utils/__init__.py\", line 122, in _gym_env_creator\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m     raise EnvError(ERR_MSG_INVALID_ENV_DESCRIPTOR.format(env_descriptor))\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m ray.rllib.utils.error.EnvError: The env string you provided ('corridor_env') is:\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m a) Not a supported or -installed environment.\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m b) Not a tune-registered environment creator.\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m c) Not a valid env class string.\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m \n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m Try one of the following:\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m a) For Atari support: `pip install gym[atari] autorom[accept-rom-license]`.\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m    For PyBullet support: `pip install pybullet`.\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m b) To register your custom env, do `from ray import tune;\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m    tune.register_env('[name]', lambda cfg: [return env obj from here using cfg])`.\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m    Then in your config, do `config.environment(env='[name]').\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m c) Make sure you provide a fully qualified classpath, e.g.:\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m    `ray.rllib.examples.envs.classes.repeat_after_me_env.RepeatAfterMeEnv`\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m 2025-08-05 20:26:00,804\tERROR env_runner_group.py:758 -- Validation of EnvRunner failed! Error=The actor died because of an error raised in its creation task, \u001b[36mray::SingleAgentEnvRunner.__init__()\u001b[39m (pid=3450240, ip=10.110.34.88, actor_id=693d2a6815d375afd5e75f6701000000, repr=<ray.rllib.env.single_agent_env_runner.SingleAgentEnvRunner object at 0x70d158b9fee0>)\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 687, in make\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m     env_spec = _find_spec(id)\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 531, in _find_spec\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m     _check_version_exists(ns, name, version)\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 397, in _check_version_exists\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m     _check_name_exists(ns, name)\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 374, in _check_name_exists\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m     raise error.NameNotFound(\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m gymnasium.error.NameNotFound: Environment `corridor_env` doesn't exist.\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m \n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m \n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m \u001b[36mray::SingleAgentEnvRunner.__init__()\u001b[39m (pid=3450240, ip=10.110.34.88, actor_id=693d2a6815d375afd5e75f6701000000, repr=<ray.rllib.env.single_agent_env_runner.SingleAgentEnvRunner object at 0x70d158b9fee0>)\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 104, in __init__\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m     self.make_env()\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 675, in make_env\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m     gym.make_vec(\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 918, in make_vec\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m     env = gym.vector.SyncVectorEnv(\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/vector/sync_vector_env.py\", line 86, in __init__\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m     self.envs = [env_fn() for env_fn in env_fns]\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/vector/sync_vector_env.py\", line 86, in <listcomp>\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m     self.envs = [env_fn() for env_fn in env_fns]\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 903, in create_single_env\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m     single_env = make(env_spec, **env_spec_kwargs.copy())\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 740, in make\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m     env = env_creator(**env_spec_kwargs)\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/utils/__init__.py\", line 122, in _gym_env_creator\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m     raise EnvError(ERR_MSG_INVALID_ENV_DESCRIPTOR.format(env_descriptor))\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m ray.rllib.utils.error.EnvError: The env string you provided ('corridor_env') is:\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m a) Not a supported or -installed environment.\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m b) Not a tune-registered environment creator.\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m c) Not a valid env class string.\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m \n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m Try one of the following:\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m a) For Atari support: `pip install gym[atari] autorom[accept-rom-license]`.\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m    For PyBullet support: `pip install pybullet`.\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m b) To register your custom env, do `from ray import tune;\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m    tune.register_env('[name]', lambda cfg: [return env obj from here using cfg])`.\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m    Then in your config, do `config.environment(env='[name]').\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m c) Make sure you provide a fully qualified classpath, e.g.:\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m    `ray.rllib.examples.envs.classes.repeat_after_me_env.RepeatAfterMeEnv`\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::PPO.__init__()\u001b[39m (pid=3450135, ip=10.110.34.88, actor_id=228c0a947ad0e6ba50ac8e0601000000, repr=PPO(env=corridor_env; env-runners=2; learners=0; multi-agent=False))\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 536, in __init__\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m     super().__init__(\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 158, in __init__\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 644, in setup\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m     self.env_runner_group = EnvRunnerGroup(\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/env_runner_group.py\", line 198, in __init__\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m     self._setup(\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/env_runner_group.py\", line 286, in _setup\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m     spaces = self.get_spaces()\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/env_runner_group.py\", line 314, in get_spaces\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m     spaces = self.foreach_env_runner(\n",
      "\u001b[36m(PPO pid=3450135)\u001b[0m IndexError: list index out of range\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3450240)\u001b[0m Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::SingleAgentEnvRunner.__init__()\u001b[39m (pid=3450240, ip=10.110.34.88, actor_id=693d2a6815d375afd5e75f6701000000, repr=<ray.rllib.env.single_agent_env_runner.SingleAgentEnvRunner object at 0x70d158b9fee0>)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3450240)\u001b[0m \n",
      "\u001b[36m(SingleAgentEnvRunner pid=3450240)\u001b[0m \n",
      "\u001b[36m(SingleAgentEnvRunner pid=3450240)\u001b[0m \n",
      "\u001b[36m(SingleAgentEnvRunner pid=3450239)\u001b[0m \n",
      "\u001b[36m(SingleAgentEnvRunner pid=3450239)\u001b[0m \n",
      "\u001b[36m(SingleAgentEnvRunner pid=3450239)\u001b[0m \n",
      "2025-08-05 20:26:00,854\tERROR tune.py:1037 -- Trials did not complete: [PPO_corridor_env_52b8c_00000]\n",
      "2025-08-05 20:26:00,855\tINFO tune.py:1041 -- Total run time: 10.14 seconds (10.10 seconds for the tuning loop).\n",
      "2025-08-05 20:26:00,858\tWARNING experiment_analysis.py:180 -- Failed to fetch metrics for 1 trial(s):\n",
      "- PPO_corridor_env_52b8c_00000: FileNotFoundError('Could not fetch metrics for PPO_corridor_env_52b8c_00000: both result.json and progress.csv were not found at /home/robotarm/ray_results/PPO_2025-08-05_20-25-50/PPO_corridor_env_52b8c_00000_0_2025-08-05_20-25-50')\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3450239)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 740, in make\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3450239)\u001b[0m     env_spec = _find_spec(id)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3450239)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 531, in _find_spec\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3450239)\u001b[0m     _check_version_exists(ns, name, version)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3450239)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 397, in _check_version_exists\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3450239)\u001b[0m     _check_name_exists(ns, name)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3450239)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 374, in _check_name_exists\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3450239)\u001b[0m     raise error.NameNotFound(\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3450239)\u001b[0m gymnasium.error.NameNotFound: Environment `corridor_env` doesn't exist.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3450239)\u001b[0m During handling of the above exception, another exception occurred:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3450239)\u001b[0m \u001b[36mray::SingleAgentEnvRunner.__init__()\u001b[39m (pid=3450239, ip=10.110.34.88, actor_id=2030e8858c269b01fd8b382c01000000, repr=<ray.rllib.env.single_agent_env_runner.SingleAgentEnvRunner object at 0x72e635a23ee0>)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3450239)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/vector/sync_vector_env.py\", line 86, in __init__\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3450239)\u001b[0m     self.make_env()\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3450239)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 675, in make_env\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3450239)\u001b[0m     gym.make_vec(\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3450239)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 918, in make_vec\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3450239)\u001b[0m     env = gym.vector.SyncVectorEnv(\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3450239)\u001b[0m     self.envs = [env_fn() for env_fn in env_fns]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3450239)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/vector/sync_vector_env.py\", line 86, in <listcomp>\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3450239)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 903, in create_single_env\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3450239)\u001b[0m     single_env = make(env_spec, **env_spec_kwargs.copy())\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3450239)\u001b[0m     env = env_creator(**env_spec_kwargs)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3450239)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/utils/__init__.py\", line 122, in _gym_env_creator\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3450239)\u001b[0m     raise EnvError(ERR_MSG_INVALID_ENV_DESCRIPTOR.format(env_descriptor))\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3450239)\u001b[0m ray.rllib.utils.error.EnvError: The env string you provided ('corridor_env') is:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3450239)\u001b[0m a) Not a supported or -installed environment.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3450239)\u001b[0m b) Not a tune-registered environment creator.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3450239)\u001b[0m c) Not a valid env class string.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3450239)\u001b[0m Try one of the following:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3450239)\u001b[0m a) For Atari support: `pip install gym[atari] autorom[accept-rom-license]`.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3450239)\u001b[0m    For PyBullet support: `pip install pybullet`.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3450239)\u001b[0m b) To register your custom env, do `from ray import tune;\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3450239)\u001b[0m    tune.register_env('[name]', lambda cfg: [return env obj from here using cfg])`.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3450239)\u001b[0m    Then in your config, do `config.environment(env='[name]').\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3450239)\u001b[0m c) Make sure you provide a fully qualified classpath, e.g.:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3450239)\u001b[0m    `ray.rllib.examples.envs.classes.repeat_after_me_env.RepeatAfterMeEnv`\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3450239)\u001b[0m Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::SingleAgentEnvRunner.__init__()\u001b[39m (pid=3450239, ip=10.110.34.88, actor_id=2030e8858c269b01fd8b382c01000000, repr=<ray.rllib.env.single_agent_env_runner.SingleAgentEnvRunner object at 0x72e635a23ee0>)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"RAY_DISABLE_IMPORT_WARNING\"] = \"1\" \n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import random\n",
    "from typing import Optional\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 1. 环境定义（与原来完全一致）\n",
    "# -------------------------------------------------------------\n",
    "class SimpleCorridor(gym.Env):\n",
    "    \"\"\"Example of a custom env in which the agent has to walk down a corridor.\"\"\"\n",
    "    def __init__(self, config: Optional[dict] = None):\n",
    "        config = config or {}\n",
    "        self.end_pos = config.get(\"corridor_length\", 7)\n",
    "        self.cur_pos = 0\n",
    "        self.action_space = Discrete(2)\n",
    "        self.observation_space = Box(0.0, self.end_pos, shape=(1,), dtype=np.float32)\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        random.seed(seed)\n",
    "        self.cur_pos = 0\n",
    "        return np.array([self.cur_pos], np.float32), {\"env_state\": \"reset\"}\n",
    "\n",
    "    def step(self, action):\n",
    "        assert action in [0, 1], action\n",
    "        if action == 0 and self.cur_pos > 0:\n",
    "            self.cur_pos -= 1\n",
    "        elif action == 1:\n",
    "            self.cur_pos += 1\n",
    "\n",
    "        terminated = self.cur_pos >= self.end_pos\n",
    "        truncated = False\n",
    "        reward = random.uniform(0.5, 1.5) if terminated else -0.01\n",
    "        infos = {}\n",
    "        return (\n",
    "            np.array([self.cur_pos], np.float32),\n",
    "            reward,\n",
    "            terminated,\n",
    "            truncated,\n",
    "            infos,\n",
    "        )\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 2. 注册环境（字符串名称为 \"corridor_env\"）\n",
    "# -------------------------------------------------------------\n",
    "register_env(\"corridor_env\", lambda cfg: SimpleCorridor(cfg))\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 3. 训练函数封装\n",
    "# -------------------------------------------------------------\n",
    "def train(\n",
    "    algo: str = \"PPO\",\n",
    "    corridor_length: int = 10,\n",
    "    num_iterations: int = 50,\n",
    "    stop_timesteps: int = 100_000,\n",
    "    framework: str = \"torch\",\n",
    "):\n",
    "    \"\"\"在 notebook 里直接调用即可开始训练。\"\"\"\n",
    "    ray.shutdown()   # 防止重复启动\n",
    "    ray.init(ignore_reinit_error=True)\n",
    "\n",
    "    config = (\n",
    "        PPOConfig()\n",
    "        if algo.upper() == \"PPO\"\n",
    "        else ray.rllib.algorithms.dqn.DQNConfig()\n",
    "        # 可以按需扩展更多算法\n",
    "    )\n",
    "    config = config.framework(framework).environment(\n",
    "        \"corridor_env\",\n",
    "        env_config={\"corridor_length\": corridor_length},\n",
    "    )\n",
    "\n",
    "    stop = {\n",
    "        \"training_iteration\": num_iterations,\n",
    "        \"timesteps_total\": stop_timesteps,\n",
    "    }\n",
    "\n",
    "    tuner = tune.Tuner(\n",
    "        algo.upper(),\n",
    "        param_space=config.to_dict(),\n",
    "        run_config=tune.RunConfig(stop=stop, verbose=1),\n",
    "    )\n",
    "    results = tuner.fit()\n",
    "    ray.shutdown()\n",
    "    return results\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 4. 开始训练（示例参数）\n",
    "# -------------------------------------------------------------\n",
    "results = train(\n",
    "    algo=\"PPO\",\n",
    "    corridor_length=10,\n",
    "    num_iterations=30,\n",
    "    stop_timesteps=50_000,\n",
    "    framework=\"torch\",\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 5. 查看训练结果\n",
    "# -------------------------------------------------------------\n",
    "df = results.get_dataframe()\n",
    "display(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1191c351",
   "metadata": {},
   "source": [
    "## 配置环境\n",
    "\n",
    "## 通过字符串指定\n",
    "默认情况下，RLlib 将字符串值解释为已注册的 gymnasium 环境名称。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41c4d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-05 20:34:45,676\tWARNING deprecation.py:50 -- DeprecationWarning: `build` has been deprecated. Use `AlgorithmConfig.build_algo` instead. This will raise an error in the future!\n",
      "2025-08-05 20:34:46,764\tINFO worker.py:1927 -- Started a local Ray instance.\n",
      "[2025-08-05 20:34:48,023 E 3440243 3440243] core_worker.cc:2740: Actor with class name: 'SingleAgentEnvRunner' and ID: '1fe1a3a9c425500126ad373301000000' has constructor arguments in the object store and max_restarts > 0. If the arguments in the object store go out of scope or are lost, the actor restart will fail. See https://github.com/ray-project/ray/issues/53727 for more details.\n",
      "[2025-08-05 20:34:48,075 E 3440243 3440243] core_worker.cc:2740: Actor with class name: 'SingleAgentEnvRunner' and ID: '76e5fe6a1250263565a7eb8601000000' has constructor arguments in the object store and max_restarts > 0. If the arguments in the object store go out of scope or are lost, the actor restart will fail. See https://github.com/ray-project/ray/issues/53727 for more details.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451772)\u001b[0m 2025-08-05 20:34:51,927\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "2025-08-05 20:34:52,056\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'timers': {'training_iteration': 14.585700060939416, 'restore_env_runners': 2.1748011931777e-05, 'training_step': 14.585240917047486, 'env_runner_sampling_timer': 3.6546126888133585, 'learner_update_timer': 10.92649948806502, 'synch_weights': 0.0034019839949905872}, 'env_runners': {'env_to_module_connector': {'timers': {'connectors': {'numpy_to_tensor': np.float64(5.8639155159843326e-05), 'add_states_from_episodes_to_batch': np.float64(7.963415578356485e-06), 'add_time_dim_to_batch_and_zero_pad': np.float64(1.2963210141504977e-05), 'add_observations_from_episodes_to_batch': np.float64(1.5038805751915865e-05), 'batch_individual_items': np.float64(3.155113668666544e-05)}}, 'connector_pipeline_timer': np.float64(0.00023713496881496386)}, 'env_to_module_sum_episodes_length_in': np.float64(404.2802303359448), 'episode_return_min': -500.0, 'num_env_steps_sampled_lifetime': 4000.0, 'episode_duration_sec_mean': 0.8947852913988754, 'module_to_env_connector': {'connector_pipeline_timer': np.float64(0.0008134045991979356), 'timers': {'connectors': {'remove_single_ts_time_rank_from_batch': np.float64(3.373241798648137e-06), 'tensor_to_numpy': np.float64(9.131757883081118e-05), 'get_actions': np.float64(0.0004045074029157094), 'un_batch_to_individual_items': np.float64(2.9986659783030003e-05), 'normalize_and_clip_actions': np.float64(5.0859684041959654e-05), 'listify_data_for_vector_env': np.float64(5.6986423017972064e-05)}}}, 'episode_len_max': 500, 'rlmodule_inference_timer': np.float64(0.00022885896243766502), 'agent_episode_return_mean': {'default_agent': -500.0}, 'env_reset_timer': np.float64(0.0005835599731653929), 'module_episode_return_mean': {'default_policy': -500.0}, 'num_env_steps_sampled': 4000.0, 'num_agent_steps_sampled_lifetime': {'default_agent': 4000.0}, 'episode_return_mean': -500.0, 'weights_seq_no': 0.0, 'env_to_module_sum_episodes_length_out': np.float64(404.2802303359448), 'num_module_steps_sampled': {'default_policy': 4000.0}, 'episode_len_min': 500, 'env_step_timer': np.float64(0.00021301928874181028), 'episode_return_max': -500.0, 'num_episodes_lifetime': 8.0, 'num_episodes': 8.0, 'num_module_steps_sampled_lifetime': {'default_policy': 4000.0}, 'episode_len_mean': 500.0, 'sample': np.float64(3.620893400395289), 'num_agent_steps_sampled': {'default_agent': 4000.0}, 'num_env_steps_sampled_lifetime_throughput': np.float64(552.2340475699315)}, 'learners': {'__all_modules__': {'learner_connector': {'timers': {'connectors': {'add_states_from_episodes_to_batch': 1.2204982340335846e-05, 'add_time_dim_to_batch_and_zero_pad': 3.934185951948166e-05, 'add_observations_from_episodes_to_batch': 0.0002847830764949322, 'batch_individual_items': 0.08366008894518018, 'add_one_ts_to_episodes_and_truncate': 0.0068622468970716, 'general_advantage_estimation': 0.055441277800127864, 'numpy_to_tensor': 0.00041825510561466217, 'add_columns_from_episodes_to_train_batch': 0.11558559793047607}}, 'connector_pipeline_timer': 0.2631617549341172}, 'learner_connector_sum_episodes_length_in': 4000, 'num_module_steps_trained_lifetime': 120320, 'num_module_steps_trained': 120320, 'num_env_steps_trained': 3767520, 'num_env_steps_trained_lifetime': 3767520, 'num_trainable_parameters': 136196, 'learner_connector_sum_episodes_length_out': 4008, 'num_non_trainable_parameters': 0, 'num_env_steps_trained_lifetime_throughput': 348955.9420764823, 'num_module_steps_trained_throughput': 11142.605943456672, 'num_module_steps_trained_lifetime_throughput': 11143.080070342774}, 'default_policy': {'weights_seq_no': 1.0, 'curr_entropy_coeff': 0.0, 'vf_loss': np.float32(9.925465), 'mean_kl_loss': np.float32(0.008218132), 'num_trainable_parameters': 136196, 'diff_num_grad_updates_vs_sampler_policy': np.float32(1.0), 'vf_loss_unclipped': np.float32(6500.0044), 'gradients_default_optimizer_global_norm': np.float32(0.40412733), 'module_train_batch_size_mean': 128.0, 'total_loss': np.float32(10.038495), 'vf_explained_var': np.float32(0.024752676), 'entropy': np.float32(1.0878911), 'default_optimizer_learning_rate': 5e-05, 'num_module_steps_trained_lifetime': 120320, 'num_module_steps_trained': 120320, 'curr_kl_coeff': 0.20000000298023224, 'policy_loss': np.float32(0.11138716), 'num_module_steps_trained_lifetime_throughput': 11142.263801035022}}, 'num_training_step_calls_per_iteration': 1, 'num_env_steps_sampled_lifetime': 4000.0, 'fault_tolerance': {'num_healthy_workers': 2, 'num_remote_worker_restarts': 0}, 'env_runner_group': {'actor_manager_num_outstanding_async_reqs': 0}, 'done': False, 'training_iteration': 1, 'trial_id': 'default', 'date': '2025-08-05_20-35-08', 'timestamp': 1754397308, 'time_this_iter_s': 14.595700025558472, 'time_total_s': 14.595700025558472, 'pid': 3440243, 'hostname': 'robotarm-Precision-7920-Tower', 'node_ip': '10.110.34.88', 'config': {'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'Acrobot-v1', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 0, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1, '_disable_preprocessor_api': False, '_disable_action_flattening': False}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'learner_config_dict': {}, 'optimizer': {}, '_learner_class': None, 'callbacks_on_algorithm_init': None, 'callbacks_on_env_runners_recreated': None, 'callbacks_on_offline_eval_runners_recreated': None, 'callbacks_on_checkpoint_loaded': None, 'callbacks_on_environment_created': None, 'callbacks_on_episode_created': None, 'callbacks_on_episode_start': None, 'callbacks_on_episode_step': None, 'callbacks_on_episode_end': None, 'callbacks_on_evaluate_start': None, 'callbacks_on_evaluate_end': None, 'callbacks_on_evaluate_offline_start': None, 'callbacks_on_evaluate_offline_end': None, 'callbacks_on_sample_end': None, 'callbacks_on_train_result': None, 'explore': True, 'enable_rl_module_and_learner': True, 'enable_env_runner_and_connector_v2': True, '_prior_exploration_config': {'type': 'StochasticSampling'}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x7b71e46303a0>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'offline_data_class': None, 'input_read_method': 'read_parquet', 'input_read_method_kwargs': {}, 'input_read_schema': {}, 'input_read_episodes': False, 'input_read_sample_batches': False, 'input_read_batch_size': None, 'input_filesystem': None, 'input_filesystem_kwargs': {}, 'input_compress_columns': ['obs', 'new_obs'], 'input_spaces_jsonable': True, 'materialize_data': False, 'materialize_mapped_data': True, 'map_batches_kwargs': {}, 'iter_batches_kwargs': {}, 'ignore_final_observation': False, 'prelearner_class': None, 'prelearner_buffer_class': None, 'prelearner_buffer_kwargs': {}, 'prelearner_module_synch_period': 10, 'dataset_num_iters_per_learner': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'output_max_rows_per_file': None, 'output_write_remaining_data': False, 'output_write_method': 'write_parquet', 'output_write_method_kwargs': {}, 'output_filesystem': None, 'output_filesystem_kwargs': {}, 'output_write_episodes': True, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_auto_duration_min_env_steps_per_sample': 100, 'evaluation_auto_duration_max_env_steps_per_sample': 2000, 'evaluation_parallel_to_training': False, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 0, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'offline_evaluation_interval': None, 'num_offline_eval_runners': 0, 'offline_evaluation_type': None, 'offline_eval_runner_class': None, 'offline_loss_for_module_fn': None, 'offline_evaluation_duration': 1, 'offline_evaluation_parallel_to_training': False, 'offline_evaluation_timeout_s': 120.0, 'num_cpus_per_offline_eval_runner': 1, 'num_gpus_per_offline_eval_runner': 0, 'custom_resources_per_offline_eval_runner': {}, 'restart_failed_offline_eval_runners': True, 'ignore_offline_eval_runner_failures': False, 'max_num_offline_eval_runner_restarts': 1000, 'offline_eval_runner_restore_timeout_s': 1800.0, 'max_requests_in_flight_per_offline_eval_runner': 1, 'validate_offline_eval_runners_after_construction': True, 'offline_eval_runner_health_probe_timeout_s': 30.0, 'offline_eval_rl_module_inference_only': False, 'broadcast_offline_eval_runner_states': False, 'offline_eval_batch_size_per_runner': 256, 'dataset_num_iters_per_eval_runner': 1, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'log_gradients': True, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'restart_failed_env_runners': True, 'ignore_env_runner_failures': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30.0, 'env_runner_restore_timeout_s': 1800.0, '_model_config': {}, '_rl_module_spec': None, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, '_validate_config': True, '_use_msgpack_checkpoints': False, '_torch_grad_scaler_class': None, '_torch_lr_scheduler_classes': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, '_dont_auto_sync_env_runner_states': False, 'env_task_fn': -1, 'enable_connectors': -1, 'simple_optimizer': False, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'always_attach_evaluation_results': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'entropy_coeff_schedule': None, 'lr_schedule': None, 'sgd_minibatch_size': -1, 'vf_share_layers': -1, 'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>, 'lambda': 1.0, 'input': 'sampler', 'policies': {'default_policy': (None, None, None, None)}, 'callbacks': <class 'ray.rllib.callbacks.callbacks.RLlibCallback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch'}, 'time_since_restore': 14.595700025558472, 'iterations_since_restore': 1, 'perf': {'cpu_util_percent': np.float64(46.00526315789474), 'ram_util_percent': np.float64(87.71052631578948), 'gpu_util_percent0': np.float64(0.20105263157894737), 'vram_util_percent0': np.float64(0.18319541529605263), 'gpu_util_percent1': np.float64(0.0), 'vram_util_percent1': np.float64(0.0005699397492265105)}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(SingleAgentEnvRunner pid=3451771)\u001b[0m 2025-08-05 20:36:51,066\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451771)\u001b[0m /home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:134: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451771)\u001b[0m   logger.warn(\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451771)\u001b[0m /home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:158: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451771)\u001b[0m   logger.warn(f\"{pre} is not within the observation space.\")\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451771)\u001b[0m /home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:134: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451771)\u001b[0m   logger.warn(\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451771)\u001b[0m /home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:158: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451771)\u001b[0m   logger.warn(f\"{pre} is not within the observation space.\")\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451773)\u001b[0m 2025-08-05 20:42:51,311\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451773)\u001b[0m /home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:134: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451773)\u001b[0m   logger.warn(\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451773)\u001b[0m /home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:158: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451773)\u001b[0m   logger.warn(f\"{pre} is not within the observation space.\")\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451773)\u001b[0m /home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:134: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451773)\u001b[0m   logger.warn(\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451773)\u001b[0m /home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:158: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451773)\u001b[0m   logger.warn(f\"{pre} is not within the observation space.\")\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451782)\u001b[0m 2025-08-06 12:44:07,448\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 12:44:49,602 E 3451695 3451695] (raylet) node_manager.cc:3041: 2 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 233c7c689851320fade6364e0333fb29c9b451bd3bd34bb9bacf2d08, IP: 10.110.34.88) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.110.34.88`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=3451777)\u001b[0m 2025-08-06 15:48:38,544\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[36m(MultiAgentEnvRunner pid=3451781)\u001b[0m 2025-08-06 15:48:43,975\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 15:48:50,119 E 3451695 3451695] (raylet) node_manager.cc:3041: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 233c7c689851320fade6364e0333fb29c9b451bd3bd34bb9bacf2d08, IP: 10.110.34.88) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.110.34.88`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451775)\u001b[0m Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::SingleAgentEnvRunner.__init__()\u001b[39m (pid=3451775, ip=10.110.34.88, actor_id=d1ad7c3ac22d52ff2a86f56301000000, repr=<ray.rllib.env.single_agent_env_runner.SingleAgentEnvRunner object at 0x78dc0083fdf0>)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451775)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 687, in make\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451775)\u001b[0m     env_spec = _find_spec(id)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451775)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 531, in _find_spec\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451775)\u001b[0m     _check_version_exists(ns, name, version)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451775)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 397, in _check_version_exists\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451775)\u001b[0m     _check_name_exists(ns, name)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451775)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 374, in _check_name_exists\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451775)\u001b[0m     raise error.NameNotFound(\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451775)\u001b[0m gymnasium.error.NameNotFound: Environment `my_multiagent_env` doesn't exist.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451775)\u001b[0m \n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451775)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451775)\u001b[0m \n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451775)\u001b[0m \u001b[36mray::SingleAgentEnvRunner.__init__()\u001b[39m (pid=3451775, ip=10.110.34.88, actor_id=d1ad7c3ac22d52ff2a86f56301000000, repr=<ray.rllib.env.single_agent_env_runner.SingleAgentEnvRunner object at 0x78dc0083fdf0>)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451775)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 104, in __init__\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451775)\u001b[0m     self.make_env()\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451775)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 675, in make_env\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451775)\u001b[0m     gym.make_vec(\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451775)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 918, in make_vec\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451775)\u001b[0m     env = gym.vector.SyncVectorEnv(\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451775)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/vector/sync_vector_env.py\", line 86, in __init__\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451775)\u001b[0m     self.envs = [env_fn() for env_fn in env_fns]\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451775)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/vector/sync_vector_env.py\", line 86, in <listcomp>\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451775)\u001b[0m     self.envs = [env_fn() for env_fn in env_fns]\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451775)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 903, in create_single_env\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451775)\u001b[0m     single_env = make(env_spec, **env_spec_kwargs.copy())\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451775)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 740, in make\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451775)\u001b[0m     env = env_creator(**env_spec_kwargs)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451775)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/utils/__init__.py\", line 122, in _gym_env_creator\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451775)\u001b[0m     raise EnvError(ERR_MSG_INVALID_ENV_DESCRIPTOR.format(env_descriptor))\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451775)\u001b[0m ray.rllib.utils.error.EnvError: The env string you provided ('my_multiagent_env') is:\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451775)\u001b[0m a) Not a supported or -installed environment.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451775)\u001b[0m b) Not a tune-registered environment creator.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451775)\u001b[0m c) Not a valid env class string.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451775)\u001b[0m \n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451775)\u001b[0m Try one of the following:\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451775)\u001b[0m a) For Atari support: `pip install gym[atari] autorom[accept-rom-license]`.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451775)\u001b[0m    For PyBullet support: `pip install pybullet`.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451775)\u001b[0m b) To register your custom env, do `from ray import tune;\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451775)\u001b[0m    tune.register_env('[name]', lambda cfg: [return env obj from here using cfg])`.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451775)\u001b[0m    Then in your config, do `config.environment(env='[name]').\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451775)\u001b[0m c) Make sure you provide a fully qualified classpath, e.g.:\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451775)\u001b[0m    `ray.rllib.examples.envs.classes.repeat_after_me_env.RepeatAfterMeEnv`\n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451776)\u001b[0m \n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451776)\u001b[0m \n",
      "\u001b[36m(SingleAgentEnvRunner pid=3451776)\u001b[0m \n",
      "\u001b[36m(MultiAgentEnvRunner pid=3451780)\u001b[0m Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::MultiAgentEnvRunner.__init__()\u001b[39m (pid=3451780, ip=10.110.34.88, actor_id=1172d246e7869f1f78a7f57101000000, repr=<ray.rllib.env.multi_agent_env_runner.MultiAgentEnvRunner object at 0x73ba57485990>)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=3451780)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 112, in __init__\n",
      "\u001b[36m(MultiAgentEnvRunner pid=3451780)\u001b[0m     self.make_env()\n",
      "\u001b[36m(MultiAgentEnvRunner pid=3451780)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 789, in make_env\n",
      "\u001b[36m(MultiAgentEnvRunner pid=3451780)\u001b[0m     raise ValueError(\n",
      "\u001b[36m(MultiAgentEnvRunner pid=3451780)\u001b[0m ValueError: `config.env` is not provided! You should provide a valid environment to your config through `config.environment([env descriptor e.g. 'CartPole-v1'])`.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=3451783)\u001b[0m Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::MultiAgentEnvRunner.__init__()\u001b[39m (pid=3451783, ip=10.110.34.88, actor_id=1c40ad55e64b77037f67a67b01000000, repr=<ray.rllib.env.multi_agent_env_runner.MultiAgentEnvRunner object at 0x7078373e5a20>)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=3451783)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 112, in __init__\n",
      "\u001b[36m(MultiAgentEnvRunner pid=3451783)\u001b[0m     self.make_env()\n",
      "\u001b[36m(MultiAgentEnvRunner pid=3451783)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 817, in make_env\n",
      "\u001b[36m(MultiAgentEnvRunner pid=3451783)\u001b[0m     self.env = make_vec(\n",
      "\u001b[36m(MultiAgentEnvRunner pid=3451783)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/vector/registration.py\", line 69, in make_vec\n",
      "\u001b[36m(MultiAgentEnvRunner pid=3451783)\u001b[0m     env = SyncVectorMultiAgentEnv(\n",
      "\u001b[36m(MultiAgentEnvRunner pid=3451783)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/vector/sync_vector_multi_agent_env.py\", line 36, in __init__\n",
      "\u001b[36m(MultiAgentEnvRunner pid=3451783)\u001b[0m     if self.envs[0].unwrapped.observation_spaces is not None:\n",
      "\u001b[36m(MultiAgentEnvRunner pid=3451783)\u001b[0m AttributeError: 'AcrobotEnv' object has no attribute 'observation_spaces'\n",
      "\u001b[36m(MultiAgentEnvRunner pid=3932281)\u001b[0m Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::MultiAgentEnvRunner.__init__()\u001b[39m (pid=3932281, ip=10.110.34.88, actor_id=657d113a1e90d924adbed6ae01000000, repr=<ray.rllib.env.multi_agent_env_runner.MultiAgentEnvRunner object at 0x7e4bff471b10>)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=3932281)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 112, in __init__\n",
      "\u001b[36m(MultiAgentEnvRunner pid=3932281)\u001b[0m     self.make_env()\n",
      "\u001b[36m(MultiAgentEnvRunner pid=3932281)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 817, in make_env\n",
      "\u001b[36m(MultiAgentEnvRunner pid=3932281)\u001b[0m     self.env = make_vec(\n",
      "\u001b[36m(MultiAgentEnvRunner pid=3932281)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/vector/registration.py\", line 69, in make_vec\n",
      "\u001b[36m(MultiAgentEnvRunner pid=3932281)\u001b[0m     env = SyncVectorMultiAgentEnv(\n",
      "\u001b[36m(MultiAgentEnvRunner pid=3932281)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/vector/sync_vector_multi_agent_env.py\", line 36, in __init__\n",
      "\u001b[36m(MultiAgentEnvRunner pid=3932281)\u001b[0m     if self.envs[0].unwrapped.observation_spaces is not None:\n",
      "\u001b[36m(MultiAgentEnvRunner pid=3932281)\u001b[0m AttributeError: 'CartPoleEnv' object has no attribute 'observation_spaces'\n",
      "\u001b[36m(MultiAgentEnvRunner pid=3932991)\u001b[0m Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::MultiAgentEnvRunner.__init__()\u001b[39m (pid=3932991, ip=10.110.34.88, actor_id=47414a5b2117509e5f5595e801000000, repr=<ray.rllib.env.multi_agent_env_runner.MultiAgentEnvRunner object at 0x709e61171b40>)\n",
      "\u001b[36m(MultiAgentEnvRunner pid=3932991)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 112, in __init__\n",
      "\u001b[36m(MultiAgentEnvRunner pid=3932991)\u001b[0m     self.make_env()\n",
      "\u001b[36m(MultiAgentEnvRunner pid=3932991)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 817, in make_env\n",
      "\u001b[36m(MultiAgentEnvRunner pid=3932991)\u001b[0m     self.env = make_vec(\n",
      "\u001b[36m(MultiAgentEnvRunner pid=3932991)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/vector/registration.py\", line 69, in make_vec\n",
      "\u001b[36m(MultiAgentEnvRunner pid=3932991)\u001b[0m     env = SyncVectorMultiAgentEnv(\n",
      "\u001b[36m(MultiAgentEnvRunner pid=3932991)\u001b[0m   File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/vector/sync_vector_multi_agent_env.py\", line 36, in __init__\n",
      "\u001b[36m(MultiAgentEnvRunner pid=3932991)\u001b[0m     if self.envs[0].unwrapped.observation_spaces is not None:\n",
      "\u001b[36m(MultiAgentEnvRunner pid=3932991)\u001b[0m AttributeError: 'PendulumEnv' object has no attribute 'observation_spaces'\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:06:50,498 E 3451695 3451695] (raylet) node_manager.cc:3041: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 233c7c689851320fade6364e0333fb29c9b451bd3bd34bb9bacf2d08, IP: 10.110.34.88) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.110.34.88`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=3947125)\u001b[0m 2025-08-06 17:07:15,191\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[36m(MultiAgentEnvRunner pid=3947273)\u001b[0m 2025-08-06 17:07:20,324\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:07:50,502 E 3451695 3451695] (raylet) node_manager.cc:3041: 16 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 233c7c689851320fade6364e0333fb29c9b451bd3bd34bb9bacf2d08, IP: 10.110.34.88) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.110.34.88`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:08:50,504 E 3451695 3451695] (raylet) node_manager.cc:3041: 16 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 233c7c689851320fade6364e0333fb29c9b451bd3bd34bb9bacf2d08, IP: 10.110.34.88) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.110.34.88`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:09:50,507 E 3451695 3451695] (raylet) node_manager.cc:3041: 15 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 233c7c689851320fade6364e0333fb29c9b451bd3bd34bb9bacf2d08, IP: 10.110.34.88) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.110.34.88`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:10:50,980 E 3451695 3451695] (raylet) node_manager.cc:3041: 16 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 233c7c689851320fade6364e0333fb29c9b451bd3bd34bb9bacf2d08, IP: 10.110.34.88) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.110.34.88`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:11:50,982 E 3451695 3451695] (raylet) node_manager.cc:3041: 17 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 233c7c689851320fade6364e0333fb29c9b451bd3bd34bb9bacf2d08, IP: 10.110.34.88) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.110.34.88`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:12:50,983 E 3451695 3451695] (raylet) node_manager.cc:3041: 14 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 233c7c689851320fade6364e0333fb29c9b451bd3bd34bb9bacf2d08, IP: 10.110.34.88) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.110.34.88`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:13:51,210 E 3451695 3451695] (raylet) node_manager.cc:3041: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 233c7c689851320fade6364e0333fb29c9b451bd3bd34bb9bacf2d08, IP: 10.110.34.88) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.110.34.88`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=3964447)\u001b[0m 2025-08-06 17:14:07,379\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:14:51,539 E 3451695 3451695] (raylet) node_manager.cc:3041: 17 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 233c7c689851320fade6364e0333fb29c9b451bd3bd34bb9bacf2d08, IP: 10.110.34.88) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.110.34.88`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:15:51,540 E 3451695 3451695] (raylet) node_manager.cc:3041: 24 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 233c7c689851320fade6364e0333fb29c9b451bd3bd34bb9bacf2d08, IP: 10.110.34.88) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.110.34.88`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:16:51,542 E 3451695 3451695] (raylet) node_manager.cc:3041: 28 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 233c7c689851320fade6364e0333fb29c9b451bd3bd34bb9bacf2d08, IP: 10.110.34.88) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.110.34.88`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:17:51,545 E 3451695 3451695] (raylet) node_manager.cc:3041: 24 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 233c7c689851320fade6364e0333fb29c9b451bd3bd34bb9bacf2d08, IP: 10.110.34.88) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.110.34.88`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:18:51,548 E 3451695 3451695] (raylet) node_manager.cc:3041: 15 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 233c7c689851320fade6364e0333fb29c9b451bd3bd34bb9bacf2d08, IP: 10.110.34.88) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.110.34.88`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:19:51,551 E 3451695 3451695] (raylet) node_manager.cc:3041: 3 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 233c7c689851320fade6364e0333fb29c9b451bd3bd34bb9bacf2d08, IP: 10.110.34.88) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.110.34.88`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:20:51,852 E 3451695 3451695] (raylet) node_manager.cc:3041: 30 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 233c7c689851320fade6364e0333fb29c9b451bd3bd34bb9bacf2d08, IP: 10.110.34.88) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.110.34.88`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=3989167)\u001b[0m 2025-08-06 17:20:59,793\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:21:51,854 E 3451695 3451695] (raylet) node_manager.cc:3041: 25 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 233c7c689851320fade6364e0333fb29c9b451bd3bd34bb9bacf2d08, IP: 10.110.34.88) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.110.34.88`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=3993504)\u001b[0m 2025-08-06 17:22:02,493\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:22:51,856 E 3451695 3451695] (raylet) node_manager.cc:3041: 19 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 233c7c689851320fade6364e0333fb29c9b451bd3bd34bb9bacf2d08, IP: 10.110.34.88) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.110.34.88`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=3998190)\u001b[0m 2025-08-06 17:23:23,349\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:23:51,857 E 3451695 3451695] (raylet) node_manager.cc:3041: 22 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 233c7c689851320fade6364e0333fb29c9b451bd3bd34bb9bacf2d08, IP: 10.110.34.88) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.110.34.88`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=4001744)\u001b[0m 2025-08-06 17:24:32,362\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:24:51,861 E 3451695 3451695] (raylet) node_manager.cc:3041: 12 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 233c7c689851320fade6364e0333fb29c9b451bd3bd34bb9bacf2d08, IP: 10.110.34.88) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.110.34.88`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:25:51,866 E 3451695 3451695] (raylet) node_manager.cc:3041: 7 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 233c7c689851320fade6364e0333fb29c9b451bd3bd34bb9bacf2d08, IP: 10.110.34.88) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.110.34.88`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=4004563)\u001b[0m 2025-08-06 17:26:10,977\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[36m(MultiAgentEnvRunner pid=4005763)\u001b[0m 2025-08-06 17:26:34,638\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:26:51,869 E 3451695 3451695] (raylet) node_manager.cc:3041: 12 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 233c7c689851320fade6364e0333fb29c9b451bd3bd34bb9bacf2d08, IP: 10.110.34.88) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.110.34.88`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=4006606)\u001b[0m 2025-08-06 17:26:58,477\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[36m(MultiAgentEnvRunner pid=4007003)\u001b[0m 2025-08-06 17:27:23,438\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:27:52,364 E 3451695 3451695] (raylet) node_manager.cc:3041: 4 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 233c7c689851320fade6364e0333fb29c9b451bd3bd34bb9bacf2d08, IP: 10.110.34.88) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.110.34.88`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=4008494)\u001b[0m 2025-08-06 17:28:19,802\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[36m(MultiAgentEnvRunner pid=4009081)\u001b[0m 2025-08-06 17:28:36,265\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:28:52,366 E 3451695 3451695] (raylet) node_manager.cc:3041: 12 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 233c7c689851320fade6364e0333fb29c9b451bd3bd34bb9bacf2d08, IP: 10.110.34.88) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.110.34.88`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:30:52,371 E 3451695 3451695] (raylet) node_manager.cc:3041: 2 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 233c7c689851320fade6364e0333fb29c9b451bd3bd34bb9bacf2d08, IP: 10.110.34.88) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.110.34.88`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:31:52,373 E 3451695 3451695] (raylet) node_manager.cc:3041: 13 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 233c7c689851320fade6364e0333fb29c9b451bd3bd34bb9bacf2d08, IP: 10.110.34.88) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.110.34.88`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=4016147)\u001b[0m 2025-08-06 17:32:28,798\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:32:52,544 E 3451695 3451695] (raylet) node_manager.cc:3041: 21 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 233c7c689851320fade6364e0333fb29c9b451bd3bd34bb9bacf2d08, IP: 10.110.34.88) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.110.34.88`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:33:52,833 E 3451695 3451695] (raylet) node_manager.cc:3041: 31 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 233c7c689851320fade6364e0333fb29c9b451bd3bd34bb9bacf2d08, IP: 10.110.34.88) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.110.34.88`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:34:52,836 E 3451695 3451695] (raylet) node_manager.cc:3041: 28 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 233c7c689851320fade6364e0333fb29c9b451bd3bd34bb9bacf2d08, IP: 10.110.34.88) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.110.34.88`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[36m(MultiAgentEnvRunner pid=4028541)\u001b[0m 2025-08-06 17:35:10,345\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:35:52,838 E 3451695 3451695] (raylet) node_manager.cc:3041: 19 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 233c7c689851320fade6364e0333fb29c9b451bd3bd34bb9bacf2d08, IP: 10.110.34.88) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.110.34.88`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:36:53,556 E 3451695 3451695] (raylet) node_manager.cc:3041: 4 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 233c7c689851320fade6364e0333fb29c9b451bd3bd34bb9bacf2d08, IP: 10.110.34.88) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.110.34.88`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:37:53,781 E 3451695 3451695] (raylet) node_manager.cc:3041: 45 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 233c7c689851320fade6364e0333fb29c9b451bd3bd34bb9bacf2d08, IP: 10.110.34.88) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.110.34.88`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:38:54,002 E 3451695 3451695] (raylet) node_manager.cc:3041: 53 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 233c7c689851320fade6364e0333fb29c9b451bd3bd34bb9bacf2d08, IP: 10.110.34.88) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.110.34.88`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:39:54,117 E 3451695 3451695] (raylet) node_manager.cc:3041: 50 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 233c7c689851320fade6364e0333fb29c9b451bd3bd34bb9bacf2d08, IP: 10.110.34.88) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.110.34.88`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:40:54,522 E 3451695 3451695] (raylet) node_manager.cc:3041: 48 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 233c7c689851320fade6364e0333fb29c9b451bd3bd34bb9bacf2d08, IP: 10.110.34.88) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.110.34.88`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:41:54,549 E 3451695 3451695] (raylet) node_manager.cc:3041: 47 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 233c7c689851320fade6364e0333fb29c9b451bd3bd34bb9bacf2d08, IP: 10.110.34.88) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.110.34.88`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:42:54,553 E 3451695 3451695] (raylet) node_manager.cc:3041: 52 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 233c7c689851320fade6364e0333fb29c9b451bd3bd34bb9bacf2d08, IP: 10.110.34.88) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.110.34.88`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:43:55,200 E 3451695 3451695] (raylet) node_manager.cc:3041: 54 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 233c7c689851320fade6364e0333fb29c9b451bd3bd34bb9bacf2d08, IP: 10.110.34.88) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.110.34.88`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:44:55,469 E 3451695 3451695] (raylet) node_manager.cc:3041: 55 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 233c7c689851320fade6364e0333fb29c9b451bd3bd34bb9bacf2d08, IP: 10.110.34.88) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.110.34.88`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:45:56,013 E 3451695 3451695] (raylet) node_manager.cc:3041: 56 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 233c7c689851320fade6364e0333fb29c9b451bd3bd34bb9bacf2d08, IP: 10.110.34.88) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.110.34.88`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:46:56,275 E 3451695 3451695] (raylet) node_manager.cc:3041: 57 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 233c7c689851320fade6364e0333fb29c9b451bd3bd34bb9bacf2d08, IP: 10.110.34.88) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.110.34.88`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:47:56,315 E 3451695 3451695] (raylet) node_manager.cc:3041: 56 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 233c7c689851320fade6364e0333fb29c9b451bd3bd34bb9bacf2d08, IP: 10.110.34.88) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.110.34.88`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:48:56,318 E 3451695 3451695] (raylet) node_manager.cc:3041: 49 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 233c7c689851320fade6364e0333fb29c9b451bd3bd34bb9bacf2d08, IP: 10.110.34.88) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.110.34.88`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:49:56,889 E 3451695 3451695] (raylet) node_manager.cc:3041: 56 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 233c7c689851320fade6364e0333fb29c9b451bd3bd34bb9bacf2d08, IP: 10.110.34.88) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.110.34.88`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:50:56,924 E 3451695 3451695] (raylet) node_manager.cc:3041: 59 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 233c7c689851320fade6364e0333fb29c9b451bd3bd34bb9bacf2d08, IP: 10.110.34.88) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.110.34.88`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:51:57,395 E 3451695 3451695] (raylet) node_manager.cc:3041: 60 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 233c7c689851320fade6364e0333fb29c9b451bd3bd34bb9bacf2d08, IP: 10.110.34.88) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.110.34.88`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:52:57,534 E 3451695 3451695] (raylet) node_manager.cc:3041: 56 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 233c7c689851320fade6364e0333fb29c9b451bd3bd34bb9bacf2d08, IP: 10.110.34.88) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.110.34.88`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:53:58,047 E 3451695 3451695] (raylet) node_manager.cc:3041: 46 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 233c7c689851320fade6364e0333fb29c9b451bd3bd34bb9bacf2d08, IP: 10.110.34.88) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.110.34.88`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m [2025-08-06 17:54:58,050 E 3451695 3451695] (raylet) node_manager.cc:3041: 51 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 233c7c689851320fade6364e0333fb29c9b451bd3bd34bb9bacf2d08, IP: 10.110.34.88) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.110.34.88`\n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    # Configure the RL environment to use as a string (by name), which\n",
    "    # is registered with Farama's gymnasium.\n",
    "    .environment(\"Acrobot-v1\")\n",
    ")\n",
    "algo = config.build()\n",
    "print(algo.train())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6180e18",
   "metadata": {},
   "source": [
    "## 通过 gymnasium.Env 的子类指定\n",
    "如果你正在使用自定义的 gymnasium.Env 类子类，可以直接传递该类而不是注册的字符串。你的子类必须在其构造函数中接受一个 config 参数（可以默认为 None）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc883b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-08-05 20:36:47,211 E 3440243 3440243] core_worker.cc:2740: Actor with class name: 'SingleAgentEnvRunner' and ID: '5dcad0efa0814cb66af79a4601000000' has constructor arguments in the object store and max_restarts > 0. If the arguments in the object store go out of scope or are lost, the actor restart will fail. See https://github.com/ray-project/ray/issues/53727 for more details.\n",
      "[2025-08-05 20:36:47,265 E 3440243 3440243] core_worker.cc:2740: Actor with class name: 'SingleAgentEnvRunner' and ID: '7102cf8376bed608ece878d701000000' has constructor arguments in the object store and max_restarts > 0. If the arguments in the object store go out of scope or are lost, the actor restart will fail. See https://github.com/ray-project/ray/issues/53727 for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'timers': {'training_iteration': 14.936725181993097, 'restore_env_runners': 3.324518911540508e-05, 'training_step': 14.936223768861964, 'env_runner_sampling_timer': 3.4384264659602195, 'learner_update_timer': 11.49317091004923, 'synch_weights': 0.003826322965323925}, 'env_runners': {'env_to_module_connector': {'timers': {'connectors': {'numpy_to_tensor': np.float64(6.049784700693104e-05), 'add_states_from_episodes_to_batch': np.float64(8.169158180437569e-06), 'add_time_dim_to_batch_and_zero_pad': np.float64(1.2553371301353672e-05), 'add_observations_from_episodes_to_batch': np.float64(1.514624764926376e-05), 'batch_individual_items': np.float64(3.165490147136356e-05)}}, 'connector_pipeline_timer': np.float64(0.00024245962985280779)}, 'env_to_module_sum_episodes_length_in': np.float64(1901.0000001845106), 'num_env_steps_sampled_lifetime': 4000.0, 'module_to_env_connector': {'connector_pipeline_timer': np.float64(0.000788165452446644), 'timers': {'connectors': {'remove_single_ts_time_rank_from_batch': np.float64(3.3410726479236298e-06), 'tensor_to_numpy': np.float64(9.41684598191408e-05), 'get_actions': np.float64(0.0003764795942333131), 'un_batch_to_individual_items': np.float64(3.0930159185036426e-05), 'normalize_and_clip_actions': np.float64(5.166367887581449e-05), 'listify_data_for_vector_env': np.float64(5.934755177664533e-05)}}}, 'rlmodule_inference_timer': np.float64(0.0002163299247579962), 'env_reset_timer': np.float64(0.0007444140501320362), 'num_env_steps_sampled': 4000.0, 'num_agent_steps_sampled_lifetime': {'default_agent': 4000.0}, 'weights_seq_no': 0.0, 'env_to_module_sum_episodes_length_out': np.float64(1901.0000001845106), 'num_module_steps_sampled': {'default_policy': 4000.0}, 'env_step_timer': np.float64(6.003071494284571e-05), 'num_episodes_lifetime': 0.0, 'num_episodes': 0.0, 'num_module_steps_sampled_lifetime': {'default_policy': 4000.0}, 'sample': np.float64(3.35324622457847), 'num_agent_steps_sampled': {'default_agent': 4000.0}, 'num_env_steps_sampled_lifetime_throughput': np.float64(596.534914266103)}, 'learners': {'__all_modules__': {'learner_connector': {'timers': {'connectors': {'add_states_from_episodes_to_batch': 1.1942116543650627e-05, 'add_time_dim_to_batch_and_zero_pad': 3.784196451306343e-05, 'add_observations_from_episodes_to_batch': 0.00010924693197011948, 'batch_individual_items': 0.28465009294450283, 'add_one_ts_to_episodes_and_truncate': 0.0063856730703264475, 'general_advantage_estimation': 0.03797129588201642, 'numpy_to_tensor': 0.0002898948732763529, 'add_columns_from_episodes_to_train_batch': 0.13152733212336898}}, 'connector_pipeline_timer': 0.4615087630227208}, 'learner_connector_sum_episodes_length_in': 4000, 'num_module_steps_trained_lifetime': 120064, 'num_module_steps_trained': 120064, 'num_env_steps_trained': 3753876, 'num_env_steps_trained_lifetime': 3753876, 'num_trainable_parameters': 133379, 'learner_connector_sum_episodes_length_out': 4002, 'num_non_trainable_parameters': 0, 'num_env_steps_trained_lifetime_throughput': 337321.583567467, 'num_module_steps_trained_throughput': 10792.868211138095, 'num_module_steps_trained_lifetime_throughput': 10790.72092034439}, 'default_policy': {'weights_seq_no': 1.0, 'curr_entropy_coeff': 0.0, 'vf_loss': np.float32(10.0), 'mean_kl_loss': np.float32(0.024730854), 'num_trainable_parameters': 133379, 'diff_num_grad_updates_vs_sampler_policy': np.float32(1.0), 'vf_loss_unclipped': np.float32(8831.652), 'gradients_default_optimizer_global_norm': np.float32(0.824052), 'module_train_batch_size_mean': 128.0, 'total_loss': np.float32(9.911955), 'vf_explained_var': np.float32(0.0), 'entropy': np.float32(0.67870903), 'default_optimizer_learning_rate': 5e-05, 'num_module_steps_trained_lifetime': 120064, 'num_module_steps_trained': 120064, 'curr_kl_coeff': 0.30000001192092896, 'policy_loss': np.float32(-0.09299038), 'num_module_steps_trained_lifetime_throughput': 10793.747804669229}}, 'num_training_step_calls_per_iteration': 1, 'num_env_steps_sampled_lifetime': 4000.0, 'fault_tolerance': {'num_healthy_workers': 2, 'num_remote_worker_restarts': 0}, 'env_runner_group': {'actor_manager_num_outstanding_async_reqs': 0}, 'done': False, 'training_iteration': 1, 'trial_id': 'default', 'date': '2025-08-05_20-37-06', 'timestamp': 1754397426, 'time_this_iter_s': 14.947767734527588, 'time_total_s': 14.947767734527588, 'pid': 3440243, 'hostname': 'robotarm-Precision-7920-Tower', 'node_ip': '10.110.34.88', 'config': {'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': <class '__main__.MyDummyEnv'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 0, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1, '_disable_preprocessor_api': False, '_disable_action_flattening': False}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'learner_config_dict': {}, 'optimizer': {}, '_learner_class': None, 'callbacks_on_algorithm_init': None, 'callbacks_on_env_runners_recreated': None, 'callbacks_on_offline_eval_runners_recreated': None, 'callbacks_on_checkpoint_loaded': None, 'callbacks_on_environment_created': None, 'callbacks_on_episode_created': None, 'callbacks_on_episode_start': None, 'callbacks_on_episode_step': None, 'callbacks_on_episode_end': None, 'callbacks_on_evaluate_start': None, 'callbacks_on_evaluate_end': None, 'callbacks_on_evaluate_offline_start': None, 'callbacks_on_evaluate_offline_end': None, 'callbacks_on_sample_end': None, 'callbacks_on_train_result': None, 'explore': True, 'enable_rl_module_and_learner': True, 'enable_env_runner_and_connector_v2': True, '_prior_exploration_config': {'type': 'StochasticSampling'}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x7b71e46303a0>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'offline_data_class': None, 'input_read_method': 'read_parquet', 'input_read_method_kwargs': {}, 'input_read_schema': {}, 'input_read_episodes': False, 'input_read_sample_batches': False, 'input_read_batch_size': None, 'input_filesystem': None, 'input_filesystem_kwargs': {}, 'input_compress_columns': ['obs', 'new_obs'], 'input_spaces_jsonable': True, 'materialize_data': False, 'materialize_mapped_data': True, 'map_batches_kwargs': {}, 'iter_batches_kwargs': {}, 'ignore_final_observation': False, 'prelearner_class': None, 'prelearner_buffer_class': None, 'prelearner_buffer_kwargs': {}, 'prelearner_module_synch_period': 10, 'dataset_num_iters_per_learner': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'output_max_rows_per_file': None, 'output_write_remaining_data': False, 'output_write_method': 'write_parquet', 'output_write_method_kwargs': {}, 'output_filesystem': None, 'output_filesystem_kwargs': {}, 'output_write_episodes': True, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_auto_duration_min_env_steps_per_sample': 100, 'evaluation_auto_duration_max_env_steps_per_sample': 2000, 'evaluation_parallel_to_training': False, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 0, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'offline_evaluation_interval': None, 'num_offline_eval_runners': 0, 'offline_evaluation_type': None, 'offline_eval_runner_class': None, 'offline_loss_for_module_fn': None, 'offline_evaluation_duration': 1, 'offline_evaluation_parallel_to_training': False, 'offline_evaluation_timeout_s': 120.0, 'num_cpus_per_offline_eval_runner': 1, 'num_gpus_per_offline_eval_runner': 0, 'custom_resources_per_offline_eval_runner': {}, 'restart_failed_offline_eval_runners': True, 'ignore_offline_eval_runner_failures': False, 'max_num_offline_eval_runner_restarts': 1000, 'offline_eval_runner_restore_timeout_s': 1800.0, 'max_requests_in_flight_per_offline_eval_runner': 1, 'validate_offline_eval_runners_after_construction': True, 'offline_eval_runner_health_probe_timeout_s': 30.0, 'offline_eval_rl_module_inference_only': False, 'broadcast_offline_eval_runner_states': False, 'offline_eval_batch_size_per_runner': 256, 'dataset_num_iters_per_eval_runner': 1, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'log_gradients': True, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'restart_failed_env_runners': True, 'ignore_env_runner_failures': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30.0, 'env_runner_restore_timeout_s': 1800.0, '_model_config': {}, '_rl_module_spec': None, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, '_validate_config': True, '_use_msgpack_checkpoints': False, '_torch_grad_scaler_class': None, '_torch_lr_scheduler_classes': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, '_dont_auto_sync_env_runner_states': False, 'env_task_fn': -1, 'enable_connectors': -1, 'simple_optimizer': False, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'always_attach_evaluation_results': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'entropy_coeff_schedule': None, 'lr_schedule': None, 'sgd_minibatch_size': -1, 'vf_share_layers': -1, 'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>, 'lambda': 1.0, 'input': 'sampler', 'policies': {'default_policy': (None, None, None, None)}, 'callbacks': <class 'ray.rllib.callbacks.callbacks.RLlibCallback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch'}, 'time_since_restore': 14.947767734527588, 'iterations_since_restore': 1, 'perf': {'cpu_util_percent': np.float64(50.205), 'ram_util_percent': np.float64(88.35000000000001), 'gpu_util_percent0': np.float64(0.15099999999999997), 'vram_util_percent0': np.float64(0.1832275390625), 'gpu_util_percent1': np.float64(0.0), 'vram_util_percent1': np.float64(0.0005699397492265105)}}\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "class MyDummyEnv(gym.Env):\n",
    "    # Write the constructor and provide a single `config` arg,\n",
    "    # which may be set to None by default.\n",
    "    def __init__(self, config=None):\n",
    "        # As per gymnasium standard, provide observation and action spaces in your\n",
    "        # constructor.\n",
    "        self.observation_space = gym.spaces.Box(-1.0, 1.0, (1,), np.float32)\n",
    "        self.action_space = gym.spaces.Discrete(2)\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        # Return (reset) observation and info dict.\n",
    "        return np.array([1.0]), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        # Return next observation, reward, terminated, truncated, and info dict.\n",
    "        return np.array([1.0]), 1.0, False, False, {}\n",
    "\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(\n",
    "        MyDummyEnv,\n",
    "        env_config={},  # `config` to pass to your env class\n",
    "    )\n",
    ")\n",
    "algo = config.build()\n",
    "print(algo.train())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f495694",
   "metadata": {},
   "source": [
    "## 通过 Tune 注册的 Lambda 指定\n",
    "向配置提供环境信息的第三种选项是使用 Ray Tune 注册一个环境创建函数（或 lambda）。该创建函数必须接受一个 config 参数，并返回一个非向量化的 gymnasium.Env 实例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6bf4654",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-08-05 20:42:47,703 E 3440243 3440243] core_worker.cc:2740: Actor with class name: 'SingleAgentEnvRunner' and ID: '34c0939bd11be68beb64254701000000' has constructor arguments in the object store and max_restarts > 0. If the arguments in the object store go out of scope or are lost, the actor restart will fail. See https://github.com/ray-project/ray/issues/53727 for more details.\n",
      "[2025-08-05 20:42:47,770 E 3440243 3440243] core_worker.cc:2740: Actor with class name: 'SingleAgentEnvRunner' and ID: '5e09e1d961f5a14a5c03403001000000' has constructor arguments in the object store and max_restarts > 0. If the arguments in the object store go out of scope or are lost, the actor restart will fail. See https://github.com/ray-project/ray/issues/53727 for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'timers': {'training_iteration': 15.026597093092278, 'restore_env_runners': 2.874014899134636e-05, 'training_step': 15.02614946803078, 'env_runner_sampling_timer': 3.465104885166511, 'learner_update_timer': 11.557350561022758, 'synch_weights': 0.0030920300632715225}, 'env_runners': {'env_to_module_connector': {'timers': {'connectors': {'numpy_to_tensor': np.float64(5.9179183663791455e-05), 'add_states_from_episodes_to_batch': np.float64(7.904484803943991e-06), 'add_time_dim_to_batch_and_zero_pad': np.float64(1.2192654074200906e-05), 'add_observations_from_episodes_to_batch': np.float64(1.4849176669030723e-05), 'batch_individual_items': np.float64(3.120312120068458e-05)}}, 'connector_pipeline_timer': np.float64(0.0002350004545787404)}, 'env_to_module_sum_episodes_length_in': np.float64(1901.0000001845106), 'num_env_steps_sampled_lifetime': 4000.0, 'module_to_env_connector': {'connector_pipeline_timer': np.float64(0.0008395126778025231), 'timers': {'connectors': {'remove_single_ts_time_rank_from_batch': np.float64(3.5864180921883116e-06), 'tensor_to_numpy': np.float64(9.078520932687185e-05), 'get_actions': np.float64(0.0004307592907748011), 'un_batch_to_individual_items': np.float64(3.0450553862119493e-05), 'normalize_and_clip_actions': np.float64(4.957297717591003e-05), 'listify_data_for_vector_env': np.float64(5.7550863482118316e-05)}}}, 'rlmodule_inference_timer': np.float64(0.00023559463161445051), 'env_reset_timer': np.float64(0.0008857869543135165), 'num_env_steps_sampled': 4000.0, 'num_agent_steps_sampled_lifetime': {'default_agent': 4000.0}, 'weights_seq_no': 0.0, 'env_to_module_sum_episodes_length_out': np.float64(1901.0000001845106), 'num_module_steps_sampled': {'default_policy': 4000.0}, 'env_step_timer': np.float64(5.907559568971786e-05), 'num_episodes_lifetime': 0.0, 'num_episodes': 0.0, 'num_module_steps_sampled_lifetime': {'default_policy': 4000.0}, 'sample': np.float64(3.373273729579523), 'num_agent_steps_sampled': {'default_agent': 4000.0}, 'num_env_steps_sampled_lifetime_throughput': np.float64(593.0354083634904)}, 'learners': {'__all_modules__': {'learner_connector': {'timers': {'connectors': {'add_states_from_episodes_to_batch': 1.1925119906663895e-05, 'add_time_dim_to_batch_and_zero_pad': 3.793579526245594e-05, 'add_observations_from_episodes_to_batch': 0.00011315988376736641, 'batch_individual_items': 0.09951670002192259, 'add_one_ts_to_episodes_and_truncate': 0.009366386104375124, 'general_advantage_estimation': 0.042760077863931656, 'numpy_to_tensor': 0.0002978718839585781, 'add_columns_from_episodes_to_train_batch': 0.13231000304222107}}, 'connector_pipeline_timer': 0.28508993284776807}, 'learner_connector_sum_episodes_length_in': 4000, 'num_module_steps_trained_lifetime': 120064, 'num_module_steps_trained': 120064, 'num_env_steps_trained': 3753876, 'num_env_steps_trained_lifetime': 3753876, 'num_trainable_parameters': 133379, 'learner_connector_sum_episodes_length_out': 4002, 'num_non_trainable_parameters': 0, 'num_env_steps_trained_lifetime_throughput': 327639.7108532457, 'num_module_steps_trained_throughput': 10478.884926103212, 'num_module_steps_trained_lifetime_throughput': 10478.973538198865}, 'default_policy': {'weights_seq_no': 1.0, 'curr_entropy_coeff': 0.0, 'vf_loss': np.float32(10.0), 'mean_kl_loss': np.float32(0.0020405261), 'num_trainable_parameters': 133379, 'diff_num_grad_updates_vs_sampler_policy': np.float32(1.0), 'vf_loss_unclipped': np.float32(9008.832), 'gradients_default_optimizer_global_norm': np.float32(0.43607286), 'module_train_batch_size_mean': 128.0, 'total_loss': np.float32(9.802702), 'vf_explained_var': np.float32(5.9604645e-08), 'entropy': np.float32(0.6684122), 'default_optimizer_learning_rate': 5e-05, 'num_module_steps_trained_lifetime': 120064, 'num_module_steps_trained': 120064, 'curr_kl_coeff': 0.10000000149011612, 'policy_loss': np.float32(-0.19770557), 'num_module_steps_trained_lifetime_throughput': 10479.20491741848}}, 'num_training_step_calls_per_iteration': 1, 'num_env_steps_sampled_lifetime': 4000.0, 'fault_tolerance': {'num_healthy_workers': 2, 'num_remote_worker_restarts': 0}, 'env_runner_group': {'actor_manager_num_outstanding_async_reqs': 0}, 'done': False, 'training_iteration': 1, 'trial_id': 'default', 'date': '2025-08-05_20-43-06', 'timestamp': 1754397786, 'time_this_iter_s': 15.033698081970215, 'time_total_s': 15.033698081970215, 'pid': 3440243, 'hostname': 'robotarm-Precision-7920-Tower', 'node_ip': '10.110.34.88', 'config': {'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'my_env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 0, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1, '_disable_preprocessor_api': False, '_disable_action_flattening': False}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'learner_config_dict': {}, 'optimizer': {}, '_learner_class': None, 'callbacks_on_algorithm_init': None, 'callbacks_on_env_runners_recreated': None, 'callbacks_on_offline_eval_runners_recreated': None, 'callbacks_on_checkpoint_loaded': None, 'callbacks_on_environment_created': None, 'callbacks_on_episode_created': None, 'callbacks_on_episode_start': None, 'callbacks_on_episode_step': None, 'callbacks_on_episode_end': None, 'callbacks_on_evaluate_start': None, 'callbacks_on_evaluate_end': None, 'callbacks_on_evaluate_offline_start': None, 'callbacks_on_evaluate_offline_end': None, 'callbacks_on_sample_end': None, 'callbacks_on_train_result': None, 'explore': True, 'enable_rl_module_and_learner': True, 'enable_env_runner_and_connector_v2': True, '_prior_exploration_config': {'type': 'StochasticSampling'}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x7b71e46303a0>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'offline_data_class': None, 'input_read_method': 'read_parquet', 'input_read_method_kwargs': {}, 'input_read_schema': {}, 'input_read_episodes': False, 'input_read_sample_batches': False, 'input_read_batch_size': None, 'input_filesystem': None, 'input_filesystem_kwargs': {}, 'input_compress_columns': ['obs', 'new_obs'], 'input_spaces_jsonable': True, 'materialize_data': False, 'materialize_mapped_data': True, 'map_batches_kwargs': {}, 'iter_batches_kwargs': {}, 'ignore_final_observation': False, 'prelearner_class': None, 'prelearner_buffer_class': None, 'prelearner_buffer_kwargs': {}, 'prelearner_module_synch_period': 10, 'dataset_num_iters_per_learner': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'output_max_rows_per_file': None, 'output_write_remaining_data': False, 'output_write_method': 'write_parquet', 'output_write_method_kwargs': {}, 'output_filesystem': None, 'output_filesystem_kwargs': {}, 'output_write_episodes': True, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_auto_duration_min_env_steps_per_sample': 100, 'evaluation_auto_duration_max_env_steps_per_sample': 2000, 'evaluation_parallel_to_training': False, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 0, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'offline_evaluation_interval': None, 'num_offline_eval_runners': 0, 'offline_evaluation_type': None, 'offline_eval_runner_class': None, 'offline_loss_for_module_fn': None, 'offline_evaluation_duration': 1, 'offline_evaluation_parallel_to_training': False, 'offline_evaluation_timeout_s': 120.0, 'num_cpus_per_offline_eval_runner': 1, 'num_gpus_per_offline_eval_runner': 0, 'custom_resources_per_offline_eval_runner': {}, 'restart_failed_offline_eval_runners': True, 'ignore_offline_eval_runner_failures': False, 'max_num_offline_eval_runner_restarts': 1000, 'offline_eval_runner_restore_timeout_s': 1800.0, 'max_requests_in_flight_per_offline_eval_runner': 1, 'validate_offline_eval_runners_after_construction': True, 'offline_eval_runner_health_probe_timeout_s': 30.0, 'offline_eval_rl_module_inference_only': False, 'broadcast_offline_eval_runner_states': False, 'offline_eval_batch_size_per_runner': 256, 'dataset_num_iters_per_eval_runner': 1, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'log_gradients': True, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'restart_failed_env_runners': True, 'ignore_env_runner_failures': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30.0, 'env_runner_restore_timeout_s': 1800.0, '_model_config': {}, '_rl_module_spec': None, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, '_validate_config': True, '_use_msgpack_checkpoints': False, '_torch_grad_scaler_class': None, '_torch_lr_scheduler_classes': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, '_dont_auto_sync_env_runner_states': False, 'env_task_fn': -1, 'enable_connectors': -1, 'simple_optimizer': False, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'always_attach_evaluation_results': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'entropy_coeff_schedule': None, 'lr_schedule': None, 'sgd_minibatch_size': -1, 'vf_share_layers': -1, 'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>, 'lambda': 1.0, 'input': 'sampler', 'policies': {'default_policy': (None, None, None, None)}, 'callbacks': <class 'ray.rllib.callbacks.callbacks.RLlibCallback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch'}, 'time_since_restore': 15.033698081970215, 'iterations_since_restore': 1, 'perf': {'cpu_util_percent': np.float64(55.605), 'ram_util_percent': np.float64(87.88), 'gpu_util_percent0': np.float64(0.3235), 'vram_util_percent0': np.float64(0.181103515625), 'gpu_util_percent1': np.float64(0.0), 'vram_util_percent1': np.float64(0.0005699397492265105)}}\n"
     ]
    }
   ],
   "source": [
    "from ray.tune.registry import register_env\n",
    "\n",
    "def env_creator(config):\n",
    "    return MyDummyEnv(config)  # Return a gymnasium.Env instance.\n",
    "\n",
    "register_env(\"my_env\", env_creator)\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(\"my_env\")  # <- Tune registered string pointing to your custom env creator.\n",
    ")\n",
    "algo = config.build()\n",
    "print(algo.train())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f91c95",
   "metadata": {},
   "source": [
    "在前面的示例中，env_creator 函数接受一个 config 参数。此配置主要是一个包含所需设置的字典。然而，你也可以在 config 变量中访问其他属性。例如，使用 config.worker_index 获取远程 EnvRunner 索引，或使用 config.num_workers 获取使用的 EnvRunner 总数。这种方法有助于自定义集成中的环境，并使在某些 EnvRunner 上运行的环境与在其他 EnvRunner 上运行的环境表现不同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1775bc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import Env\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "def choose_env_for(worker_index: int, vector_index: int) -> str:\n",
    "    \"\"\"根据 worker_index 和 vector_index 返回对应环境 ID\"\"\"\n",
    "    # 示例：交替返回两个内置环境\n",
    "    if (worker_index + vector_index) % 2 == 0:\n",
    "        return \"CartPole-v1\"\n",
    "    return \"MountainCarContinuous-v0\"\n",
    "\n",
    "class EnvDependingOnWorkerAndVectorIndex(gym.Env):\n",
    "    def __init__(self, config):\n",
    "        # Pick actual env based on worker and env indexes.\n",
    "        self.env = gym.make(\n",
    "            choose_env_for(config.worker_index, config.vector_index)\n",
    "        )\n",
    "        self.action_space = self.env.action_space\n",
    "        self.observation_space = self.env.observation_space\n",
    "\n",
    "    def reset(self, seed, options):\n",
    "        return self.env.reset(seed, options)\n",
    "\n",
    "    def step(self, action):\n",
    "        return self.env.step(action)\n",
    "\n",
    "register_env(\"multi_env\", lambda config: EnvDependingOnWorkerAndVectorIndex(config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8b705a",
   "metadata": {},
   "source": [
    "# 多智能体环境\n",
    "可以使用几种不同的策略网络来控制各种智能体。因此，环境中的每个智能体都映射到恰好一个特定的策略。这种映射由用户提供的函数决定，称为“映射函数”。请注意，如果存在映射到 M 个策略的 N 个智能体，则 N 总是大于或等于 M，允许任何策略控制多个智能体。\n",
    "\n",
    "多智能体设置： N 个智能体位于环境中，并执行由 M 个策略网络计算出的动作。智能体到策略的映射是灵活的，由用户提供的映射函数决定。这里，agent_1 和 agent_3 都映射到 policy_1，而 agent_2 映射到 policy_2。\n",
    "\n",
    "## RLlib 的 MultiAgentEnv API\n",
    "\n",
    "RLlib 的 :py:class`~ray.rllib.env.multi_agent_env.MultiAgentEnv` API 紧密遵循 Farama 的 gymnasium（单智能体）环境的约定和 API，甚至继承自 gymnasium.Env。然而，自定义的 :py:class`~ray.rllib.env.multi_agent_env.MultiAgentEnv` 实现不是从 reset() 和 step() 发布单个观察、奖励以及终止/截断标志，而是输出字典，一个用于观察，一个用于奖励，等等。在每个这样的多智能体字典中，智能体 ID 映射到各自的单个智能体的观察/奖励/等。\n",
    "\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "\n",
    "class MyMultiAgentEnv(MultiAgentEnv):\n",
    "\n",
    "    def __init__(self, config=None):\n",
    "        super().__init__()\n",
    "        ...\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        ...\n",
    "        # return observation dict and infos dict.\n",
    "        return {\"agent_1\": [obs of agent_1], \"agent_2\": [obs of agent_2]}, {}\n",
    "\n",
    "    def step(self, action_dict):\n",
    "        # return observation dict, rewards dict, termination/truncation dicts, and infos dict\n",
    "        return {\"agent_1\": [obs of agent_1]}, {...}, ...\n",
    "\n",
    "\n",
    "## 智能体定义\n",
    "\n",
    "环境中的智能体数量及其 ID 完全由您的 :py:class`~ray.rllib.env.multi_agent_env.MultiAgentEnv` 代码控制。您的环境决定哪些智能体在 episode 重置后开始，哪些智能体稍后进入 episode，哪些智能体提前终止 episode，以及哪些智能体留在 episode 中直到整个 episode 结束。\n",
    "\n",
    "def __init__(self, config=None):\n",
    "    super().__init__()\n",
    "    ...\n",
    "    # Define all agent IDs that might even show up in your episodes.\n",
    "    self.possible_agents = [\"agent_1\", \"agent_2\"]\n",
    "    ...\n",
    "\n",
    "\n",
    "如果您的环境仅以部分智能体 ID 开始和/或在 episode 结束前终止部分智能体 ID，您还需要在整个 episode 过程中永久调整 self.agents 属性。另一方面，如果所有智能体 ID 在您的 episode 中是静态的，您可以将 self.agents 设置为与 self.possible_agents 相同，并且在您代码的其余部分不更改其值。\n",
    "\n",
    "\n",
    "def __init__(self, config=None):\n",
    "    super().__init__()\n",
    "    ...\n",
    "    # If your agents never change throughout the episode, set\n",
    "    # `self.agents` to the same list as `self.possible_agents`.\n",
    "    self.agents = self.possible_agents = [\"agent_1\", \"agent_2\"]\n",
    "    # Otherwise, you will have to adjust `self.agents` in `reset()` and `step()` to whatever the\n",
    "    # currently \"alive\" agents are.\n",
    "    ...\n",
    "\n",
    "## 观察空间和动作空间\n",
    "接下来，您应该在构造函数中设置每个（可能）智能体 ID 的观察空间和动作空间。使用 self.observation_spaces 和 self.action_spaces 属性来定义将智能体 ID 映射到各个智能体空间的字典。例如\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "...\n",
    "\n",
    "    def __init__(self, config=None):\n",
    "        super().__init__()\n",
    "        ...\n",
    "        self.observation_spaces = {\n",
    "            \"agent_1\": gym.spaces.Box(-1.0, 1.0, (4,), np.float32),\n",
    "            \"agent_2\": gym.spaces.Box(-1.0, 1.0, (3,), np.float32),\n",
    "        }\n",
    "        self.action_spaces = {\n",
    "            \"agent_1\": gym.spaces.Discrete(2),\n",
    "            \"agent_2\": gym.spaces.Box(0.0, 1.0, (1,), np.float32),\n",
    "        }\n",
    "        ...\n",
    "\n",
    "\n",
    "如果您的 episode 包含大量智能体，其中一些共享相同的观察空间或动作空间，并且您不想创建非常大的空间字典，您还可以覆盖 get_observation_space() 和 get_action_space() 方法，并自己实现从智能体 ID 到空间的映射逻辑。例如\n",
    "\n",
    "def get_observation_space(self, agent_id):\n",
    "    if agent_id.startswith(\"robot_\"):\n",
    "        return gym.spaces.Box(0, 255, (84, 84, 3), np.uint8)\n",
    "    elif agent_id.startswith(\"decision_maker\"):\n",
    "        return gym.spaces.Discrete(2)\n",
    "    else:\n",
    "        raise ValueError(f\"bad agent id: {agent_id}!\")\n",
    "\n",
    "\n",
    "## 观察、奖励和终止字典\n",
    "\n",
    "在自定义的 MultiAgentEnv 中，您还需要实现 reset() 和 step() 方法。与单智能体 gymnasium.Env 类似，您需要从 reset() 返回观察和信息，并从 step() 返回观察、奖励、终止/截断标志和信息。然而，这些返回值不再是单个值，而必须是字典，将智能体 ID 映射到各个智能体的对应值。\n",
    "\n",
    "def reset(self, *, seed=None, options=None):\n",
    "    ...\n",
    "    return {\n",
    "        \"agent_1\": np.array([0.0, 1.0, 0.0, 0.0]),\n",
    "        \"agent_2\": np.array([0.0, 0.0, 1.0]),\n",
    "    }, {}  # <- empty info dict\n",
    "\n",
    "\n",
    "智能体同时行动的环境： 两个智能体在每个时间步都收到其观察结果，包括紧随 reset() 之后。请注意，每当返回的观察字典中存在该智能体的观察时，该智能体必须计算并发送一个动作到下一次 step() 调用中。\n",
    "\n",
    "智能体轮流行动的环境： 两个智能体通过轮流行动。 agent_1 在 reset() 后收到第一个观察结果，因此必须首先计算并发送一个动作。接收到此动作后，环境返回 agent_2 的观察结果，此时 agent_2 需要行动。接收到 agent_2 的动作后，环境返回 agent_1 的下一个观察结果，依此类推。\n",
    "\n",
    "回合顺序复杂的环境： 三个智能体以看似混乱的顺序行动。 agent_1 和 agent_3 在 reset() 后收到其初始观察结果，因此必须首先计算并发送动作。接收到这两个动作后，环境返回 agent_1 和 agent_2 的观察结果，此时它们必须同时行动。接收到 agent_1 和 agent_2 的动作后，环境返回 agent_2 和 agent_3 的观察结果，依此类推。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae278a0e",
   "metadata": {},
   "source": [
    "## 智能体同时行动的环境\n",
    "智能体总是同时行动的多智能体环境的一个很好的简单示例是石头剪刀布游戏，其中两个智能体总共需要进行 N 次移动，每次都在“石头”、“剪刀”或“布”动作中选择。每次移动后，比较动作选择。石头胜剪刀，布胜石头，剪刀胜布。赢得移动的玩家获得 +1 奖励，输家获得 -1 奖励。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "524f9252",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-08-06 15:48:33,695 E 3440243 3440243] core_worker.cc:2740: Actor with class name: 'MultiAgentEnvRunner' and ID: '9d7e0511acc21bb39d1fa18501000000' has constructor arguments in the object store and max_restarts > 0. If the arguments in the object store go out of scope or are lost, the actor restart will fail. See https://github.com/ray-project/ray/issues/53727 for more details.\n",
      "[2025-08-06 15:48:33,742 E 3440243 3440243] core_worker.cc:2740: Actor with class name: 'MultiAgentEnvRunner' and ID: '5fe12fc06b62e06d2a123a6201000000' has constructor arguments in the object store and max_restarts > 0. If the arguments in the object store go out of scope or are lost, the actor restart will fail. See https://github.com/ray-project/ray/issues/53727 for more details.\n",
      "2025-08-06 15:48:48,037\tERROR actor_manager.py:873 -- Ray error (The actor 5fe12fc06b62e06d2a123a6201000000 is unavailable: The actor is temporarily unavailable: IOError: The actor was restarted. The task may or may not have been executed on the actor.), taking actor 2 out of service.\n",
      "2025-08-06 15:48:48,038\tERROR actor_manager.py:674 -- The actor 5fe12fc06b62e06d2a123a6201000000 is unavailable: The actor is temporarily unavailable: IOError: The actor was restarted. The task may or may not have been executed on the actor.\n",
      "NoneType: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'timers': {'training_iteration': 40.02873206604272, 'restore_env_runners': 2.35771294683218e-05, 'training_step': 40.028292561881244, 'env_runner_sampling_timer': 16.731119564035907, 'learner_update_timer': 23.292531518032774, 'synch_weights': 0.004503910895437002}, 'env_runners': {'agent_episode_returns_mean': {'player2': -0.16, 'player1': 0.16}, 'env_to_module_connector': {'timers': {'connectors': {'numpy_to_tensor': np.float64(9.567372640827856e-05), 'add_states_from_episodes_to_batch': np.float64(1.4088377370608972e-05), 'add_time_dim_to_batch_and_zero_pad': np.float64(2.081415253709641e-05), 'add_observations_from_episodes_to_batch': np.float64(4.183911714907247e-05), 'batch_individual_items': np.float64(4.8569046073058694e-05), 'flatten_observations': np.float64(0.00014957863420221948), 'agent_to_module_mapping': np.float64(1.0993880391283682e-05)}}, 'connector_pipeline_timer': np.float64(0.0005539883795363839)}, 'env_to_module_sum_episodes_length_in': np.float64(4.145023788250812), 'episode_return_min': 0.0, 'num_env_steps_sampled_lifetime': 6000.0, 'num_module_steps_sampled': {'player2': 4000.0, 'player1': 4000.0}, 'time_between_sampling': 0.19786425190977752, 'episode_duration_sec_mean': 0.04161873042583466, 'num_module_steps_sampled_lifetime': {'player1': 6000.0, 'player2': 6000.0}, 'module_to_env_connector': {'connector_pipeline_timer': np.float64(0.0012971835938024334), 'timers': {'connectors': {'remove_single_ts_time_rank_from_batch': np.float64(3.669227284716123e-06), 'module_to_agent_unmapping': np.float64(1.000819854328876e-05), 'tensor_to_numpy': np.float64(0.00015817016022066119), 'get_actions': np.float64(0.0007404102375777913), 'un_batch_to_individual_items': np.float64(6.25400458002063e-05), 'normalize_and_clip_actions': np.float64(8.273123634466788e-05), 'listify_data_for_vector_env': np.float64(2.302933861197388e-05)}}}, 'episode_len_max': 10, 'num_agent_steps_sampled': {'player1': 4000.0, 'player2': 4000.0}, 'rlmodule_inference_timer': np.float64(0.00046373216612851856), 'env_reset_timer': np.float64(7.108505815267563e-05), 'timers': {'connectors': {'add_states_from_episodes_to_batch': np.float64(2.8077978640794754e-05), 'add_time_dim_to_batch_and_zero_pad': np.float64(4.63658943772316e-05), 'add_observations_from_episodes_to_batch': np.float64(9.190896525979042e-05), 'batch_individual_items': np.float64(0.0001081470400094986), 'flatten_observations': np.float64(0.000576007878407836), 'agent_to_module_mapping': np.float64(1.8635066226124763e-05), 'numpy_to_tensor': np.float64(0.00017563393339514732)}}, 'num_env_steps_sampled': 4000.0, 'agent_steps': {'player1': 10.0, 'player2': 10.0}, 'episode_return_mean': 0.0, 'weights_seq_no': 0.0, 'env_to_module_sum_episodes_length_out': np.float64(4.145023788250812), 'episode_len_min': 10, 'env_step_timer': np.float64(5.246195459713224e-05), 'episode_return_max': 0.0, 'num_agent_steps_sampled_lifetime': {'player1': 6000.0, 'player2': 6000.0}, 'num_episodes_lifetime': 600.0, 'num_episodes': 400.0, 'connector_pipeline_timer': np.float64(0.0019618289079517126), 'module_episode_returns_mean': {'player1': 0.16, 'player2': -0.16}, 'episode_len_mean': 10.0, 'sample': np.float64(8.495583494878373), 'num_env_steps_sampled_lifetime_throughput': np.float64(240.2853078343223)}, 'learners': {'player2': {'num_trainable_parameters': 134660, 'diff_num_grad_updates_vs_sampler_policy': np.float32(1.0), 'vf_loss_unclipped': np.float32(3.782137), 'default_optimizer_learning_rate': 5e-05, 'mean_kl_loss': np.float32(0.015516646), 'vf_loss': np.float32(3.028769), 'vf_explained_var': np.float32(0.010058224), 'module_train_batch_size_mean': 128.0, 'total_loss': np.float32(2.8972964), 'num_module_steps_trained_lifetime': 132096, 'curr_kl_coeff': 0.20000000298023224, 'curr_entropy_coeff': 0.0, 'gradients_default_optimizer_global_norm': np.float32(2.4507644), 'entropy': np.float32(1.0766664), 'num_module_steps_trained': 132096, 'policy_loss': np.float32(-0.13457574), 'weights_seq_no': 1.0, 'num_module_steps_trained_lifetime_throughput': 5961.4233326254525}, 'player1': {'entropy': np.float32(1.0621932), 'num_module_steps_trained': 132096, 'weights_seq_no': 1.0, 'num_trainable_parameters': 134660, 'diff_num_grad_updates_vs_sampler_policy': np.float32(1.0), 'vf_loss_unclipped': np.float32(3.420869), 'default_optimizer_learning_rate': 5e-05, 'mean_kl_loss': np.float32(0.023522269), 'vf_loss': np.float32(2.8353014), 'vf_explained_var': np.float32(0.00025886297), 'curr_kl_coeff': 0.30000001192092896, 'curr_entropy_coeff': 0.0, 'module_train_batch_size_mean': 128.0, 'total_loss': np.float32(2.750733), 'num_module_steps_trained_lifetime': 132096, 'policy_loss': np.float32(-0.08927326), 'gradients_default_optimizer_global_norm': np.float32(2.1285172), 'num_module_steps_trained_lifetime_throughput': 5961.741083749154}, '__all_modules__': {'learner_connector': {'timers': {'connectors': {'add_one_ts_to_episodes_and_truncate': 0.10158118698745966, 'agent_to_module_mapping': 0.018963858019560575, 'numpy_to_tensor': 0.0005045800935477018, 'add_time_dim_to_batch_and_zero_pad': 5.961698479950428e-05, 'add_observations_from_episodes_to_batch': 0.015013654017820954, 'batch_individual_items': 0.18420203309506178, 'general_advantage_estimation': 0.06951017607934773, 'add_columns_from_episodes_to_train_batch': 0.29995923093520105, 'add_states_from_episodes_to_batch': 2.4145934730768204e-05}}, 'connector_pipeline_timer': 0.6906587400007993}, 'learner_connector_sum_episodes_length_in': 4000, 'num_trainable_parameters': 269320, 'num_module_steps_trained': 264192, 'num_env_steps_trained': 4128000, 'learner_connector_sum_episodes_length_out': 4000, 'num_non_trainable_parameters': 0, 'num_module_steps_trained_lifetime': 264192, 'num_env_steps_trained_lifetime': 4128000, 'num_env_steps_trained_lifetime_throughput': 186312.79166670056, 'num_module_steps_trained_throughput': 319030.03015191975, 'num_module_steps_trained_lifetime_throughput': 331470.71560317057}}, 'num_training_step_calls_per_iteration': 1, 'num_env_steps_sampled_lifetime': 6000.0, 'fault_tolerance': {'num_healthy_workers': 1, 'num_remote_worker_restarts': 0}, 'env_runner_group': {'actor_manager_num_outstanding_async_reqs': 0}, 'done': False, 'training_iteration': 1, 'trial_id': 'default', 'date': '2025-08-06_15-49-19', 'timestamp': 1754466559, 'time_this_iter_s': 40.04099631309509, 'time_total_s': 40.04099631309509, 'pid': 3440243, 'hostname': 'robotarm-Precision-7920-Tower', 'node_ip': '10.110.34.88', 'config': {'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': <class '__main__.RockPaperScissors'>, 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': <function <lambda> at 0x7b717aed5ea0>, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 0, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1, '_disable_preprocessor_api': False, '_disable_action_flattening': False}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'learner_config_dict': {}, 'optimizer': {}, '_learner_class': None, 'callbacks_on_algorithm_init': None, 'callbacks_on_env_runners_recreated': None, 'callbacks_on_offline_eval_runners_recreated': None, 'callbacks_on_checkpoint_loaded': None, 'callbacks_on_environment_created': None, 'callbacks_on_episode_created': None, 'callbacks_on_episode_start': None, 'callbacks_on_episode_step': None, 'callbacks_on_episode_end': None, 'callbacks_on_evaluate_start': None, 'callbacks_on_evaluate_end': None, 'callbacks_on_evaluate_offline_start': None, 'callbacks_on_evaluate_offline_end': None, 'callbacks_on_sample_end': None, 'callbacks_on_train_result': None, 'explore': True, 'enable_rl_module_and_learner': True, 'enable_env_runner_and_connector_v2': True, '_prior_exploration_config': {'type': 'StochasticSampling'}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function <lambda> at 0x7b717aed6170>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'offline_data_class': None, 'input_read_method': 'read_parquet', 'input_read_method_kwargs': {}, 'input_read_schema': {}, 'input_read_episodes': False, 'input_read_sample_batches': False, 'input_read_batch_size': None, 'input_filesystem': None, 'input_filesystem_kwargs': {}, 'input_compress_columns': ['obs', 'new_obs'], 'input_spaces_jsonable': True, 'materialize_data': False, 'materialize_mapped_data': True, 'map_batches_kwargs': {}, 'iter_batches_kwargs': {}, 'ignore_final_observation': False, 'prelearner_class': None, 'prelearner_buffer_class': None, 'prelearner_buffer_kwargs': {}, 'prelearner_module_synch_period': 10, 'dataset_num_iters_per_learner': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'output_max_rows_per_file': None, 'output_write_remaining_data': False, 'output_write_method': 'write_parquet', 'output_write_method_kwargs': {}, 'output_filesystem': None, 'output_filesystem_kwargs': {}, 'output_write_episodes': True, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_auto_duration_min_env_steps_per_sample': 100, 'evaluation_auto_duration_max_env_steps_per_sample': 2000, 'evaluation_parallel_to_training': False, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 0, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'offline_evaluation_interval': None, 'num_offline_eval_runners': 0, 'offline_evaluation_type': None, 'offline_eval_runner_class': None, 'offline_loss_for_module_fn': None, 'offline_evaluation_duration': 1, 'offline_evaluation_parallel_to_training': False, 'offline_evaluation_timeout_s': 120.0, 'num_cpus_per_offline_eval_runner': 1, 'num_gpus_per_offline_eval_runner': 0, 'custom_resources_per_offline_eval_runner': {}, 'restart_failed_offline_eval_runners': True, 'ignore_offline_eval_runner_failures': False, 'max_num_offline_eval_runner_restarts': 1000, 'offline_eval_runner_restore_timeout_s': 1800.0, 'max_requests_in_flight_per_offline_eval_runner': 1, 'validate_offline_eval_runners_after_construction': True, 'offline_eval_runner_health_probe_timeout_s': 30.0, 'offline_eval_rl_module_inference_only': False, 'broadcast_offline_eval_runner_states': False, 'offline_eval_batch_size_per_runner': 256, 'dataset_num_iters_per_eval_runner': 1, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'log_gradients': True, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'restart_failed_env_runners': True, 'ignore_env_runner_failures': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30.0, 'env_runner_restore_timeout_s': 1800.0, '_model_config': {}, '_rl_module_spec': None, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, '_validate_config': True, '_use_msgpack_checkpoints': False, '_torch_grad_scaler_class': None, '_torch_lr_scheduler_classes': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, '_dont_auto_sync_env_runner_states': False, 'env_task_fn': -1, 'enable_connectors': -1, 'simple_optimizer': True, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'always_attach_evaluation_results': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'entropy_coeff_schedule': None, 'lr_schedule': None, 'sgd_minibatch_size': -1, 'vf_share_layers': -1, 'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>, 'lambda': 1.0, 'input': 'sampler', 'policies': {'player2': (None, None, None, None), 'player1': (None, None, None, None)}, 'callbacks': <class 'ray.rllib.callbacks.callbacks.RLlibCallback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch'}, 'time_since_restore': 40.04099631309509, 'iterations_since_restore': 1, 'perf': {'cpu_util_percent': np.float64(49.67884615384615), 'ram_util_percent': np.float64(90.45384615384614), 'gpu_util_percent0': np.float64(0.4496153846153846), 'vram_util_percent0': np.float64(0.17451829176682693), 'gpu_util_percent1': np.float64(0.0), 'vram_util_percent1': np.float64(0.0005699397492265103)}}\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray.rllib.connectors.env_to_module.flatten_observations import FlattenObservations\n",
    "from ray.rllib.utils.test_utils import (\n",
    "    add_rllib_example_script_args,\n",
    "    run_rllib_example_script_experiment,\n",
    ")\n",
    "from ray.tune.registry import get_trainable_cls, register_env  # noqa\n",
    "\n",
    "class RockPaperScissors(MultiAgentEnv):\n",
    "    \"\"\"Two-player environment for the famous rock paper scissors game.\n",
    "    Both players always move simultaneously over a course of 10 timesteps in total.\n",
    "    The winner of each timestep receives reward of +1, the losing player -1.0.\n",
    "\n",
    "    The observation of each player is the last opponent action.\n",
    "    \"\"\"\n",
    "\n",
    "    ROCK = 0\n",
    "    PAPER = 1\n",
    "    SCISSORS = 2\n",
    "    LIZARD = 3\n",
    "    SPOCK = 4\n",
    "\n",
    "    WIN_MATRIX = {\n",
    "        (ROCK, ROCK): (0, 0),\n",
    "        (ROCK, PAPER): (-1, 1),\n",
    "        (ROCK, SCISSORS): (1, -1),\n",
    "        (PAPER, ROCK): (1, -1),\n",
    "        (PAPER, PAPER): (0, 0),\n",
    "        (PAPER, SCISSORS): (-1, 1),\n",
    "        (SCISSORS, ROCK): (-1, 1),\n",
    "        (SCISSORS, PAPER): (1, -1),\n",
    "        (SCISSORS, SCISSORS): (0, 0),\n",
    "    }\n",
    "    def __init__(self, config=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.agents = self.possible_agents = [\"player1\", \"player2\"]\n",
    "\n",
    "        # The observations are always the last taken actions. Hence observation- and\n",
    "        # action spaces are identical.\n",
    "        self.observation_spaces = self.action_spaces = {\n",
    "            \"player1\": gym.spaces.Discrete(3),\n",
    "            \"player2\": gym.spaces.Discrete(3),\n",
    "        }\n",
    "        self.last_move = None\n",
    "        self.num_moves = 0\n",
    "    \n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        self.num_moves = 0\n",
    "\n",
    "        # The first observation should not matter (none of the agents has moved yet).\n",
    "        # Set them to 0.\n",
    "        return {\n",
    "            \"player1\": 0,\n",
    "            \"player2\": 0,\n",
    "        }, {}  # <- empty infos dict\n",
    "\n",
    "    def step(self, action_dict):\n",
    "        self.num_moves += 1\n",
    "\n",
    "        move1 = action_dict[\"player1\"]\n",
    "        move2 = action_dict[\"player2\"]\n",
    "\n",
    "        # Set the next observations (simply use the other player's action).\n",
    "        # Note that because we are publishing both players in the observations dict,\n",
    "        # we expect both players to act in the next `step()` (simultaneous stepping).\n",
    "        observations = {\"player1\": move2, \"player2\": move1}\n",
    "\n",
    "        # Compute rewards for each player based on the win-matrix.\n",
    "        r1, r2 = self.WIN_MATRIX[move1, move2]\n",
    "        rewards = {\"player1\": r1, \"player2\": r2}\n",
    "\n",
    "        # Terminate the entire episode (for all agents) once 10 moves have been made.\n",
    "        terminateds = {\"__all__\": self.num_moves >= 10}\n",
    "\n",
    "        # Leave truncateds and infos empty.\n",
    "        return observations, rewards, terminateds, {}, {}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_config = (\n",
    "        PPOConfig()\n",
    "        .environment(\n",
    "            RockPaperScissors,\n",
    "            env_config={},\n",
    "        )\n",
    "        .env_runners(\n",
    "            env_to_module_connector=(\n",
    "                lambda env, spaces, device: FlattenObservations(multi_agent=True)\n",
    "            ),\n",
    "        )\n",
    "        .multi_agent(\n",
    "            # Define two policies.\n",
    "            policies={\"player1\", \"player2\"},\n",
    "            # Map agent \"player1\" to policy \"player1\" and agent \"player2\" to policy\n",
    "            # \"player2\".\n",
    "            policy_mapping_fn=lambda agent_id, episode, **kw: agent_id,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    algo = base_config.build()\n",
    "    print(algo.train())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d83ef2",
   "metadata": {},
   "source": [
    "## 回合制环境\n",
    "我们实现著名的井字棋游戏（有一点微小的变动），在 3x3 的棋盘上进行。每个玩家一次在棋盘上放置一个棋子。棋子一旦放置就不能移动。首先完成一行（水平、对角线或垂直）的玩家赢得游戏并获得 +1 奖励。输家获得 -1 奖励。为了简化实现，与原始游戏的变动是，尝试将棋子放在已占用的区域会导致棋盘完全不变，但移动的玩家会因此受到 -5 的惩罚奖励（在原始游戏中，这种移动是根本不允许发生的）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37e76da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "\n",
    "\n",
    "class TicTacToe(MultiAgentEnv):\n",
    "    \"\"\"A two-player game in which any player tries to complete one row in a 3x3 field.\n",
    "\n",
    "    The observation space is Box(0.0, 1.0, (9,)), where each index represents a distinct\n",
    "    field on a 3x3 board and values of 0.0 mean the field is empty, -1.0 means\n",
    "    the opponend owns the field, and 1.0 means we occupy the field:\n",
    "    ----------\n",
    "    | 0| 1| 2|\n",
    "    ----------\n",
    "    | 3| 4| 5|\n",
    "    ----------\n",
    "    | 6| 7| 8|\n",
    "    ----------\n",
    "\n",
    "    The action space is Discrete(9) and actions landing on an already occupied field\n",
    "    are simply ignored (and thus useless to the player taking these actions).\n",
    "\n",
    "    Once a player completes a row, they receive +1.0 reward, the losing player receives\n",
    "    -1.0 reward. In all other cases, both players receive 0.0 reward.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config=None):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define the agents in the game.\n",
    "        self.agents = self.possible_agents = [\"player1\", \"player2\"]\n",
    "\n",
    "        # Each agent observes a 9D tensor, representing the 3x3 fields of the board.\n",
    "        # A 0 means an empty field, a 1 represents a piece of player 1, a -1 a piece of\n",
    "        # player 2.\n",
    "        self.observation_spaces = {\n",
    "            \"player1\": gym.spaces.Box(-1.0, 1.0, (9,), np.float32),\n",
    "            \"player2\": gym.spaces.Box(-1.0, 1.0, (9,), np.float32),\n",
    "        }\n",
    "        # Each player has 9 actions, encoding the 9 fields each player can place a piece\n",
    "        # on during their turn.\n",
    "        self.action_spaces = {\n",
    "            \"player1\": gym.spaces.Discrete(9),\n",
    "            \"player2\": gym.spaces.Discrete(9),\n",
    "        }\n",
    "\n",
    "        self.board = None\n",
    "        self.current_player = None\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        self.board = [\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "        ]\n",
    "        # Pick a random player to start the game.\n",
    "        self.current_player = np.random.choice([\"player1\", \"player2\"])\n",
    "        # Return observations dict (only with the starting player, which is the one\n",
    "        # we expect to act next).\n",
    "        return {\n",
    "            self.current_player: np.array(self.board, np.float32),\n",
    "        }, {}\n",
    "\n",
    "    def step(self, action_dict):\n",
    "        action = action_dict[self.current_player]\n",
    "\n",
    "        # Create a rewards-dict (containing the rewards of the agent that just acted).\n",
    "        rewards = {self.current_player: 0.0}\n",
    "        # Create a terminateds-dict with the special `__all__` agent ID, indicating that\n",
    "        # if True, the episode ends for all agents.\n",
    "        terminateds = {\"__all__\": False}\n",
    "\n",
    "        opponent = \"player1\" if self.current_player == \"player2\" else \"player2\"\n",
    "\n",
    "        # Penalize trying to place a piece on an already occupied field.\n",
    "        if self.board[action] != 0:\n",
    "            rewards[self.current_player] -= 5.0\n",
    "        # Change the board according to the (valid) action taken.\n",
    "        else:\n",
    "            self.board[action] = 1 if self.current_player == \"player1\" else -1\n",
    "\n",
    "            # After having placed a new piece, figure out whether the current player\n",
    "            # won or not.\n",
    "            if self.current_player == \"player1\":\n",
    "                win_val = [1, 1, 1]\n",
    "            else:\n",
    "                win_val = [-1, -1, -1]\n",
    "            if (\n",
    "                # Horizontal win.\n",
    "                self.board[:3] == win_val\n",
    "                or self.board[3:6] == win_val\n",
    "                or self.board[6:] == win_val\n",
    "                # Vertical win.\n",
    "                or self.board[0:7:3] == win_val\n",
    "                or self.board[1:8:3] == win_val\n",
    "                or self.board[2:9:3] == win_val\n",
    "                # Diagonal win.\n",
    "                or self.board[::3] == win_val\n",
    "                or self.board[2:7:2] == win_val\n",
    "            ):\n",
    "                # Final reward is +5 for victory and -5 for a loss.\n",
    "                rewards[self.current_player] += 5.0\n",
    "                rewards[opponent] = -5.0\n",
    "\n",
    "                # Episode is done and needs to be reset for a new game.\n",
    "                terminateds[\"__all__\"] = True\n",
    "\n",
    "            # The board might also be full w/o any player having won/lost.\n",
    "            # In this case, we simply end the episode and none of the players receives\n",
    "            # +1 or -1 reward.\n",
    "            elif 0 not in self.board:\n",
    "                terminateds[\"__all__\"] = True\n",
    "\n",
    "        # Flip players and return an observations dict with only the next player to\n",
    "        # make a move in it.\n",
    "        self.current_player = opponent\n",
    "\n",
    "        return (\n",
    "            {self.current_player: np.array(self.board, np.float32)},\n",
    "            rewards,\n",
    "            terminateds,\n",
    "            {},\n",
    "            {},\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c33d9c6",
   "metadata": {},
   "source": [
    "## 智能体分组\n",
    "在多智能体强化学习中，常见的情况是存在智能体组，其中每个组都被视为一个具有元组动作和观察空间（元组中的每个项对应组中的每个单独智能体）的单智能体。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee60f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def with_agent_groups(\n",
    "    self,\n",
    "    groups: Dict[str, List[AgentID]],\n",
    "    obs_space: gym.Space = None,\n",
    "    act_space: gym.Space = None,\n",
    ") -> \"MultiAgentEnv\":\n",
    "    \"\"\"Convenience method for grouping together agents in this env.\n",
    "\n",
    "    An agent group is a list of agent IDs that are mapped to a single\n",
    "    logical agent. All agents of the group must act at the same time in the\n",
    "    environment. The grouped agent exposes Tuple action and observation\n",
    "    spaces that are the concatenated action and obs spaces of the\n",
    "    individual agents.\n",
    "\n",
    "    The rewards of all the agents in a group are summed. The individual\n",
    "    agent rewards are available under the \"individual_rewards\" key of the\n",
    "    group info return.\n",
    "\n",
    "    Agent grouping is required to leverage algorithms such as Q-Mix.\n",
    "\n",
    "    Args:\n",
    "        groups: Mapping from group id to a list of the agent ids\n",
    "            of group members. If an agent id is not present in any group\n",
    "            value, it will be left ungrouped. The group id becomes a new agent ID\n",
    "            in the final environment.\n",
    "        obs_space: Optional observation space for the grouped\n",
    "            env. Must be a tuple space. If not provided, will infer this to be a\n",
    "            Tuple of n individual agents spaces (n=num agents in a group).\n",
    "        act_space: Optional action space for the grouped env.\n",
    "            Must be a tuple space. If not provided, will infer this to be a Tuple\n",
    "            of n individual agents spaces (n=num agents in a group).\n",
    "\n",
    "    .. testcode::\n",
    "        :skipif: True\n",
    "\n",
    "        from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "        class MyMultiAgentEnv(MultiAgentEnv):\n",
    "            # define your env here\n",
    "            ...\n",
    "        env = MyMultiAgentEnv(...)\n",
    "        grouped_env = env.with_agent_groups(env, {\n",
    "            \"group1\": [\"agent1\", \"agent2\", \"agent3\"],\n",
    "            \"group2\": [\"agent4\", \"agent5\"],\n",
    "        })\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    from ray.rllib.env.wrappers.group_agents_wrapper import \\\n",
    "        GroupAgentsWrapper\n",
    "    return GroupAgentsWrapper(self, groups, obs_space, act_space)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6342a097",
   "metadata": {},
   "source": [
    "## 使用 MultiAgentEnv 运行实际训练实验\n",
    "如果所有智能体使用相同的算法类来训练其策略，请按如下方式配置多智能体训练："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e15cf4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06 16:11:41,124\tWARNING deprecation.py:50 -- DeprecationWarning: `` has been deprecated. This will raise an error in the future!\n",
      "[2025-08-06 16:11:41,185 E 3440243 3440243] core_worker.cc:2740: Actor with class name: 'SingleAgentEnvRunner' and ID: '0eb248dc9805a7b7fb6aafbd01000000' has constructor arguments in the object store and max_restarts > 0. If the arguments in the object store go out of scope or are lost, the actor restart will fail. See https://github.com/ray-project/ray/issues/53727 for more details.\n",
      "[2025-08-06 16:11:41,234 E 3440243 3440243] core_worker.cc:2740: Actor with class name: 'SingleAgentEnvRunner' and ID: 'd1ad7c3ac22d52ff2a86f56301000000' has constructor arguments in the object store and max_restarts > 0. If the arguments in the object store go out of scope or are lost, the actor restart will fail. See https://github.com/ray-project/ray/issues/53727 for more details.\n",
      "2025-08-06 16:11:45,252\tERROR actor_manager.py:873 -- Ray error (The actor died because of an error raised in its creation task, \u001b[36mray::SingleAgentEnvRunner.__init__()\u001b[39m (pid=3451776, ip=10.110.34.88, actor_id=0eb248dc9805a7b7fb6aafbd01000000, repr=<ray.rllib.env.single_agent_env_runner.SingleAgentEnvRunner object at 0x79390023fd60>)\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 687, in make\n",
      "    env_spec = _find_spec(id)\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 531, in _find_spec\n",
      "    _check_version_exists(ns, name, version)\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 397, in _check_version_exists\n",
      "    _check_name_exists(ns, name)\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 374, in _check_name_exists\n",
      "    raise error.NameNotFound(\n",
      "gymnasium.error.NameNotFound: Environment `my_multiagent_env` doesn't exist.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::SingleAgentEnvRunner.__init__()\u001b[39m (pid=3451776, ip=10.110.34.88, actor_id=0eb248dc9805a7b7fb6aafbd01000000, repr=<ray.rllib.env.single_agent_env_runner.SingleAgentEnvRunner object at 0x79390023fd60>)\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 104, in __init__\n",
      "    self.make_env()\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 675, in make_env\n",
      "    gym.make_vec(\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 918, in make_vec\n",
      "    env = gym.vector.SyncVectorEnv(\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/vector/sync_vector_env.py\", line 86, in __init__\n",
      "    self.envs = [env_fn() for env_fn in env_fns]\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/vector/sync_vector_env.py\", line 86, in <listcomp>\n",
      "    self.envs = [env_fn() for env_fn in env_fns]\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 903, in create_single_env\n",
      "    single_env = make(env_spec, **env_spec_kwargs.copy())\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 740, in make\n",
      "    env = env_creator(**env_spec_kwargs)\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/utils/__init__.py\", line 122, in _gym_env_creator\n",
      "    raise EnvError(ERR_MSG_INVALID_ENV_DESCRIPTOR.format(env_descriptor))\n",
      "ray.rllib.utils.error.EnvError: The env string you provided ('my_multiagent_env') is:\n",
      "a) Not a supported or -installed environment.\n",
      "b) Not a tune-registered environment creator.\n",
      "c) Not a valid env class string.\n",
      "\n",
      "Try one of the following:\n",
      "a) For Atari support: `pip install gym[atari] autorom[accept-rom-license]`.\n",
      "   For PyBullet support: `pip install pybullet`.\n",
      "b) To register your custom env, do `from ray import tune;\n",
      "   tune.register_env('[name]', lambda cfg: [return env obj from here using cfg])`.\n",
      "   Then in your config, do `config.environment(env='[name]').\n",
      "c) Make sure you provide a fully qualified classpath, e.g.:\n",
      "   `ray.rllib.examples.envs.classes.repeat_after_me_env.RepeatAfterMeEnv`), taking actor 1 out of service.\n",
      "2025-08-06 16:11:45,254\tERROR actor_manager.py:873 -- Ray error (The actor died because of an error raised in its creation task, \u001b[36mray::SingleAgentEnvRunner.__init__()\u001b[39m (pid=3451775, ip=10.110.34.88, actor_id=d1ad7c3ac22d52ff2a86f56301000000, repr=<ray.rllib.env.single_agent_env_runner.SingleAgentEnvRunner object at 0x78dc0083fdf0>)\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 687, in make\n",
      "    env_spec = _find_spec(id)\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 531, in _find_spec\n",
      "    _check_version_exists(ns, name, version)\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 397, in _check_version_exists\n",
      "    _check_name_exists(ns, name)\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 374, in _check_name_exists\n",
      "    raise error.NameNotFound(\n",
      "gymnasium.error.NameNotFound: Environment `my_multiagent_env` doesn't exist.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::SingleAgentEnvRunner.__init__()\u001b[39m (pid=3451775, ip=10.110.34.88, actor_id=d1ad7c3ac22d52ff2a86f56301000000, repr=<ray.rllib.env.single_agent_env_runner.SingleAgentEnvRunner object at 0x78dc0083fdf0>)\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 104, in __init__\n",
      "    self.make_env()\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 675, in make_env\n",
      "    gym.make_vec(\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 918, in make_vec\n",
      "    env = gym.vector.SyncVectorEnv(\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/vector/sync_vector_env.py\", line 86, in __init__\n",
      "    self.envs = [env_fn() for env_fn in env_fns]\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/vector/sync_vector_env.py\", line 86, in <listcomp>\n",
      "    self.envs = [env_fn() for env_fn in env_fns]\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 903, in create_single_env\n",
      "    single_env = make(env_spec, **env_spec_kwargs.copy())\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 740, in make\n",
      "    env = env_creator(**env_spec_kwargs)\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/utils/__init__.py\", line 122, in _gym_env_creator\n",
      "    raise EnvError(ERR_MSG_INVALID_ENV_DESCRIPTOR.format(env_descriptor))\n",
      "ray.rllib.utils.error.EnvError: The env string you provided ('my_multiagent_env') is:\n",
      "a) Not a supported or -installed environment.\n",
      "b) Not a tune-registered environment creator.\n",
      "c) Not a valid env class string.\n",
      "\n",
      "Try one of the following:\n",
      "a) For Atari support: `pip install gym[atari] autorom[accept-rom-license]`.\n",
      "   For PyBullet support: `pip install pybullet`.\n",
      "b) To register your custom env, do `from ray import tune;\n",
      "   tune.register_env('[name]', lambda cfg: [return env obj from here using cfg])`.\n",
      "   Then in your config, do `config.environment(env='[name]').\n",
      "c) Make sure you provide a fully qualified classpath, e.g.:\n",
      "   `ray.rllib.examples.envs.classes.repeat_after_me_env.RepeatAfterMeEnv`), taking actor 2 out of service.\n",
      "2025-08-06 16:11:45,255\tERROR env_runner_group.py:758 -- Validation of EnvRunner failed! Error=The actor died because of an error raised in its creation task, \u001b[36mray::SingleAgentEnvRunner.__init__()\u001b[39m (pid=3451776, ip=10.110.34.88, actor_id=0eb248dc9805a7b7fb6aafbd01000000, repr=<ray.rllib.env.single_agent_env_runner.SingleAgentEnvRunner object at 0x79390023fd60>)\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 687, in make\n",
      "    env_spec = _find_spec(id)\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 531, in _find_spec\n",
      "    _check_version_exists(ns, name, version)\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 397, in _check_version_exists\n",
      "    _check_name_exists(ns, name)\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 374, in _check_name_exists\n",
      "    raise error.NameNotFound(\n",
      "gymnasium.error.NameNotFound: Environment `my_multiagent_env` doesn't exist.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::SingleAgentEnvRunner.__init__()\u001b[39m (pid=3451776, ip=10.110.34.88, actor_id=0eb248dc9805a7b7fb6aafbd01000000, repr=<ray.rllib.env.single_agent_env_runner.SingleAgentEnvRunner object at 0x79390023fd60>)\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 104, in __init__\n",
      "    self.make_env()\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 675, in make_env\n",
      "    gym.make_vec(\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 918, in make_vec\n",
      "    env = gym.vector.SyncVectorEnv(\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/vector/sync_vector_env.py\", line 86, in __init__\n",
      "    self.envs = [env_fn() for env_fn in env_fns]\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/vector/sync_vector_env.py\", line 86, in <listcomp>\n",
      "    self.envs = [env_fn() for env_fn in env_fns]\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 903, in create_single_env\n",
      "    single_env = make(env_spec, **env_spec_kwargs.copy())\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 740, in make\n",
      "    env = env_creator(**env_spec_kwargs)\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/utils/__init__.py\", line 122, in _gym_env_creator\n",
      "    raise EnvError(ERR_MSG_INVALID_ENV_DESCRIPTOR.format(env_descriptor))\n",
      "ray.rllib.utils.error.EnvError: The env string you provided ('my_multiagent_env') is:\n",
      "a) Not a supported or -installed environment.\n",
      "b) Not a tune-registered environment creator.\n",
      "c) Not a valid env class string.\n",
      "\n",
      "Try one of the following:\n",
      "a) For Atari support: `pip install gym[atari] autorom[accept-rom-license]`.\n",
      "   For PyBullet support: `pip install pybullet`.\n",
      "b) To register your custom env, do `from ray import tune;\n",
      "   tune.register_env('[name]', lambda cfg: [return env obj from here using cfg])`.\n",
      "   Then in your config, do `config.environment(env='[name]').\n",
      "c) Make sure you provide a fully qualified classpath, e.g.:\n",
      "   `ray.rllib.examples.envs.classes.repeat_after_me_env.RepeatAfterMeEnv`\n",
      "2025-08-06 16:11:45,256\tERROR env_runner_group.py:758 -- Validation of EnvRunner failed! Error=The actor died because of an error raised in its creation task, \u001b[36mray::SingleAgentEnvRunner.__init__()\u001b[39m (pid=3451775, ip=10.110.34.88, actor_id=d1ad7c3ac22d52ff2a86f56301000000, repr=<ray.rllib.env.single_agent_env_runner.SingleAgentEnvRunner object at 0x78dc0083fdf0>)\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 687, in make\n",
      "    env_spec = _find_spec(id)\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 531, in _find_spec\n",
      "    _check_version_exists(ns, name, version)\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 397, in _check_version_exists\n",
      "    _check_name_exists(ns, name)\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 374, in _check_name_exists\n",
      "    raise error.NameNotFound(\n",
      "gymnasium.error.NameNotFound: Environment `my_multiagent_env` doesn't exist.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::SingleAgentEnvRunner.__init__()\u001b[39m (pid=3451775, ip=10.110.34.88, actor_id=d1ad7c3ac22d52ff2a86f56301000000, repr=<ray.rllib.env.single_agent_env_runner.SingleAgentEnvRunner object at 0x78dc0083fdf0>)\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 104, in __init__\n",
      "    self.make_env()\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 675, in make_env\n",
      "    gym.make_vec(\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 918, in make_vec\n",
      "    env = gym.vector.SyncVectorEnv(\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/vector/sync_vector_env.py\", line 86, in __init__\n",
      "    self.envs = [env_fn() for env_fn in env_fns]\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/vector/sync_vector_env.py\", line 86, in <listcomp>\n",
      "    self.envs = [env_fn() for env_fn in env_fns]\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 903, in create_single_env\n",
      "    single_env = make(env_spec, **env_spec_kwargs.copy())\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 740, in make\n",
      "    env = env_creator(**env_spec_kwargs)\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/utils/__init__.py\", line 122, in _gym_env_creator\n",
      "    raise EnvError(ERR_MSG_INVALID_ENV_DESCRIPTOR.format(env_descriptor))\n",
      "ray.rllib.utils.error.EnvError: The env string you provided ('my_multiagent_env') is:\n",
      "a) Not a supported or -installed environment.\n",
      "b) Not a tune-registered environment creator.\n",
      "c) Not a valid env class string.\n",
      "\n",
      "Try one of the following:\n",
      "a) For Atari support: `pip install gym[atari] autorom[accept-rom-license]`.\n",
      "   For PyBullet support: `pip install pybullet`.\n",
      "b) To register your custom env, do `from ray import tune;\n",
      "   tune.register_env('[name]', lambda cfg: [return env obj from here using cfg])`.\n",
      "   Then in your config, do `config.environment(env='[name]').\n",
      "c) Make sure you provide a fully qualified classpath, e.g.:\n",
      "   `ray.rllib.examples.envs.classes.repeat_after_me_env.RepeatAfterMeEnv`\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 27\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrl_module\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrl_module\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RLModuleSpec\n\u001b[1;32m      5\u001b[0m config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      6\u001b[0m     PPOConfig()\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;241m.\u001b[39menvironment(env\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy_multiagent_env\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     )\n\u001b[1;32m     25\u001b[0m )\n\u001b[0;32m---> 27\u001b[0m algo \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(algo\u001b[38;5;241m.\u001b[39mtrain())\n",
      "File \u001b[0;32m~/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/utils/deprecation.py:128\u001b[0m, in \u001b[0;36mDeprecated.<locals>._inner.<locals>._ctor\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     deprecation_warning(\n\u001b[1;32m    122\u001b[0m         old\u001b[38;5;241m=\u001b[39mold \u001b[38;5;129;01mor\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[1;32m    123\u001b[0m         new\u001b[38;5;241m=\u001b[39mnew,\n\u001b[1;32m    124\u001b[0m         help\u001b[38;5;241m=\u001b[39mhelp,\n\u001b[1;32m    125\u001b[0m         error\u001b[38;5;241m=\u001b[39merror,\n\u001b[1;32m    126\u001b[0m     )\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# Call the deprecated method/function.\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm_config.py:5794\u001b[0m, in \u001b[0;36mAlgorithmConfig.build\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   5792\u001b[0m \u001b[38;5;129m@Deprecated\u001b[39m(new\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlgorithmConfig.build_algo\u001b[39m\u001b[38;5;124m\"\u001b[39m, error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   5793\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbuild\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 5794\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_algo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm_config.py:1001\u001b[0m, in \u001b[0;36mAlgorithmConfig.build_algo\u001b[0;34m(self, env, logger_creator, use_copy)\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malgo_class, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    999\u001b[0m     algo_class \u001b[38;5;241m=\u001b[39m get_trainable_cls(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malgo_class)\n\u001b[0;32m-> 1001\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgo_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1002\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muse_copy\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogger_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:536\u001b[0m, in \u001b[0;36mAlgorithm.__init__\u001b[0;34m(self, config, env, logger_creator, **kwargs)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;66;03m# Evaluation EnvRunnerGroup and metrics last returned by `self.evaluate()`.\u001b[39;00m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_env_runner_group: Optional[EnvRunnerGroup] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 536\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/tune/trainable/trainable.py:158\u001b[0m, in \u001b[0;36mTrainable.__init__\u001b[0;34m(self, config, logger_creator, storage)\u001b[0m\n\u001b[1;32m    154\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStorageContext on the TRAINABLE:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mstorage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open_logfiles(stdout_file, stderr_file)\n\u001b[0;32m--> 158\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m setup_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m setup_time \u001b[38;5;241m>\u001b[39m SETUP_TIME_THRESHOLD:\n",
      "File \u001b[0;32m~/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:644\u001b[0m, in \u001b[0;36mAlgorithm.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffline_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_online \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39menable_env_runner_and_connector_v2:\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;66;03m# Create a set of env runner actors via a EnvRunnerGroup.\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv_runner_group \u001b[38;5;241m=\u001b[39m \u001b[43mEnvRunnerGroup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdefault_policy_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_default_policy_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# New API stack: User decides whether to create local env runner.\u001b[39;49;00m\n\u001b[1;32m    650\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Old API stack: Always create local EnvRunner.\u001b[39;49;00m\n\u001b[1;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_env_runner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    653\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menable_env_runner_and_connector_v2\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_local_env_runner\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogdir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogdir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtune_trial_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrial_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;66;03m# Compile, validate, and freeze an evaluation config.\u001b[39;00m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget_evaluation_config_object()\n",
      "File \u001b[0;32m~/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/env_runner_group.py:198\u001b[0m, in \u001b[0;36mEnvRunnerGroup.__init__\u001b[0;34m(self, env_creator, validate_env, default_policy_class, config, local_env_runner, logdir, _setup, tune_trial_id, pg_offset, num_env_runners, num_workers, local_worker)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _setup:\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 198\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_env_runners\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m                \u001b[49m\u001b[43mnum_env_runners\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnum_env_runners\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    204\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_env_runners\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_env_runner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_env_runner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;66;03m# EnvRunnerGroup creation possibly fails, if some (remote) workers cannot\u001b[39;00m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;66;03m# be initialized properly (due to some errors in the EnvRunners's\u001b[39;00m\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;66;03m# constructor).\u001b[39;00m\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m RayActorError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    212\u001b[0m         \u001b[38;5;66;03m# In case of an actor (remote worker) init failure, the remote worker\u001b[39;00m\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;66;03m# may still exist and will be accessible, however, e.g. calling\u001b[39;00m\n\u001b[1;32m    214\u001b[0m         \u001b[38;5;66;03m# its `sample.remote()` would result in strange \"property not found\"\u001b[39;00m\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;66;03m# errors.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/env_runner_group.py:286\u001b[0m, in \u001b[0;36mEnvRunnerGroup._setup\u001b[0;34m(self, validate_env, config, num_env_runners, local_env_runner)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;66;03m# If num_env_runners > 0 and we don't have an env on the local worker,\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;66;03m# get the observation- and action spaces for each policy from\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;66;03m# the first remote worker (which does have an env).\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    281\u001b[0m     local_env_runner\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_worker_manager\u001b[38;5;241m.\u001b[39mnum_actors() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39mcreate_env_on_local_worker\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39mobservation_space \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39maction_space)\n\u001b[1;32m    285\u001b[0m ):\n\u001b[0;32m--> 286\u001b[0m     spaces \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_spaces\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    288\u001b[0m     spaces \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/env_runner_group.py:314\u001b[0m, in \u001b[0;36mEnvRunnerGroup.get_spaces\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;66;03m# Get ID of the first remote worker.\u001b[39;00m\n\u001b[1;32m    308\u001b[0m remote_worker_ids \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    309\u001b[0m     [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_worker_manager\u001b[38;5;241m.\u001b[39mactor_ids()[\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_worker_manager\u001b[38;5;241m.\u001b[39mactor_ids()\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[1;32m    312\u001b[0m )\n\u001b[0;32m--> 314\u001b[0m spaces \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforeach_env_runner\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43menv_runner\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_spaces\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremote_worker_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremote_worker_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_env_runner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mremote_worker_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    320\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInferred observation/action spaces from remote \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworker (local worker has no env): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspaces\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    323\u001b[0m )\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m spaces\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.core.rl_module.multi_rl_module import MultiRLModuleSpec\n",
    "from ray.rllib.core.rl_module.rl_module import RLModuleSpec\n",
    "\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(env=\"my_multiagent_env\")\n",
    "    .multi_agent(\n",
    "        policy_mapping_fn=lambda agent_id, episode, **kwargs: (\n",
    "            \"traffic_light\" if agent_id.startswith(\"traffic_light_\")\n",
    "            else random.choice([\"car1\", \"car2\"])\n",
    "        ),\n",
    "        algorithm_config_overrides_per_module={\n",
    "            \"car1\": PPOConfig.overrides(gamma=0.85),\n",
    "            \"car2\": PPOConfig.overrides(lr=0.00001),\n",
    "        },\n",
    "    )\n",
    "    .rl_module(\n",
    "        rl_module_spec=MultiRLModuleSpec(rl_module_specs={\n",
    "            \"car1\": RLModuleSpec(),\n",
    "            \"car2\": RLModuleSpec(),\n",
    "            \"traffic_light\": RLModuleSpec(),\n",
    "        }),\n",
    "    )\n",
    ")\n",
    "\n",
    "algo = config.build()\n",
    "print(algo.train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a4e712",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.core.rl_module import RLModule\n",
    "\n",
    "def policy_mapping_fn(agent_id, episode, **kwargs):\n",
    "    agent_idx = int(agent_id[-1])  # 0 (player1) or 1 (player2)\n",
    "    return \"learning_policy\" if episode.id_ % 2 == agent_idx else \"random_policy\"\n",
    "\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(env=\"two_player_game\")\n",
    "    .multi_agent(\n",
    "        policy_mapping_fn=policy_mapping_fn,\n",
    "        policies_to_train=[\"learning_policy\"],\n",
    "    )\n",
    "    .rl_module(\n",
    "        rl_module_spec=MultiRLModuleSpec(rl_module_specs={\n",
    "            \"learning_policy\": RLModuleSpec(),\n",
    "            \"random_policy\": RLModuleSpec(rl_module_class=RandomRLModule),\n",
    "        }),\n",
    "    )\n",
    ")\n",
    "\n",
    "algo = config.build()\n",
    "print(algo.train())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73db00ce",
   "metadata": {},
   "source": [
    "# 分层环境\n",
    "可以将分层动作模式中的任何一种实现为一个包含各种类型智能体（例如高层智能体和低层智能体）的多智能体环境。当使用正确的智能体到模块映射函数进行设置时，从 RLlib 的角度来看，该问题就变成了一个简单的、具有不同类型策略的独立多智能体问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da06ef89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-08-06 16:40:28,744 E 3440243 3440243] core_worker.cc:2740: Actor with class name: 'MultiAgentEnvRunner' and ID: '75ace56423b1bef47f2366bb01000000' has constructor arguments in the object store and max_restarts > 0. If the arguments in the object store go out of scope or are lost, the actor restart will fail. See https://github.com/ray-project/ray/issues/53727 for more details.\n",
      "[2025-08-06 16:40:28,795 E 3440243 3440243] core_worker.cc:2740: Actor with class name: 'MultiAgentEnvRunner' and ID: '47414a5b2117509e5f5595e801000000' has constructor arguments in the object store and max_restarts > 0. If the arguments in the object store go out of scope or are lost, the actor restart will fail. See https://github.com/ray-project/ray/issues/53727 for more details.\n",
      "2025-08-06 16:40:32,882\tERROR actor_manager.py:873 -- Ray error (The actor died because of an error raised in its creation task, \u001b[36mray::MultiAgentEnvRunner.__init__()\u001b[39m (pid=3932990, ip=10.110.34.88, actor_id=75ace56423b1bef47f2366bb01000000, repr=<ray.rllib.env.multi_agent_env_runner.MultiAgentEnvRunner object at 0x70b710ef9ba0>)\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 112, in __init__\n",
      "    self.make_env()\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 817, in make_env\n",
      "    self.env = make_vec(\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/vector/registration.py\", line 69, in make_vec\n",
      "    env = SyncVectorMultiAgentEnv(\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/vector/sync_vector_multi_agent_env.py\", line 36, in __init__\n",
      "    if self.envs[0].unwrapped.observation_spaces is not None:\n",
      "AttributeError: 'PendulumEnv' object has no attribute 'observation_spaces'), taking actor 1 out of service.\n",
      "2025-08-06 16:40:32,884\tERROR actor_manager.py:873 -- Ray error (The actor died because of an error raised in its creation task, \u001b[36mray::MultiAgentEnvRunner.__init__()\u001b[39m (pid=3932991, ip=10.110.34.88, actor_id=47414a5b2117509e5f5595e801000000, repr=<ray.rllib.env.multi_agent_env_runner.MultiAgentEnvRunner object at 0x709e61171b40>)\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 112, in __init__\n",
      "    self.make_env()\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 817, in make_env\n",
      "    self.env = make_vec(\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/vector/registration.py\", line 69, in make_vec\n",
      "    env = SyncVectorMultiAgentEnv(\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/vector/sync_vector_multi_agent_env.py\", line 36, in __init__\n",
      "    if self.envs[0].unwrapped.observation_spaces is not None:\n",
      "AttributeError: 'PendulumEnv' object has no attribute 'observation_spaces'), taking actor 2 out of service.\n",
      "2025-08-06 16:40:32,885\tERROR env_runner_group.py:758 -- Validation of EnvRunner failed! Error=The actor died because of an error raised in its creation task, \u001b[36mray::MultiAgentEnvRunner.__init__()\u001b[39m (pid=3932990, ip=10.110.34.88, actor_id=75ace56423b1bef47f2366bb01000000, repr=<ray.rllib.env.multi_agent_env_runner.MultiAgentEnvRunner object at 0x70b710ef9ba0>)\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 112, in __init__\n",
      "    self.make_env()\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 817, in make_env\n",
      "    self.env = make_vec(\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/vector/registration.py\", line 69, in make_vec\n",
      "    env = SyncVectorMultiAgentEnv(\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/vector/sync_vector_multi_agent_env.py\", line 36, in __init__\n",
      "    if self.envs[0].unwrapped.observation_spaces is not None:\n",
      "AttributeError: 'PendulumEnv' object has no attribute 'observation_spaces'\n",
      "2025-08-06 16:40:32,886\tERROR env_runner_group.py:758 -- Validation of EnvRunner failed! Error=The actor died because of an error raised in its creation task, \u001b[36mray::MultiAgentEnvRunner.__init__()\u001b[39m (pid=3932991, ip=10.110.34.88, actor_id=47414a5b2117509e5f5595e801000000, repr=<ray.rllib.env.multi_agent_env_runner.MultiAgentEnvRunner object at 0x709e61171b40>)\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 112, in __init__\n",
      "    self.make_env()\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 817, in make_env\n",
      "    self.env = make_vec(\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/vector/registration.py\", line 69, in make_vec\n",
      "    env = SyncVectorMultiAgentEnv(\n",
      "  File \"/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/vector/sync_vector_multi_agent_env.py\", line 36, in __init__\n",
      "    if self.envs[0].unwrapped.observation_spaces is not None:\n",
      "AttributeError: 'PendulumEnv' object has no attribute 'observation_spaces'\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 15\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgorithms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mppo\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PPOConfig\n\u001b[1;32m      3\u001b[0m config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      4\u001b[0m     PPOConfig()\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39menvironment(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPendulum-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     )\n\u001b[1;32m     13\u001b[0m )\n\u001b[0;32m---> 15\u001b[0m algo \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(algo\u001b[38;5;241m.\u001b[39mtrain())\n",
      "File \u001b[0;32m~/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/utils/deprecation.py:128\u001b[0m, in \u001b[0;36mDeprecated.<locals>._inner.<locals>._ctor\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     deprecation_warning(\n\u001b[1;32m    122\u001b[0m         old\u001b[38;5;241m=\u001b[39mold \u001b[38;5;129;01mor\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[1;32m    123\u001b[0m         new\u001b[38;5;241m=\u001b[39mnew,\n\u001b[1;32m    124\u001b[0m         help\u001b[38;5;241m=\u001b[39mhelp,\n\u001b[1;32m    125\u001b[0m         error\u001b[38;5;241m=\u001b[39merror,\n\u001b[1;32m    126\u001b[0m     )\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# Call the deprecated method/function.\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm_config.py:5794\u001b[0m, in \u001b[0;36mAlgorithmConfig.build\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   5792\u001b[0m \u001b[38;5;129m@Deprecated\u001b[39m(new\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlgorithmConfig.build_algo\u001b[39m\u001b[38;5;124m\"\u001b[39m, error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   5793\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbuild\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 5794\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_algo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm_config.py:1001\u001b[0m, in \u001b[0;36mAlgorithmConfig.build_algo\u001b[0;34m(self, env, logger_creator, use_copy)\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malgo_class, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    999\u001b[0m     algo_class \u001b[38;5;241m=\u001b[39m get_trainable_cls(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malgo_class)\n\u001b[0;32m-> 1001\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgo_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1002\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muse_copy\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogger_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:536\u001b[0m, in \u001b[0;36mAlgorithm.__init__\u001b[0;34m(self, config, env, logger_creator, **kwargs)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;66;03m# Evaluation EnvRunnerGroup and metrics last returned by `self.evaluate()`.\u001b[39;00m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_env_runner_group: Optional[EnvRunnerGroup] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 536\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/tune/trainable/trainable.py:158\u001b[0m, in \u001b[0;36mTrainable.__init__\u001b[0;34m(self, config, logger_creator, storage)\u001b[0m\n\u001b[1;32m    154\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStorageContext on the TRAINABLE:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mstorage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open_logfiles(stdout_file, stderr_file)\n\u001b[0;32m--> 158\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m setup_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m setup_time \u001b[38;5;241m>\u001b[39m SETUP_TIME_THRESHOLD:\n",
      "File \u001b[0;32m~/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:644\u001b[0m, in \u001b[0;36mAlgorithm.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffline_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_online \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39menable_env_runner_and_connector_v2:\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;66;03m# Create a set of env runner actors via a EnvRunnerGroup.\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv_runner_group \u001b[38;5;241m=\u001b[39m \u001b[43mEnvRunnerGroup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdefault_policy_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_default_policy_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# New API stack: User decides whether to create local env runner.\u001b[39;49;00m\n\u001b[1;32m    650\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Old API stack: Always create local EnvRunner.\u001b[39;49;00m\n\u001b[1;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_env_runner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    653\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menable_env_runner_and_connector_v2\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_local_env_runner\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogdir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogdir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtune_trial_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrial_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;66;03m# Compile, validate, and freeze an evaluation config.\u001b[39;00m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget_evaluation_config_object()\n",
      "File \u001b[0;32m~/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/env_runner_group.py:198\u001b[0m, in \u001b[0;36mEnvRunnerGroup.__init__\u001b[0;34m(self, env_creator, validate_env, default_policy_class, config, local_env_runner, logdir, _setup, tune_trial_id, pg_offset, num_env_runners, num_workers, local_worker)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _setup:\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 198\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_env_runners\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m                \u001b[49m\u001b[43mnum_env_runners\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnum_env_runners\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    204\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_env_runners\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_env_runner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_env_runner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;66;03m# EnvRunnerGroup creation possibly fails, if some (remote) workers cannot\u001b[39;00m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;66;03m# be initialized properly (due to some errors in the EnvRunners's\u001b[39;00m\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;66;03m# constructor).\u001b[39;00m\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m RayActorError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    212\u001b[0m         \u001b[38;5;66;03m# In case of an actor (remote worker) init failure, the remote worker\u001b[39;00m\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;66;03m# may still exist and will be accessible, however, e.g. calling\u001b[39;00m\n\u001b[1;32m    214\u001b[0m         \u001b[38;5;66;03m# its `sample.remote()` would result in strange \"property not found\"\u001b[39;00m\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;66;03m# errors.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/env_runner_group.py:286\u001b[0m, in \u001b[0;36mEnvRunnerGroup._setup\u001b[0;34m(self, validate_env, config, num_env_runners, local_env_runner)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;66;03m# If num_env_runners > 0 and we don't have an env on the local worker,\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;66;03m# get the observation- and action spaces for each policy from\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;66;03m# the first remote worker (which does have an env).\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    281\u001b[0m     local_env_runner\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_worker_manager\u001b[38;5;241m.\u001b[39mnum_actors() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39mcreate_env_on_local_worker\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39mobservation_space \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39maction_space)\n\u001b[1;32m    285\u001b[0m ):\n\u001b[0;32m--> 286\u001b[0m     spaces \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_spaces\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    288\u001b[0m     spaces \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/env_runner_group.py:314\u001b[0m, in \u001b[0;36mEnvRunnerGroup.get_spaces\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;66;03m# Get ID of the first remote worker.\u001b[39;00m\n\u001b[1;32m    308\u001b[0m remote_worker_ids \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    309\u001b[0m     [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_worker_manager\u001b[38;5;241m.\u001b[39mactor_ids()[\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_worker_manager\u001b[38;5;241m.\u001b[39mactor_ids()\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[1;32m    312\u001b[0m )\n\u001b[0;32m--> 314\u001b[0m spaces \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforeach_env_runner\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43menv_runner\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_spaces\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremote_worker_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremote_worker_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_env_runner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mremote_worker_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    320\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInferred observation/action spaces from remote \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworker (local worker has no env): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspaces\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    323\u001b[0m )\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m spaces\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(\"Pendulum-v1\")\n",
    "    .multi_agent(\n",
    "        policies={\"top_level\", \"low_level\"},\n",
    "        policy_mapping_fn=(\n",
    "            lambda aid, eps, **kw: \"low_level\" if aid.startswith(\"low_level\") else \"top_level\"\n",
    "        ),\n",
    "        policies_to_train=[\"top_level\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "algo = config.build()\n",
    "print(algo.train())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ray_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d272a945",
   "metadata": {},
   "source": [
    "# Checkpointable API\n",
    "## 使用 save_to_path() 创建新检查点\n",
    "你可以通过 save_to_path() 方法从已实例化的 RLlib 对象创建新检查点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0520f4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06 20:37:19,933\tWARNING deprecation.py:50 -- DeprecationWarning: `build` has been deprecated. Use `AlgorithmConfig.build_algo` instead. This will raise an error in the future!\n",
      "2025-08-06 20:37:19,937\tWARNING algorithm_config.py:5033 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:520: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2025-08-06 20:37:21,745\tINFO worker.py:1927 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "# Configure and build an initial algorithm.\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(\"Pendulum-v1\")\n",
    ")\n",
    "ppo = config.build()\n",
    "\n",
    "# Train for one iteration, then save to a checkpoint.\n",
    "print(ppo.train())\n",
    "checkpoint_dir = ppo.save_to_path()\n",
    "print(f\"saved algo to {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9520fc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.examples.envs.classes.multi_agent import MultiAgentPendulum\n",
    "from ray.tune import register_env\n",
    "\n",
    "register_env(\"multi-pendulum\", lambda cfg: MultiAgentPendulum({\"num_agents\": 2}))\n",
    "\n",
    "# Configure and build an initial algorithm.\n",
    "multi_agent_config = (\n",
    "    PPOConfig()\n",
    "    .environment(\"multi-pendulum\")\n",
    "    .multi_agent(\n",
    "        policies={\"p0\", \"p1\"},\n",
    "        # Agent IDs are 0 and 1 -> map to p0 and p1, respectively.\n",
    "        policy_mapping_fn=lambda aid, eps, **kw: f\"p{aid}\"\n",
    "    )\n",
    ")\n",
    "ppo = multi_agent_config.build()\n",
    "\n",
    "# Train for one iteration, then save to a checkpoint.\n",
    "print(ppo.train())\n",
    "multi_agent_checkpoint_dir = ppo.save_to_path()\n",
    "print(f\"saved multi-agent algo to {multi_agent_checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b50277",
   "metadata": {},
   "source": [
    "## 使用 from_checkpoint 从检查点创建实例\n",
    "一旦你有了训练好的 Algorithm 或其任何子组件的检查点，你可以直接从该检查点重新创建新对象。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35d44bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the correct class to create from scratch using the checkpoint.\n",
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "\n",
    "# Use the already existing checkpoint in `checkpoint_dir`.\n",
    "new_ppo = Algorithm.from_checkpoint(checkpoint_dir)\n",
    "# Confirm the `new_ppo` matches the originally checkpointed one.\n",
    "assert new_ppo.config.env == \"Pendulum-v1\"\n",
    "\n",
    "# Continue training.\n",
    "new_ppo.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22639300",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "# Import the correct class to create from scratch using the checkpoint.\n",
    "from ray.rllib.core.rl_module.rl_module import RLModule\n",
    "\n",
    "# Use the already existing checkpoint in `checkpoint_dir`, but go further down\n",
    "# into its subdirectory for the single RLModule.\n",
    "# See the preceding section on \"RLlib component tree\" for the various elements in the RLlib\n",
    "# component tree.\n",
    "rl_module_checkpoint_dir = Path(checkpoint_dir) / \"learner_group\" / \"learner\" / \"rl_module\" / \"default_policy\"\n",
    "\n",
    "# Now that you have the correct subdirectory, create the actual RLModule.\n",
    "rl_module = RLModule.from_checkpoint(rl_module_checkpoint_dir)\n",
    "\n",
    "# Run a forward pass to compute action logits.\n",
    "# Use a dummy Pendulum observation tensor (3d) and add a batch dim (B=1).\n",
    "results = rl_module.forward_inference(\n",
    "    {\"obs\": torch.tensor([0.5, 0.25, -0.3]).unsqueeze(0).float()}\n",
    ")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad444e40",
   "metadata": {},
   "source": [
    "## 使用 restore_from_path 从检查点恢复状态\n",
    "restore_from_path() 将状态加载到已经运行的对象中，例如你的 Algorithm，或者加载到该对象的子组件中，例如你的 Algorithm 内的特定 RLModule。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740fb5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate the preceding PPO from the config.\n",
    "new_ppo = config.build()\n",
    "\n",
    "# Load the state stored previously in `checkpoint_dir` into the\n",
    "# running algorithm instance.\n",
    "new_ppo.restore_from_path(checkpoint_dir)\n",
    "\n",
    "# Run another training iteration.\n",
    "new_ppo.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db857998",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "\n",
    "# Reuse the preceding PPOConfig (`config`).\n",
    "# Inject custom callback code that runs right after algorithm's initialization.\n",
    "config.callbacks(\n",
    "    on_algorithm_init=(\n",
    "        lambda algorithm, _dir=checkpoint_dir, **kw: algorithm.restore_from_path(_dir)\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Run the experiment, continuing from the checkpoint, through Ray Tune.\n",
    "results = tune.Tuner(\n",
    "    config.algo_class,\n",
    "    param_space=config,\n",
    "    run_config=tune.RunConfig(stop={\"num_env_steps_sampled_lifetime\": 8000})\n",
    ").fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10e0acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse the preceding multi-agent PPOConfig (`multi_agent_config`).\n",
    "\n",
    "# But swap out ``p1`` with the state of the ``default_policy`` from the\n",
    "# single-agent run, using a callback and the correct path through the\n",
    "# RLlib component tree:\n",
    "multi_rl_module_component_tree = \"learner_group/learner/rl_module\"\n",
    "\n",
    "# Inject custom callback code that runs right after algorithm's initialization.\n",
    "\n",
    "def _on_algo_init(algorithm, **kwargs):\n",
    "    algorithm.restore_from_path(\n",
    "        # Checkpoint was single-agent (has \"default_policy\" subdir).\n",
    "        path=Path(checkpoint_dir) / multi_rl_module_component_tree / \"default_policy\",\n",
    "        # Algo is multi-agent (has \"p0\" and \"p1\" subdirs).\n",
    "        component=multi_rl_module_component_tree + \"/p1\",\n",
    "    )\n",
    "\n",
    "# Inject callback.\n",
    "multi_agent_config.callbacks(on_algorithm_init=_on_algo_init)\n",
    "\n",
    "# Run the experiment through Ray Tune.\n",
    "results = tune.Tuner(\n",
    "    multi_agent_config.algo_class,\n",
    "    param_space=multi_agent_config,\n",
    "    run_config=tune.RunConfig(stop={\"num_env_steps_sampled_lifetime\": 8000})\n",
    ").fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ray_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

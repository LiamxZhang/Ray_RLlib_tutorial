{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "432b383d",
   "metadata": {},
   "source": [
    "# 算法 \n",
    "## On-Policy\n",
    "## 近端策略优化 (PPO)\n",
    "PPO 架构：在一个训练迭代中，PPO 执行三个主要步骤：1. 采样一组 episode 或 episode 片段 1. 将它们转换为训练批次，并使用 clipped objective 和多次 SGD 遍历该批次来更新模型 1. 将 Learner 的权重同步回 EnvRunner。PPO 在两个方向上都能扩展，支持多个 EnvRunner 进行样本收集，以及多个基于 GPU 或 CPU 的 Learner 更新模型。\n",
    "\n",
    "class ray.rllib.algorithms.ppo.ppo.PPOConfig(algo_class=None)\n",
    "定义一个配置类，可以从中构建 PPO 算法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3ce4393",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:520: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "[2025-08-06 19:16:06,568 E 4032506 4032506] core_worker.cc:2740: Actor with class name: 'SingleAgentEnvRunner' and ID: 'e26e84d5351721ccbd1186d501000000' has constructor arguments in the object store and max_restarts > 0. If the arguments in the object store go out of scope or are lost, the actor restart will fail. See https://github.com/ray-project/ray/issues/53727 for more details.\n",
      "2025-08-06 19:16:40,206\tINFO trainable.py:161 -- Trainable.setup took 33.700 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'timers': {'training_iteration': 1.5853639098349959,\n",
       "  'restore_env_runners': 2.5487970560789108e-05,\n",
       "  'training_step': 1.58493511704728,\n",
       "  'env_runner_sampling_timer': 0.5528545319102705,\n",
       "  'learner_update_timer': 1.0288492140825838,\n",
       "  'synch_weights': 0.0027918238192796707},\n",
       " 'env_runners': {'num_env_steps_sampled_lifetime': 256,\n",
       "  'env_to_module_connector': {'timers': {'connectors': {'add_states_from_episodes_to_batch': 9.538870859409285e-06,\n",
       "     'batch_individual_items': 3.748741463657136e-05,\n",
       "     'add_time_dim_to_batch_and_zero_pad': 1.6902450732924846e-05,\n",
       "     'add_observations_from_episodes_to_batch': 1.8243293477540685e-05,\n",
       "     'numpy_to_tensor': 7.132719848563005e-05}},\n",
       "   'connector_pipeline_timer': 0.0003292565112888338},\n",
       "  'num_env_steps_sampled': 256,\n",
       "  'module_to_env_connector': {'timers': {'connectors': {'normalize_and_clip_actions': 0.0001058564655639427,\n",
       "     'listify_data_for_vector_env': 6.923905912260099e-05,\n",
       "     'un_batch_to_individual_items': 3.633027966971566e-05,\n",
       "     'get_actions': 0.0019862558942762287,\n",
       "     'remove_single_ts_time_rank_from_batch': 4.031644456771605e-06,\n",
       "     'tensor_to_numpy': 0.00013957474408681685}},\n",
       "   'connector_pipeline_timer': 0.002559977707142366},\n",
       "  'agent_episode_return_mean': {'default_agent': 22.454545454545453},\n",
       "  'env_to_module_sum_episodes_length_out': 11.496872184147426,\n",
       "  'num_episodes': 11,\n",
       "  'num_agent_steps_sampled_lifetime': {'default_agent': 256},\n",
       "  'num_module_steps_sampled_lifetime': {'default_policy': 256},\n",
       "  'module_episode_return_mean': {'default_policy': 22.454545454545453},\n",
       "  'episode_len_mean': 22.454545454545453,\n",
       "  'episode_len_min': 12,\n",
       "  'episode_duration_sec_mean': 0.04393054498359561,\n",
       "  'episode_return_mean': 22.454545454545453,\n",
       "  'env_step_timer': 0.00013564177017726884,\n",
       "  'num_module_steps_sampled': {'default_policy': 256},\n",
       "  'num_agent_steps_sampled': {'default_agent': 256},\n",
       "  'weights_seq_no': 0.0,\n",
       "  'episode_return_max': 34.0,\n",
       "  'episode_len_max': 34,\n",
       "  'rlmodule_inference_timer': 0.0009464662638584833,\n",
       "  'sample': 0.5304778290446848,\n",
       "  'num_episodes_lifetime': 11,\n",
       "  'episode_return_min': 12.0,\n",
       "  'env_to_module_sum_episodes_length_in': 11.496872184147426,\n",
       "  'env_reset_timer': 0.000632123090326786,\n",
       "  'num_env_steps_sampled_lifetime_throughput': 481.814874404329},\n",
       " 'learners': {'__all_modules__': {'num_non_trainable_parameters': 0,\n",
       "   'learner_connector_sum_episodes_length_in': 256,\n",
       "   'learner_connector_sum_episodes_length_out': 268,\n",
       "   'learner_connector': {'timers': {'connectors': {'add_one_ts_to_episodes_and_truncate': 0.003004214959219098,\n",
       "      'add_time_dim_to_batch_and_zero_pad': 4.461687058210373e-05,\n",
       "      'add_observations_from_episodes_to_batch': 0.0002306180540472269,\n",
       "      'general_advantage_estimation': 0.035842269891873,\n",
       "      'add_columns_from_episodes_to_train_batch': 0.011778559070080519,\n",
       "      'numpy_to_tensor': 0.0008222470059990883,\n",
       "      'batch_individual_items': 0.006749848835170269,\n",
       "      'add_states_from_episodes_to_batch': 0.00012929807417094707}},\n",
       "    'connector_pipeline_timer': 0.059695975156500936},\n",
       "   'num_env_steps_trained': 16884,\n",
       "   'num_env_steps_trained_lifetime': 16884,\n",
       "   'num_trainable_parameters': 134915,\n",
       "   'num_module_steps_trained': 8064,\n",
       "   'num_module_steps_trained_lifetime': 8064,\n",
       "   'num_env_steps_trained_lifetime_throughput': 22616.575291183,\n",
       "   'num_module_steps_trained_throughput': 10804.938476234189,\n",
       "   'num_module_steps_trained_lifetime_throughput': 10803.814277311812},\n",
       "  'default_policy': {'diff_num_grad_updates_vs_sampler_policy': np.float32(1.0),\n",
       "   'vf_loss': np.float32(1.193469),\n",
       "   'gradients_default_optimizer_global_norm': np.float32(4.6126437),\n",
       "   'total_loss': np.float32(1.1481717),\n",
       "   'weights_seq_no': 1.0,\n",
       "   'mean_kl_loss': np.float32(0.013639636),\n",
       "   'vf_loss_unclipped': np.float32(2.0141654),\n",
       "   'policy_loss': np.float32(-0.049389467),\n",
       "   'default_optimizer_learning_rate': 0.01,\n",
       "   'num_trainable_parameters': 134915,\n",
       "   'curr_entropy_coeff': 0.0,\n",
       "   'module_train_batch_size_mean': 128.0,\n",
       "   'num_module_steps_trained': 8064,\n",
       "   'curr_kl_coeff': 0.30000001192092896,\n",
       "   'num_module_steps_trained_lifetime': 8064,\n",
       "   'entropy': np.float32(0.68019557),\n",
       "   'vf_explained_var': np.float32(0.71045977),\n",
       "   'num_module_steps_trained_lifetime_throughput': 10805.9557908379}},\n",
       " 'num_training_step_calls_per_iteration': 1,\n",
       " 'num_env_steps_sampled_lifetime': 256,\n",
       " 'fault_tolerance': {'num_healthy_workers': 1,\n",
       "  'num_remote_worker_restarts': 0},\n",
       " 'env_runner_group': {'actor_manager_num_outstanding_async_reqs': 0},\n",
       " 'done': False,\n",
       " 'training_iteration': 1,\n",
       " 'trial_id': 'default',\n",
       " 'date': '2025-08-06_19-16-41',\n",
       " 'timestamp': 1754479001,\n",
       " 'time_this_iter_s': 1.5928466320037842,\n",
       " 'time_total_s': 1.5928466320037842,\n",
       " 'pid': 4032506,\n",
       " 'hostname': 'robotarm-Precision-7920-Tower',\n",
       " 'node_ip': '198.18.0.1',\n",
       " 'config': {'exploration_config': {},\n",
       "  'extra_python_environs_for_driver': {},\n",
       "  'extra_python_environs_for_worker': {},\n",
       "  'placement_strategy': 'PACK',\n",
       "  'num_gpus': 0,\n",
       "  '_fake_gpus': False,\n",
       "  'num_cpus_for_main_process': 1,\n",
       "  'eager_tracing': True,\n",
       "  'eager_max_retraces': 20,\n",
       "  'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "   'inter_op_parallelism_threads': 2,\n",
       "   'gpu_options': {'allow_growth': True},\n",
       "   'log_device_placement': False,\n",
       "   'device_count': {'CPU': 1},\n",
       "   'allow_soft_placement': True},\n",
       "  'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "   'inter_op_parallelism_threads': 8},\n",
       "  'torch_compile_learner': False,\n",
       "  'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>,\n",
       "  'torch_compile_learner_dynamo_backend': 'inductor',\n",
       "  'torch_compile_learner_dynamo_mode': None,\n",
       "  'torch_compile_worker': False,\n",
       "  'torch_compile_worker_dynamo_backend': 'onnxrt',\n",
       "  'torch_compile_worker_dynamo_mode': None,\n",
       "  'torch_ddp_kwargs': {},\n",
       "  'torch_skip_nan_gradients': False,\n",
       "  'env': 'CartPole-v1',\n",
       "  'env_config': {},\n",
       "  'observation_space': None,\n",
       "  'action_space': None,\n",
       "  'clip_rewards': None,\n",
       "  'normalize_actions': True,\n",
       "  'clip_actions': False,\n",
       "  '_is_atari': None,\n",
       "  'disable_env_checking': False,\n",
       "  'render_env': False,\n",
       "  'action_mask_key': 'action_mask',\n",
       "  'env_runner_cls': None,\n",
       "  'num_env_runners': 1,\n",
       "  'create_local_env_runner': True,\n",
       "  'num_envs_per_env_runner': 1,\n",
       "  'gym_env_vectorize_mode': 'SYNC',\n",
       "  'num_cpus_per_env_runner': 1,\n",
       "  'num_gpus_per_env_runner': 0,\n",
       "  'custom_resources_per_env_runner': {},\n",
       "  'validate_env_runners_after_construction': True,\n",
       "  'episodes_to_numpy': True,\n",
       "  'max_requests_in_flight_per_env_runner': 1,\n",
       "  'sample_timeout_s': 60.0,\n",
       "  '_env_to_module_connector': None,\n",
       "  'add_default_connectors_to_env_to_module_pipeline': True,\n",
       "  '_module_to_env_connector': None,\n",
       "  'add_default_connectors_to_module_to_env_pipeline': True,\n",
       "  'merge_env_runner_states': 'training_only',\n",
       "  'broadcast_env_runner_states': True,\n",
       "  'episode_lookback_horizon': 1,\n",
       "  'rollout_fragment_length': 'auto',\n",
       "  'batch_mode': 'truncate_episodes',\n",
       "  'compress_observations': False,\n",
       "  'remote_worker_envs': False,\n",
       "  'remote_env_batch_wait_ms': 0,\n",
       "  'enable_tf1_exec_eagerly': False,\n",
       "  'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       "  'preprocessor_pref': 'deepmind',\n",
       "  'observation_filter': 'NoFilter',\n",
       "  'update_worker_filter_stats': True,\n",
       "  'use_worker_filter_stats': True,\n",
       "  'sampler_perf_stats_ema_coef': None,\n",
       "  '_is_online': True,\n",
       "  'num_learners': 0,\n",
       "  'num_gpus_per_learner': 0,\n",
       "  'num_cpus_per_learner': 'auto',\n",
       "  'num_aggregator_actors_per_learner': 0,\n",
       "  'max_requests_in_flight_per_aggregator_actor': 3,\n",
       "  'local_gpu_idx': 0,\n",
       "  'max_requests_in_flight_per_learner': 3,\n",
       "  'gamma': 0.9,\n",
       "  'lr': 0.01,\n",
       "  'grad_clip': None,\n",
       "  'grad_clip_by': 'global_norm',\n",
       "  '_train_batch_size_per_learner': 256,\n",
       "  'train_batch_size': 4000,\n",
       "  'num_epochs': 30,\n",
       "  'minibatch_size': 128,\n",
       "  'shuffle_batch_per_epoch': True,\n",
       "  'model': {'fcnet_hiddens': [256, 256],\n",
       "   'fcnet_activation': 'tanh',\n",
       "   'fcnet_weights_initializer': None,\n",
       "   'fcnet_weights_initializer_config': None,\n",
       "   'fcnet_bias_initializer': None,\n",
       "   'fcnet_bias_initializer_config': None,\n",
       "   'conv_filters': None,\n",
       "   'conv_activation': 'relu',\n",
       "   'conv_kernel_initializer': None,\n",
       "   'conv_kernel_initializer_config': None,\n",
       "   'conv_bias_initializer': None,\n",
       "   'conv_bias_initializer_config': None,\n",
       "   'conv_transpose_kernel_initializer': None,\n",
       "   'conv_transpose_kernel_initializer_config': None,\n",
       "   'conv_transpose_bias_initializer': None,\n",
       "   'conv_transpose_bias_initializer_config': None,\n",
       "   'post_fcnet_hiddens': [],\n",
       "   'post_fcnet_activation': 'relu',\n",
       "   'post_fcnet_weights_initializer': None,\n",
       "   'post_fcnet_weights_initializer_config': None,\n",
       "   'post_fcnet_bias_initializer': None,\n",
       "   'post_fcnet_bias_initializer_config': None,\n",
       "   'free_log_std': False,\n",
       "   'log_std_clip_param': 20.0,\n",
       "   'no_final_linear': False,\n",
       "   'vf_share_layers': False,\n",
       "   'use_lstm': False,\n",
       "   'max_seq_len': 20,\n",
       "   'lstm_cell_size': 256,\n",
       "   'lstm_use_prev_action': False,\n",
       "   'lstm_use_prev_reward': False,\n",
       "   'lstm_weights_initializer': None,\n",
       "   'lstm_weights_initializer_config': None,\n",
       "   'lstm_bias_initializer': None,\n",
       "   'lstm_bias_initializer_config': None,\n",
       "   '_time_major': False,\n",
       "   'use_attention': False,\n",
       "   'attention_num_transformer_units': 1,\n",
       "   'attention_dim': 64,\n",
       "   'attention_num_heads': 1,\n",
       "   'attention_head_dim': 32,\n",
       "   'attention_memory_inference': 50,\n",
       "   'attention_memory_training': 50,\n",
       "   'attention_position_wise_mlp_dim': 32,\n",
       "   'attention_init_gru_gate_bias': 2.0,\n",
       "   'attention_use_n_prev_actions': 0,\n",
       "   'attention_use_n_prev_rewards': 0,\n",
       "   'framestack': True,\n",
       "   'dim': 84,\n",
       "   'grayscale': False,\n",
       "   'zero_mean': True,\n",
       "   'custom_model': None,\n",
       "   'custom_model_config': {},\n",
       "   'custom_action_dist': None,\n",
       "   'custom_preprocessor': None,\n",
       "   'encoder_latent_dim': None,\n",
       "   'always_check_shapes': False,\n",
       "   'lstm_use_prev_action_reward': -1,\n",
       "   '_use_default_native_models': -1,\n",
       "   '_disable_preprocessor_api': False,\n",
       "   '_disable_action_flattening': False},\n",
       "  '_learner_connector': None,\n",
       "  'add_default_connectors_to_learner_pipeline': True,\n",
       "  'learner_config_dict': {},\n",
       "  'optimizer': {},\n",
       "  '_learner_class': None,\n",
       "  'callbacks_on_algorithm_init': None,\n",
       "  'callbacks_on_env_runners_recreated': None,\n",
       "  'callbacks_on_offline_eval_runners_recreated': None,\n",
       "  'callbacks_on_checkpoint_loaded': None,\n",
       "  'callbacks_on_environment_created': None,\n",
       "  'callbacks_on_episode_created': None,\n",
       "  'callbacks_on_episode_start': None,\n",
       "  'callbacks_on_episode_step': None,\n",
       "  'callbacks_on_episode_end': None,\n",
       "  'callbacks_on_evaluate_start': None,\n",
       "  'callbacks_on_evaluate_end': None,\n",
       "  'callbacks_on_evaluate_offline_start': None,\n",
       "  'callbacks_on_evaluate_offline_end': None,\n",
       "  'callbacks_on_sample_end': None,\n",
       "  'callbacks_on_train_result': None,\n",
       "  'explore': True,\n",
       "  'enable_rl_module_and_learner': True,\n",
       "  'enable_env_runner_and_connector_v2': True,\n",
       "  '_prior_exploration_config': {'type': 'StochasticSampling'},\n",
       "  'count_steps_by': 'env_steps',\n",
       "  'policy_map_capacity': 100,\n",
       "  'policy_mapping_fn': <function ray.rllib.algorithms.algorithm_config.AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN(aid, episode, worker, **kwargs)>,\n",
       "  'policies_to_train': None,\n",
       "  'policy_states_are_swappable': False,\n",
       "  'observation_fn': None,\n",
       "  'offline_data_class': None,\n",
       "  'input_read_method': 'read_parquet',\n",
       "  'input_read_method_kwargs': {},\n",
       "  'input_read_schema': {},\n",
       "  'input_read_episodes': False,\n",
       "  'input_read_sample_batches': False,\n",
       "  'input_read_batch_size': None,\n",
       "  'input_filesystem': None,\n",
       "  'input_filesystem_kwargs': {},\n",
       "  'input_compress_columns': ['obs', 'new_obs'],\n",
       "  'input_spaces_jsonable': True,\n",
       "  'materialize_data': False,\n",
       "  'materialize_mapped_data': True,\n",
       "  'map_batches_kwargs': {},\n",
       "  'iter_batches_kwargs': {},\n",
       "  'ignore_final_observation': False,\n",
       "  'prelearner_class': None,\n",
       "  'prelearner_buffer_class': None,\n",
       "  'prelearner_buffer_kwargs': {},\n",
       "  'prelearner_module_synch_period': 10,\n",
       "  'dataset_num_iters_per_learner': None,\n",
       "  'input_config': {},\n",
       "  'actions_in_input_normalized': False,\n",
       "  'postprocess_inputs': False,\n",
       "  'shuffle_buffer_size': 0,\n",
       "  'output': None,\n",
       "  'output_config': {},\n",
       "  'output_compress_columns': ['obs', 'new_obs'],\n",
       "  'output_max_file_size': 67108864,\n",
       "  'output_max_rows_per_file': None,\n",
       "  'output_write_remaining_data': False,\n",
       "  'output_write_method': 'write_parquet',\n",
       "  'output_write_method_kwargs': {},\n",
       "  'output_filesystem': None,\n",
       "  'output_filesystem_kwargs': {},\n",
       "  'output_write_episodes': True,\n",
       "  'offline_sampling': False,\n",
       "  'evaluation_interval': None,\n",
       "  'evaluation_duration': 10,\n",
       "  'evaluation_duration_unit': 'episodes',\n",
       "  'evaluation_sample_timeout_s': 120.0,\n",
       "  'evaluation_auto_duration_min_env_steps_per_sample': 100,\n",
       "  'evaluation_auto_duration_max_env_steps_per_sample': 2000,\n",
       "  'evaluation_parallel_to_training': False,\n",
       "  'evaluation_force_reset_envs_before_iteration': True,\n",
       "  'evaluation_config': None,\n",
       "  'off_policy_estimation_methods': {},\n",
       "  'ope_split_batch_by_episode': True,\n",
       "  'evaluation_num_env_runners': 0,\n",
       "  'in_evaluation': False,\n",
       "  'sync_filters_on_rollout_workers_timeout_s': 10.0,\n",
       "  'offline_evaluation_interval': None,\n",
       "  'num_offline_eval_runners': 0,\n",
       "  'offline_evaluation_type': None,\n",
       "  'offline_eval_runner_class': None,\n",
       "  'offline_loss_for_module_fn': None,\n",
       "  'offline_evaluation_duration': 1,\n",
       "  'offline_evaluation_parallel_to_training': False,\n",
       "  'offline_evaluation_timeout_s': 120.0,\n",
       "  'num_cpus_per_offline_eval_runner': 1,\n",
       "  'num_gpus_per_offline_eval_runner': 0,\n",
       "  'custom_resources_per_offline_eval_runner': {},\n",
       "  'restart_failed_offline_eval_runners': True,\n",
       "  'ignore_offline_eval_runner_failures': False,\n",
       "  'max_num_offline_eval_runner_restarts': 1000,\n",
       "  'offline_eval_runner_restore_timeout_s': 1800.0,\n",
       "  'max_requests_in_flight_per_offline_eval_runner': 1,\n",
       "  'validate_offline_eval_runners_after_construction': True,\n",
       "  'offline_eval_runner_health_probe_timeout_s': 30.0,\n",
       "  'offline_eval_rl_module_inference_only': False,\n",
       "  'broadcast_offline_eval_runner_states': False,\n",
       "  'offline_eval_batch_size_per_runner': 256,\n",
       "  'dataset_num_iters_per_eval_runner': 1,\n",
       "  'keep_per_episode_custom_metrics': False,\n",
       "  'metrics_episode_collection_timeout_s': 60.0,\n",
       "  'metrics_num_episodes_for_smoothing': 100,\n",
       "  'min_time_s_per_iteration': None,\n",
       "  'min_train_timesteps_per_iteration': 0,\n",
       "  'min_sample_timesteps_per_iteration': 0,\n",
       "  'log_gradients': True,\n",
       "  'export_native_model_files': False,\n",
       "  'checkpoint_trainable_policies_only': False,\n",
       "  'logger_creator': None,\n",
       "  'logger_config': None,\n",
       "  'log_level': 'WARN',\n",
       "  'log_sys_usage': True,\n",
       "  'fake_sampler': False,\n",
       "  'seed': None,\n",
       "  'restart_failed_env_runners': True,\n",
       "  'ignore_env_runner_failures': False,\n",
       "  'max_num_env_runner_restarts': 1000,\n",
       "  'delay_between_env_runner_restarts_s': 60.0,\n",
       "  'restart_failed_sub_environments': False,\n",
       "  'num_consecutive_env_runner_failures_tolerance': 100,\n",
       "  'env_runner_health_probe_timeout_s': 30.0,\n",
       "  'env_runner_restore_timeout_s': 1800.0,\n",
       "  '_model_config': {},\n",
       "  '_rl_module_spec': None,\n",
       "  'algorithm_config_overrides_per_module': {},\n",
       "  '_per_module_overrides': {},\n",
       "  '_validate_config': True,\n",
       "  '_use_msgpack_checkpoints': False,\n",
       "  '_torch_grad_scaler_class': None,\n",
       "  '_torch_lr_scheduler_classes': None,\n",
       "  '_tf_policy_handles_more_than_one_loss': False,\n",
       "  '_disable_preprocessor_api': False,\n",
       "  '_disable_action_flattening': False,\n",
       "  '_disable_initialize_loss_from_dummy_batch': False,\n",
       "  '_dont_auto_sync_env_runner_states': False,\n",
       "  'env_task_fn': -1,\n",
       "  'enable_connectors': -1,\n",
       "  'simple_optimizer': False,\n",
       "  'policy_map_cache': -1,\n",
       "  'worker_cls': -1,\n",
       "  'synchronize_filters': -1,\n",
       "  'enable_async_evaluation': -1,\n",
       "  'custom_async_evaluation_function': -1,\n",
       "  '_enable_rl_module_api': -1,\n",
       "  'auto_wrap_old_gym_envs': -1,\n",
       "  'always_attach_evaluation_results': -1,\n",
       "  'replay_sequence_length': None,\n",
       "  '_disable_execution_plan_api': -1,\n",
       "  'use_critic': True,\n",
       "  'use_gae': True,\n",
       "  'use_kl_loss': True,\n",
       "  'kl_coeff': 0.3,\n",
       "  'kl_target': 0.01,\n",
       "  'vf_loss_coeff': 1.0,\n",
       "  'entropy_coeff': 0.0,\n",
       "  'clip_param': 0.3,\n",
       "  'vf_clip_param': 10.0,\n",
       "  'entropy_coeff_schedule': None,\n",
       "  'lr_schedule': None,\n",
       "  'sgd_minibatch_size': -1,\n",
       "  'vf_share_layers': -1,\n",
       "  'class': ray.rllib.algorithms.ppo.ppo.PPOConfig,\n",
       "  'lambda': 1.0,\n",
       "  'input': 'sampler',\n",
       "  'policies': {'default_policy': (None, None, None, None)},\n",
       "  'callbacks': ray.rllib.callbacks.callbacks.RLlibCallback,\n",
       "  'create_env_on_driver': False,\n",
       "  'custom_eval_function': None,\n",
       "  'framework': 'torch'},\n",
       " 'time_since_restore': 1.5928466320037842,\n",
       " 'iterations_since_restore': 1,\n",
       " 'perf': {'cpu_util_percent': np.float64(42.55),\n",
       "  'ram_util_percent': np.float64(94.30000000000001),\n",
       "  'gpu_util_percent0': np.float64(0.715),\n",
       "  'vram_util_percent0': np.float64(0.187744140625),\n",
       "  'gpu_util_percent1': np.float64(0.0),\n",
       "  'vram_util_percent1': np.float64(0.0005699397492265104)}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "config = PPOConfig()\n",
    "config.environment(\"CartPole-v1\")\n",
    "config.env_runners(num_env_runners=1)\n",
    "config.training(\n",
    "    gamma=0.9, lr=0.01, kl_coeff=0.3, train_batch_size_per_learner=256\n",
    ")\n",
    "\n",
    "# Build a Algorithm object from the config and run 1 training iteration.\n",
    "algo = config.build_algo()\n",
    "algo.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e5a72c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-08-06 19:18:42</td></tr>\n",
       "<tr><td>Running for: </td><td>00:01:08.12        </td></tr>\n",
       "<tr><td>Memory:      </td><td>14.7/15.3 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 3.0/16 CPUs, 0/2 GPUs (0.0/1.0 accelerator_type:P1000)\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  : ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
       "  \n",
       "  \n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v1_f3807_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06 19:18:42,445\tWARNING tune.py:219 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2025-08-06 19:18:42,468\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/robotarm/ray_results/PPO_2025-08-06_19-17-34' in 0.0191s.\n",
      "2025-08-06 19:18:52,498\tINFO tune.py:1041 -- Total run time: 78.17 seconds (68.10 seconds for the tuning loop).\n",
      "2025-08-06 19:18:52,506\tWARNING tune.py:1056 -- Experiment has been interrupted, but the most recent state was saved.\n",
      "Resume experiment with: Tuner.restore(path=\"/home/robotarm/ray_results/PPO_2025-08-06_19-17-34\", trainable=...)\n",
      "2025-08-06 19:18:52,530\tWARNING experiment_analysis.py:180 -- Failed to fetch metrics for 1 trial(s):\n",
      "- PPO_CartPole-v1_f3807_00000: FileNotFoundError('Could not fetch metrics for PPO_CartPole-v1_f3807_00000: both result.json and progress.csv were not found at /home/robotarm/ray_results/PPO_2025-08-06_19-17-34/PPO_CartPole-v1_f3807_00000_0_2025-08-06_19-17-34')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResultGrid<[\n",
       "  Result(\n",
       "    metrics={},\n",
       "    path='/home/robotarm/ray_results/PPO_2025-08-06_19-17-34/PPO_CartPole-v1_f3807_00000_0_2025-08-06_19-17-34',\n",
       "    filesystem='local',\n",
       "    checkpoint=None\n",
       "  )\n",
       "]>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray import tune\n",
    "\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    # Set the config object's env.\n",
    "    .environment(env=\"CartPole-v1\")\n",
    "    # Update the config object's training parameters.\n",
    "    .training(\n",
    "        lr=0.001, clip_param=0.2\n",
    "    )\n",
    ")\n",
    "\n",
    "tune.Tuner(\n",
    "    \"PPO\",\n",
    "    run_config=tune.RunConfig(stop={\"training_iteration\": 1}),\n",
    "    param_space=config,\n",
    ").fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e059910",
   "metadata": {},
   "source": [
    "## Off-Policy\n",
    "## 深度 Q 网络 (DQN, Rainbow, Parametric DQN)\n",
    "DQN 架构：DQN 使用回放缓冲区临时存储 RLlib 从环境中收集的 episode 样本。在不同的训练迭代中，这些 episode 和 episode 片段会从缓冲区中重新采样并重新用于更新模型，最终在缓冲区达到容量且新样本不断进入时被丢弃 (FIFO)。这种训练数据的重用使得 DQN 的样本效率很高且是离策略的。DQN 在两个方向上都能扩展，支持多个 EnvRunner 进行样本收集，以及多个基于 GPU 或 CPU 的 Learner 更新模型。\n",
    "\n",
    " class ray.rllib.algorithms.dqn.dqn.DQNConfig(algo_class=None) 定义一个配置类，可以从中构建 DQN 算法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b61f78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.dqn.dqn import DQNConfig\n",
    "\n",
    "config = (\n",
    "    DQNConfig()\n",
    "    .environment(\"CartPole-v1\")\n",
    "    .training(replay_buffer_config={\n",
    "        \"type\": \"PrioritizedEpisodeReplayBuffer\",\n",
    "        \"capacity\": 60000,\n",
    "        \"alpha\": 0.5,\n",
    "        \"beta\": 0.5,\n",
    "    })\n",
    "    .env_runners(num_env_runners=1)\n",
    ")\n",
    "algo = config.build_algo()\n",
    "algo.train()\n",
    "algo.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbde442",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.dqn.dqn import DQNConfig\n",
    "from ray import tune\n",
    "\n",
    "config = (\n",
    "    DQNConfig()\n",
    "    .environment(\"CartPole-v1\")\n",
    "    .training(\n",
    "        num_atoms=tune.grid_search([1,])\n",
    "    )\n",
    ")\n",
    "tune.Tuner(\n",
    "    \"DQN\",\n",
    "    run_config=tune.RunConfig(stop={\"training_iteration\":1}),\n",
    "    param_space=config,\n",
    ").fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51027097",
   "metadata": {},
   "source": [
    "## 软 Actor-Critic (SAC)\n",
    "SAC 架构：SAC 使用回放缓冲区临时存储 RLlib 从环境中收集的 episode 样本。在不同的训练迭代中，这些 episode 和 episode 片段会从缓冲区中重新采样并重新用于更新模型，最终在缓冲区达到容量且新样本不断进入时被丢弃 (FIFO)。这种训练数据的重用使得 SAC 的样本效率很高且是离策略的。SAC 在两个方向上都能扩展，支持多个 EnvRunner 进行样本收集，以及多个基于 GPU 或 CPU 的 Learner 更新模型。\n",
    "\n",
    "class ray.rllib.algorithms.sac.sac.SACConfig(algo_class=None) 定义一个配置类，可以从中构建 SAC 算法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ceb40bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06 19:19:06,034\tWARNING sac.py:487 -- You are running SAC on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:520: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "[2025-08-06 19:19:07,320 E 4032506 4032506] core_worker.cc:2740: Actor with class name: 'SingleAgentEnvRunner' and ID: '7caae00e4b3e50fa0743633901000000' has constructor arguments in the object store and max_restarts > 0. If the arguments in the object store go out of scope or are lost, the actor restart will fail. See https://github.com/ray-project/ray/issues/53727 for more details.\n",
      "2025-08-06 19:19:14,558\tWARNING sac.py:487 -- You are running SAC on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'timers': {'training_iteration': 2.49877782096155,\n",
       "  'restore_env_runners': 2.619828366461121e-05,\n",
       "  'training_step': 0.14277760602769302,\n",
       "  'env_runner_sampling_timer': 0.1358278902293092,\n",
       "  'replay_buffer_add_data_timer': 0.003704311900377226},\n",
       " 'env_runners': {'num_env_steps_sampled_lifetime': 100,\n",
       "  'env_to_module_connector': {'timers': {'connectors': {'add_states_from_episodes_to_batch': 8.805160120346227e-06,\n",
       "     'batch_individual_items': 7.885414424821597e-05,\n",
       "     'add_time_dim_to_batch_and_zero_pad': 1.4853592734398081e-05,\n",
       "     'add_observations_from_episodes_to_batch': 4.0668916042309075e-05,\n",
       "     'numpy_to_tensor': 0.00013696652123822622}},\n",
       "   'connector_pipeline_timer': 0.0008803033944130772},\n",
       "  'num_env_steps_sampled': 100,\n",
       "  'module_to_env_connector': {'timers': {'connectors': {'normalize_and_clip_actions': 0.00033713240959476807,\n",
       "     'listify_data_for_vector_env': 0.00010940701427942472,\n",
       "     'un_batch_to_individual_items': 6.779415586698412e-05,\n",
       "     'get_actions': 0.0067960093062247725,\n",
       "     'remove_single_ts_time_rank_from_batch': 8.067352552890748e-06,\n",
       "     'tensor_to_numpy': 0.0003869981794015033}},\n",
       "   'connector_pipeline_timer': 0.008187588117144127},\n",
       "  'env_to_module_sum_episodes_length_out': 0.6339676587267711,\n",
       "  'num_episodes': 0,\n",
       "  'num_agent_steps_sampled_lifetime': {'default_agent': 100},\n",
       "  'num_module_steps_sampled_lifetime': {'default_policy': 100},\n",
       "  'env_step_timer': 0.0003526554183056244,\n",
       "  'num_module_steps_sampled': {'default_policy': 100},\n",
       "  'num_agent_steps_sampled': {'default_agent': 100},\n",
       "  'weights_seq_no': 0.0,\n",
       "  'rlmodule_inference_timer': 0.005142703078397408,\n",
       "  'sample': 0.01879180674972101,\n",
       "  'num_episodes_lifetime': 0,\n",
       "  'env_to_module_sum_episodes_length_in': 0.6339676587267711,\n",
       "  'env_reset_timer': 0.0016055151354521513,\n",
       "  'time_between_sampling': 0.018679943692094556,\n",
       "  'num_env_steps_sampled_lifetime_throughput': 51.30452355201048},\n",
       " 'num_training_step_calls_per_iteration': 100,\n",
       " 'num_env_steps_sampled_lifetime': 100,\n",
       " 'fault_tolerance': {'num_healthy_workers': 1,\n",
       "  'num_remote_worker_restarts': 0},\n",
       " 'env_runner_group': {'actor_manager_num_outstanding_async_reqs': 0},\n",
       " 'done': False,\n",
       " 'training_iteration': 1,\n",
       " 'trial_id': 'default',\n",
       " 'date': '2025-08-06_19-19-17',\n",
       " 'timestamp': 1754479157,\n",
       " 'time_this_iter_s': 2.504804849624634,\n",
       " 'time_total_s': 2.504804849624634,\n",
       " 'pid': 4032506,\n",
       " 'hostname': 'robotarm-Precision-7920-Tower',\n",
       " 'node_ip': '198.18.0.1',\n",
       " 'config': {'exploration_config': {},\n",
       "  'extra_python_environs_for_driver': {},\n",
       "  'extra_python_environs_for_worker': {},\n",
       "  'placement_strategy': 'PACK',\n",
       "  'num_gpus': 0,\n",
       "  '_fake_gpus': False,\n",
       "  'num_cpus_for_main_process': 1,\n",
       "  'eager_tracing': True,\n",
       "  'eager_max_retraces': 20,\n",
       "  'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "   'inter_op_parallelism_threads': 2,\n",
       "   'gpu_options': {'allow_growth': True},\n",
       "   'log_device_placement': False,\n",
       "   'device_count': {'CPU': 1},\n",
       "   'allow_soft_placement': True},\n",
       "  'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "   'inter_op_parallelism_threads': 8},\n",
       "  'torch_compile_learner': False,\n",
       "  'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>,\n",
       "  'torch_compile_learner_dynamo_backend': 'inductor',\n",
       "  'torch_compile_learner_dynamo_mode': None,\n",
       "  'torch_compile_worker': False,\n",
       "  'torch_compile_worker_dynamo_backend': 'onnxrt',\n",
       "  'torch_compile_worker_dynamo_mode': None,\n",
       "  'torch_ddp_kwargs': {},\n",
       "  'torch_skip_nan_gradients': False,\n",
       "  'env': 'Pendulum-v1',\n",
       "  'env_config': {},\n",
       "  'observation_space': None,\n",
       "  'action_space': None,\n",
       "  'clip_rewards': None,\n",
       "  'normalize_actions': True,\n",
       "  'clip_actions': False,\n",
       "  '_is_atari': None,\n",
       "  'disable_env_checking': False,\n",
       "  'render_env': False,\n",
       "  'action_mask_key': 'action_mask',\n",
       "  'env_runner_cls': None,\n",
       "  'num_env_runners': 1,\n",
       "  'create_local_env_runner': True,\n",
       "  'num_envs_per_env_runner': 1,\n",
       "  'gym_env_vectorize_mode': 'SYNC',\n",
       "  'num_cpus_per_env_runner': 1,\n",
       "  'num_gpus_per_env_runner': 0,\n",
       "  'custom_resources_per_env_runner': {},\n",
       "  'validate_env_runners_after_construction': True,\n",
       "  'episodes_to_numpy': True,\n",
       "  'max_requests_in_flight_per_env_runner': 1,\n",
       "  'sample_timeout_s': 60.0,\n",
       "  '_env_to_module_connector': None,\n",
       "  'add_default_connectors_to_env_to_module_pipeline': True,\n",
       "  '_module_to_env_connector': None,\n",
       "  'add_default_connectors_to_module_to_env_pipeline': True,\n",
       "  'merge_env_runner_states': 'training_only',\n",
       "  'broadcast_env_runner_states': True,\n",
       "  'episode_lookback_horizon': 1,\n",
       "  'rollout_fragment_length': 'auto',\n",
       "  'batch_mode': 'truncate_episodes',\n",
       "  'compress_observations': False,\n",
       "  'remote_worker_envs': False,\n",
       "  'remote_env_batch_wait_ms': 0,\n",
       "  'enable_tf1_exec_eagerly': False,\n",
       "  'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       "  'preprocessor_pref': 'deepmind',\n",
       "  'observation_filter': 'NoFilter',\n",
       "  'update_worker_filter_stats': True,\n",
       "  'use_worker_filter_stats': True,\n",
       "  'sampler_perf_stats_ema_coef': None,\n",
       "  '_is_online': True,\n",
       "  'num_learners': 0,\n",
       "  'num_gpus_per_learner': 0,\n",
       "  'num_cpus_per_learner': 'auto',\n",
       "  'num_aggregator_actors_per_learner': 0,\n",
       "  'max_requests_in_flight_per_aggregator_actor': 3,\n",
       "  'local_gpu_idx': 0,\n",
       "  'max_requests_in_flight_per_learner': 3,\n",
       "  'gamma': 0.9,\n",
       "  'lr': None,\n",
       "  'grad_clip': None,\n",
       "  'grad_clip_by': 'global_norm',\n",
       "  '_train_batch_size_per_learner': 32,\n",
       "  'train_batch_size': 256,\n",
       "  'num_epochs': 1,\n",
       "  'minibatch_size': None,\n",
       "  'shuffle_batch_per_epoch': False,\n",
       "  'model': {'fcnet_hiddens': [256, 256],\n",
       "   'fcnet_activation': 'tanh',\n",
       "   'fcnet_weights_initializer': None,\n",
       "   'fcnet_weights_initializer_config': None,\n",
       "   'fcnet_bias_initializer': None,\n",
       "   'fcnet_bias_initializer_config': None,\n",
       "   'conv_filters': None,\n",
       "   'conv_activation': 'relu',\n",
       "   'conv_kernel_initializer': None,\n",
       "   'conv_kernel_initializer_config': None,\n",
       "   'conv_bias_initializer': None,\n",
       "   'conv_bias_initializer_config': None,\n",
       "   'conv_transpose_kernel_initializer': None,\n",
       "   'conv_transpose_kernel_initializer_config': None,\n",
       "   'conv_transpose_bias_initializer': None,\n",
       "   'conv_transpose_bias_initializer_config': None,\n",
       "   'post_fcnet_hiddens': [],\n",
       "   'post_fcnet_activation': 'relu',\n",
       "   'post_fcnet_weights_initializer': None,\n",
       "   'post_fcnet_weights_initializer_config': None,\n",
       "   'post_fcnet_bias_initializer': None,\n",
       "   'post_fcnet_bias_initializer_config': None,\n",
       "   'free_log_std': False,\n",
       "   'log_std_clip_param': 20.0,\n",
       "   'no_final_linear': False,\n",
       "   'vf_share_layers': True,\n",
       "   'use_lstm': False,\n",
       "   'max_seq_len': 20,\n",
       "   'lstm_cell_size': 256,\n",
       "   'lstm_use_prev_action': False,\n",
       "   'lstm_use_prev_reward': False,\n",
       "   'lstm_weights_initializer': None,\n",
       "   'lstm_weights_initializer_config': None,\n",
       "   'lstm_bias_initializer': None,\n",
       "   'lstm_bias_initializer_config': None,\n",
       "   '_time_major': False,\n",
       "   'use_attention': False,\n",
       "   'attention_num_transformer_units': 1,\n",
       "   'attention_dim': 64,\n",
       "   'attention_num_heads': 1,\n",
       "   'attention_head_dim': 32,\n",
       "   'attention_memory_inference': 50,\n",
       "   'attention_memory_training': 50,\n",
       "   'attention_position_wise_mlp_dim': 32,\n",
       "   'attention_init_gru_gate_bias': 2.0,\n",
       "   'attention_use_n_prev_actions': 0,\n",
       "   'attention_use_n_prev_rewards': 0,\n",
       "   'framestack': True,\n",
       "   'dim': 84,\n",
       "   'grayscale': False,\n",
       "   'zero_mean': True,\n",
       "   'custom_model': None,\n",
       "   'custom_model_config': {},\n",
       "   'custom_action_dist': None,\n",
       "   'custom_preprocessor': None,\n",
       "   'encoder_latent_dim': None,\n",
       "   'always_check_shapes': False,\n",
       "   'lstm_use_prev_action_reward': -1,\n",
       "   '_use_default_native_models': -1,\n",
       "   '_disable_preprocessor_api': False,\n",
       "   '_disable_action_flattening': False},\n",
       "  '_learner_connector': None,\n",
       "  'add_default_connectors_to_learner_pipeline': True,\n",
       "  'learner_config_dict': {},\n",
       "  'optimizer': {},\n",
       "  '_learner_class': None,\n",
       "  'callbacks_on_algorithm_init': None,\n",
       "  'callbacks_on_env_runners_recreated': None,\n",
       "  'callbacks_on_offline_eval_runners_recreated': None,\n",
       "  'callbacks_on_checkpoint_loaded': None,\n",
       "  'callbacks_on_environment_created': None,\n",
       "  'callbacks_on_episode_created': None,\n",
       "  'callbacks_on_episode_start': None,\n",
       "  'callbacks_on_episode_step': None,\n",
       "  'callbacks_on_episode_end': None,\n",
       "  'callbacks_on_evaluate_start': None,\n",
       "  'callbacks_on_evaluate_end': None,\n",
       "  'callbacks_on_evaluate_offline_start': None,\n",
       "  'callbacks_on_evaluate_offline_end': None,\n",
       "  'callbacks_on_sample_end': None,\n",
       "  'callbacks_on_train_result': None,\n",
       "  'explore': True,\n",
       "  'enable_rl_module_and_learner': True,\n",
       "  'enable_env_runner_and_connector_v2': True,\n",
       "  '_prior_exploration_config': {'type': 'StochasticSampling'},\n",
       "  'count_steps_by': 'env_steps',\n",
       "  'policy_map_capacity': 100,\n",
       "  'policy_mapping_fn': <function ray.rllib.algorithms.algorithm_config.AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN(aid, episode, worker, **kwargs)>,\n",
       "  'policies_to_train': None,\n",
       "  'policy_states_are_swappable': False,\n",
       "  'observation_fn': None,\n",
       "  'offline_data_class': None,\n",
       "  'input_read_method': 'read_parquet',\n",
       "  'input_read_method_kwargs': {},\n",
       "  'input_read_schema': {},\n",
       "  'input_read_episodes': False,\n",
       "  'input_read_sample_batches': False,\n",
       "  'input_read_batch_size': None,\n",
       "  'input_filesystem': None,\n",
       "  'input_filesystem_kwargs': {},\n",
       "  'input_compress_columns': ['obs', 'new_obs'],\n",
       "  'input_spaces_jsonable': True,\n",
       "  'materialize_data': False,\n",
       "  'materialize_mapped_data': True,\n",
       "  'map_batches_kwargs': {},\n",
       "  'iter_batches_kwargs': {},\n",
       "  'ignore_final_observation': False,\n",
       "  'prelearner_class': None,\n",
       "  'prelearner_buffer_class': None,\n",
       "  'prelearner_buffer_kwargs': {},\n",
       "  'prelearner_module_synch_period': 10,\n",
       "  'dataset_num_iters_per_learner': None,\n",
       "  'input_config': {},\n",
       "  'actions_in_input_normalized': False,\n",
       "  'postprocess_inputs': False,\n",
       "  'shuffle_buffer_size': 0,\n",
       "  'output': None,\n",
       "  'output_config': {},\n",
       "  'output_compress_columns': ['obs', 'new_obs'],\n",
       "  'output_max_file_size': 67108864,\n",
       "  'output_max_rows_per_file': None,\n",
       "  'output_write_remaining_data': False,\n",
       "  'output_write_method': 'write_parquet',\n",
       "  'output_write_method_kwargs': {},\n",
       "  'output_filesystem': None,\n",
       "  'output_filesystem_kwargs': {},\n",
       "  'output_write_episodes': True,\n",
       "  'offline_sampling': False,\n",
       "  'evaluation_interval': None,\n",
       "  'evaluation_duration': 10,\n",
       "  'evaluation_duration_unit': 'episodes',\n",
       "  'evaluation_sample_timeout_s': 120.0,\n",
       "  'evaluation_auto_duration_min_env_steps_per_sample': 100,\n",
       "  'evaluation_auto_duration_max_env_steps_per_sample': 2000,\n",
       "  'evaluation_parallel_to_training': False,\n",
       "  'evaluation_force_reset_envs_before_iteration': True,\n",
       "  'evaluation_config': None,\n",
       "  'off_policy_estimation_methods': {},\n",
       "  'ope_split_batch_by_episode': True,\n",
       "  'evaluation_num_env_runners': 0,\n",
       "  'in_evaluation': False,\n",
       "  'sync_filters_on_rollout_workers_timeout_s': 10.0,\n",
       "  'offline_evaluation_interval': None,\n",
       "  'num_offline_eval_runners': 0,\n",
       "  'offline_evaluation_type': None,\n",
       "  'offline_eval_runner_class': None,\n",
       "  'offline_loss_for_module_fn': None,\n",
       "  'offline_evaluation_duration': 1,\n",
       "  'offline_evaluation_parallel_to_training': False,\n",
       "  'offline_evaluation_timeout_s': 120.0,\n",
       "  'num_cpus_per_offline_eval_runner': 1,\n",
       "  'num_gpus_per_offline_eval_runner': 0,\n",
       "  'custom_resources_per_offline_eval_runner': {},\n",
       "  'restart_failed_offline_eval_runners': True,\n",
       "  'ignore_offline_eval_runner_failures': False,\n",
       "  'max_num_offline_eval_runner_restarts': 1000,\n",
       "  'offline_eval_runner_restore_timeout_s': 1800.0,\n",
       "  'max_requests_in_flight_per_offline_eval_runner': 1,\n",
       "  'validate_offline_eval_runners_after_construction': True,\n",
       "  'offline_eval_runner_health_probe_timeout_s': 30.0,\n",
       "  'offline_eval_rl_module_inference_only': False,\n",
       "  'broadcast_offline_eval_runner_states': False,\n",
       "  'offline_eval_batch_size_per_runner': 256,\n",
       "  'dataset_num_iters_per_eval_runner': 1,\n",
       "  'keep_per_episode_custom_metrics': False,\n",
       "  'metrics_episode_collection_timeout_s': 60.0,\n",
       "  'metrics_num_episodes_for_smoothing': 100,\n",
       "  'min_time_s_per_iteration': 1,\n",
       "  'min_train_timesteps_per_iteration': 0,\n",
       "  'min_sample_timesteps_per_iteration': 100,\n",
       "  'log_gradients': True,\n",
       "  'export_native_model_files': False,\n",
       "  'checkpoint_trainable_policies_only': False,\n",
       "  'logger_creator': None,\n",
       "  'logger_config': None,\n",
       "  'log_level': 'WARN',\n",
       "  'log_sys_usage': True,\n",
       "  'fake_sampler': False,\n",
       "  'seed': None,\n",
       "  'restart_failed_env_runners': True,\n",
       "  'ignore_env_runner_failures': False,\n",
       "  'max_num_env_runner_restarts': 1000,\n",
       "  'delay_between_env_runner_restarts_s': 60.0,\n",
       "  'restart_failed_sub_environments': False,\n",
       "  'num_consecutive_env_runner_failures_tolerance': 100,\n",
       "  'env_runner_health_probe_timeout_s': 30.0,\n",
       "  'env_runner_restore_timeout_s': 1800.0,\n",
       "  '_model_config': {},\n",
       "  '_rl_module_spec': None,\n",
       "  'algorithm_config_overrides_per_module': {},\n",
       "  '_per_module_overrides': {},\n",
       "  '_validate_config': True,\n",
       "  '_use_msgpack_checkpoints': False,\n",
       "  '_torch_grad_scaler_class': None,\n",
       "  '_torch_lr_scheduler_classes': None,\n",
       "  '_tf_policy_handles_more_than_one_loss': False,\n",
       "  '_disable_preprocessor_api': False,\n",
       "  '_disable_action_flattening': False,\n",
       "  '_disable_initialize_loss_from_dummy_batch': False,\n",
       "  '_dont_auto_sync_env_runner_states': False,\n",
       "  'env_task_fn': -1,\n",
       "  'enable_connectors': -1,\n",
       "  'simple_optimizer': False,\n",
       "  'policy_map_cache': -1,\n",
       "  'worker_cls': -1,\n",
       "  'synchronize_filters': -1,\n",
       "  'enable_async_evaluation': -1,\n",
       "  'custom_async_evaluation_function': -1,\n",
       "  '_enable_rl_module_api': -1,\n",
       "  'auto_wrap_old_gym_envs': -1,\n",
       "  'always_attach_evaluation_results': -1,\n",
       "  'replay_sequence_length': None,\n",
       "  '_disable_execution_plan_api': -1,\n",
       "  'twin_q': True,\n",
       "  'q_model_config': {'fcnet_hiddens': [256, 256],\n",
       "   'fcnet_activation': 'relu',\n",
       "   'post_fcnet_hiddens': [],\n",
       "   'post_fcnet_activation': None,\n",
       "   'custom_model': None,\n",
       "   'custom_model_config': {}},\n",
       "  'policy_model_config': {'fcnet_hiddens': [256, 256],\n",
       "   'fcnet_activation': 'relu',\n",
       "   'post_fcnet_hiddens': [],\n",
       "   'post_fcnet_activation': None,\n",
       "   'custom_model': None,\n",
       "   'custom_model_config': {}},\n",
       "  'tau': 0.005,\n",
       "  'initial_alpha': 1.0,\n",
       "  'target_entropy': 'auto',\n",
       "  'n_step': 1,\n",
       "  'replay_buffer_config': {'type': 'PrioritizedEpisodeReplayBuffer',\n",
       "   'capacity': 1000000,\n",
       "   'alpha': 0.6,\n",
       "   'beta': 0.4,\n",
       "   'metrics_num_episodes_for_smoothing': 100},\n",
       "  'store_buffer_in_checkpoints': False,\n",
       "  'training_intensity': None,\n",
       "  'optimization': {'actor_learning_rate': 0.0003,\n",
       "   'critic_learning_rate': 0.0003,\n",
       "   'entropy_learning_rate': 0.0003},\n",
       "  'actor_lr': 0.001,\n",
       "  'critic_lr': 0.002,\n",
       "  'alpha_lr': 0.0003,\n",
       "  'target_network_update_freq': 0,\n",
       "  'num_steps_sampled_before_learning_starts': 1500,\n",
       "  '_deterministic_loss': False,\n",
       "  '_use_beta_distribution': False,\n",
       "  'use_state_preprocessor': -1,\n",
       "  'worker_side_prioritization': -1,\n",
       "  'class': ray.rllib.algorithms.sac.sac.SACConfig,\n",
       "  'input': 'sampler',\n",
       "  'policies': {'default_policy': (None, None, None, None)},\n",
       "  'callbacks': ray.rllib.callbacks.callbacks.RLlibCallback,\n",
       "  'create_env_on_driver': False,\n",
       "  'custom_eval_function': None,\n",
       "  'framework': 'torch'},\n",
       " 'time_since_restore': 2.504804849624634,\n",
       " 'iterations_since_restore': 1,\n",
       " 'perf': {'cpu_util_percent': np.float64(53.833333333333336),\n",
       "  'ram_util_percent': np.float64(95.33333333333333),\n",
       "  'gpu_util_percent0': np.float64(0.38666666666666666),\n",
       "  'vram_util_percent0': np.float64(0.18562825520833334),\n",
       "  'gpu_util_percent1': np.float64(0.0),\n",
       "  'vram_util_percent1': np.float64(0.0005699397492265104)}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray.rllib.algorithms.sac.sac import SACConfig\n",
    "\n",
    "config = (\n",
    "    SACConfig()\n",
    "    .environment(\"Pendulum-v1\")\n",
    "    .env_runners(num_env_runners=1)\n",
    "    .training(\n",
    "        gamma=0.9,\n",
    "        actor_lr=0.001,\n",
    "        critic_lr=0.002,\n",
    "        train_batch_size_per_learner=32,\n",
    "    )\n",
    ")\n",
    "# Build the SAC algo object from the config and run 1 training iteration.\n",
    "algo = config.build_algo()\n",
    "algo.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddc2ad4",
   "metadata": {},
   "source": [
    "## 高吞吐量 On-Policy 和 Off-Policy\n",
    "APPO 架构：APPO 是基于 IMPALA 架构的 近端策略优化 (PPO) 的异步变体，但使用带有 clipping 的代理策略损失，允许每收集一个训练批次进行多次 SGD 遍历。在一个训练迭代中，APPO 异步地从所有 EnvRunner 请求样本，收集到的 episode 样本作为 Ray 引用返回给主算法进程，而不是本地进程上可用的实际对象。然后 APPO 将这些 episode 引用传递给 Learner 进行模型的异步更新。RLlib 在新的模型版本可用后不会总是立即将权重同步回 EnvRunner。为了考虑 EnvRunner 是离策略的，APPO 使用 IMPALA 论文中描述的 v-trace 过程。APPO 在两个方向上都能扩展，支持多个 EnvRunner 进行样本收集，以及多个基于 GPU 或 CPU 的 Learner 更新模型。\n",
    "\n",
    " class ray.rllib.algorithms.appo.appo.APPOConfig(algo_class=None) 定义一个配置类，可以从中构建 APPO 算法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c181251d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:520: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/robotarm/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "[2025-08-06 19:47:17,924 E 4032506 4032506] core_worker.cc:2740: Actor with class name: 'SingleAgentEnvRunner' and ID: 'ae7194612e0c6f719ffa607601000000' has constructor arguments in the object store and max_restarts > 0. If the arguments in the object store go out of scope or are lost, the actor restart will fail. See https://github.com/ray-project/ray/issues/53727 for more details.\n",
      "2025-08-06 19:50:42,764\tERROR actor_manager.py:873 -- Ray error (The actor ae7194612e0c6f719ffa607601000000 is unavailable: The actor is temporarily unavailable: IOError: The actor is restarting.. The task may or may not have been executed on the actor.), taking actor 1 out of service.\n",
      "2025-08-06 19:50:42,766\tERROR actor_manager.py:674 -- The actor ae7194612e0c6f719ffa607601000000 is unavailable: The actor is temporarily unavailable: IOError: The actor is restarting.. The task may or may not have been executed on the actor.\n",
      "NoneType: None\n",
      "2025-08-06 19:50:42,768\tINFO trainable.py:161 -- Trainable.setup took 204.921 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "<ray.rllib.env.single_agent_env_runner.SingleAgentEnvRunner object at 0x7df0b263b2b0> doesn't have an env! Can't call `sample()` on it.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Build an Algorithm object from the config and run 1 training iteration.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m algo \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mbuild_algo() \n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43malgo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m algo\n",
      "File \u001b[0;32m~/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/tune/trainable/trainable.py:331\u001b[0m, in \u001b[0;36mTrainable.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    330\u001b[0m     skipped \u001b[38;5;241m=\u001b[39m skip_exceptions(e)\n\u001b[0;32m--> 331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m skipped \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexception_cause\u001b[39;00m(skipped)\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mdict\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep() needs to return a dict.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;66;03m# We do not modify internal state nor update this result if duplicate.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/tune/trainable/trainable.py:328\u001b[0m, in \u001b[0;36mTrainable.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    326\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 328\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    330\u001b[0m     skipped \u001b[38;5;241m=\u001b[39m skip_exceptions(e)\n",
      "File \u001b[0;32m~/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:1035\u001b[0m, in \u001b[0;36mAlgorithm.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;66;03m# - No evaluation necessary, just run the next training iteration.\u001b[39;00m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;66;03m# - We have to evaluate in this training iteration, but no parallelism ->\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;66;03m#   evaluate after the training iteration is entirely done.\u001b[39;00m\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1034\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39menable_env_runner_and_connector_v2:\n\u001b[0;32m-> 1035\u001b[0m         train_results, train_iter_ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_one_training_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1037\u001b[0m         (\n\u001b[1;32m   1038\u001b[0m             train_results,\n\u001b[1;32m   1039\u001b[0m             train_iter_ctx,\n\u001b[1;32m   1040\u001b[0m         ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_one_training_iteration_old_api_stack()\n",
      "File \u001b[0;32m~/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:3352\u001b[0m, in \u001b[0;36mAlgorithm._run_one_training_iteration\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3350\u001b[0m \u001b[38;5;66;03m# Try to train one step.\u001b[39;00m\n\u001b[1;32m   3351\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mlog_time((TIMERS, TRAINING_STEP_TIMER)):\n\u001b[0;32m-> 3352\u001b[0m     training_step_return_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3353\u001b[0m     has_run_once \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3355\u001b[0m \u001b[38;5;66;03m# On the new API stack, results should NOT be returned anymore as\u001b[39;00m\n\u001b[1;32m   3356\u001b[0m \u001b[38;5;66;03m# a dict, but purely logged through the `MetricsLogger` API. This\u001b[39;00m\n\u001b[1;32m   3357\u001b[0m \u001b[38;5;66;03m# way, we make sure to never miss a single stats/counter/timer\u001b[39;00m\n\u001b[1;32m   3358\u001b[0m \u001b[38;5;66;03m# when calling `self.training_step()` more than once within the same\u001b[39;00m\n\u001b[1;32m   3359\u001b[0m \u001b[38;5;66;03m# iteration.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/algorithms/appo/appo.py:360\u001b[0m, in \u001b[0;36mAPPO.training_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;129m@override\u001b[39m(IMPALA)\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39menable_rl_module_and_learner:\n\u001b[0;32m--> 360\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    362\u001b[0m     train_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mtraining_step()\n\u001b[1;32m    363\u001b[0m     \u001b[38;5;66;03m# Update the target network and the KL coefficient for the APPO-loss.\u001b[39;00m\n\u001b[1;32m    364\u001b[0m     \u001b[38;5;66;03m# The target network update frequency is calculated automatically by the product\u001b[39;00m\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;66;03m# of `num_epochs` setting (usually 1 for APPO) and `minibatch_buffer_size`.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/algorithms/impala/impala.py:615\u001b[0m, in \u001b[0;36mIMPALA.training_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;66;03m# Asynchronously request all EnvRunners to sample and return their current\u001b[39;00m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;66;03m# (e.g. ConnectorV2) states and sampling metrics/stats.\u001b[39;00m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;66;03m# Note that each item in `episode_refs` is a reference to a list of Episodes.\u001b[39;00m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mlog_time((TIMERS, SAMPLE_TIMER)):\n\u001b[1;32m    610\u001b[0m     (\n\u001b[1;32m    611\u001b[0m         episode_refs,\n\u001b[1;32m    612\u001b[0m         connector_states,\n\u001b[1;32m    613\u001b[0m         env_runner_metrics,\n\u001b[1;32m    614\u001b[0m         env_runner_indices_to_update,\n\u001b[0;32m--> 615\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample_and_get_connector_states\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;66;03m# Reduce EnvRunner metrics over the n EnvRunners.\u001b[39;00m\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39maggregate(\n\u001b[1;32m    618\u001b[0m         env_runner_metrics,\n\u001b[1;32m    619\u001b[0m         key\u001b[38;5;241m=\u001b[39mENV_RUNNER_RESULTS,\n\u001b[1;32m    620\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/algorithms/impala/impala.py:824\u001b[0m, in \u001b[0;36mIMPALA._sample_and_get_connector_states\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    821\u001b[0m         env_runner_metrics\u001b[38;5;241m.\u001b[39mappend(metrics)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# Sample from the local EnvRunner.\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    825\u001b[0m     env_runner_metrics \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv_runner\u001b[38;5;241m.\u001b[39mget_metrics()]\n\u001b[1;32m    826\u001b[0m     episode_refs \u001b[38;5;241m=\u001b[39m [ray\u001b[38;5;241m.\u001b[39mput(episodes)]\n",
      "File \u001b[0;32m~/anaconda3/envs/ray_env/lib/python3.10/site-packages/ray/rllib/env/single_agent_env_runner.py:181\u001b[0m, in \u001b[0;36mSingleAgentEnvRunner.sample\u001b[0;34m(self, num_timesteps, num_episodes, explore, random_actions, force_reset)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs and returns a sample (n timesteps or m episodes) on the env(s).\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m    A list of `SingleAgentEpisode` instances, carrying the sampled data.\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 181\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    182\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have an env! Can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt call `sample()` on it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    183\u001b[0m     )\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (num_timesteps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m num_episodes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# Log time between `sample()` requests.\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: <ray.rllib.env.single_agent_env_runner.SingleAgentEnvRunner object at 0x7df0b263b2b0> doesn't have an env! Can't call `sample()` on it."
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.appo import APPOConfig\n",
    "config = (\n",
    "    APPOConfig()\n",
    "    .training(lr=0.01, grad_clip=30.0, train_batch_size_per_learner=50)\n",
    ")\n",
    "config = config.learners(num_learners=1)\n",
    "config = config.env_runners(num_env_runners=1)\n",
    "config = config.environment(\"CartPole-v1\")\n",
    "\n",
    "# Build an Algorithm object from the config and run 1 training iteration.\n",
    "algo = config.build_algo() \n",
    "print(algo.train())\n",
    "del algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ced8d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06 19:11:29,707\tWARNING tune.py:219 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2025-08-06 19:11:29,725\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/robotarm/ray_results/APPO_2025-08-06_19-11-04' in 0.0150s.\n",
      "2025-08-06 19:11:39,740\tWARNING tune.py:1056 -- Experiment has been interrupted, but the most recent state was saved.\n",
      "Resume experiment with: Tuner.restore(path=\"/home/robotarm/ray_results/APPO_2025-08-06_19-11-04\", trainable=...)\n",
      "2025-08-06 19:11:39,763\tWARNING experiment_analysis.py:180 -- Failed to fetch metrics for 1 trial(s):\n",
      "- APPO_CartPole-v1_0b1c9_00000: FileNotFoundError('Could not fetch metrics for APPO_CartPole-v1_0b1c9_00000: both result.json and progress.csv were not found at /home/robotarm/ray_results/APPO_2025-08-06_19-11-04/APPO_CartPole-v1_0b1c9_00000_0_lr=0.0010_2025-08-06_19-11-04')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResultGrid<[\n",
       "  Result(\n",
       "    metrics={},\n",
       "    path='/home/robotarm/ray_results/APPO_2025-08-06_19-11-04/APPO_CartPole-v1_0b1c9_00000_0_lr=0.0010_2025-08-06_19-11-04',\n",
       "    filesystem='local',\n",
       "    checkpoint=None\n",
       "  )\n",
       "]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray.rllib.algorithms.appo import APPOConfig\n",
    "from ray import tune\n",
    "\n",
    "config = APPOConfig()\n",
    "# Update the config object.\n",
    "config = config.training(lr=tune.grid_search([0.001,]))\n",
    "# Set the config object's env.\n",
    "config = config.environment(env=\"CartPole-v1\")\n",
    "# Use to_dict() to get the old-style python config dict when running with tune.\n",
    "tune.Tuner(\n",
    "    \"APPO\",\n",
    "    run_config=tune.RunConfig(\n",
    "        stop={\"training_iteration\": 1},\n",
    "        verbose=0,\n",
    "    ),\n",
    "    param_space=config.to_dict(),\n",
    "\n",
    ").fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd7d7df",
   "metadata": {},
   "source": [
    "## 重要性加权 Actor-Learner 架构 (IMPALA)\n",
    "IMPALA 架构：在一个训练迭代中，IMPALA 异步地向所有 EnvRunners 请求样本，并将收集到的 episode 作为 Ray 引用返回给主算法进程，而不是本地进程上可用的实际对象。然后 IMPALA 将这些 episode 引用传递给 Learners 进行模型的异步更新。当新的模型版本可用时，RLlib 并不会立即将权重同步回 EnvRunners。为了应对 EnvRunners 处于 off-policy 状态，IMPALA 使用了一种称为 v-trace 的过程，如论文中所述。IMPALA 在两个方面进行扩展，支持多个 EnvRunners 进行样本收集，以及多个基于 GPU 或 CPU 的 Learners 进行模型更新。\n",
    "\n",
    "ray.rllib.algorithms.impala.impala.IMPALAConfig(algo_class=None) 定义一个配置类，可以从中构建一个 Impala 算法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef678e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.impala import IMPALAConfig\n",
    "\n",
    "config = (\n",
    "    IMPALAConfig()\n",
    "    .environment(\"CartPole-v1\")\n",
    "    .env_runners(num_env_runners=1)\n",
    "    .training(lr=0.0003, train_batch_size_per_learner=512)\n",
    "    .learners(num_learners=1)\n",
    ")\n",
    "# Build a Algorithm object from the config and run 1 training iteration.\n",
    "algo = config.build_algo()\n",
    "algo.train()\n",
    "del algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815bcc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.impala import IMPALAConfig\n",
    "from ray import tune\n",
    "\n",
    "config = (\n",
    "    IMPALAConfig()\n",
    "    .environment(\"CartPole-v1\")\n",
    "    .env_runners(num_env_runners=1)\n",
    "    .training(lr=tune.grid_search([0.0001, 0.0002]), grad_clip=20.0)\n",
    "    .learners(num_learners=1)\n",
    ")\n",
    "# Run with tune.\n",
    "tune.Tuner(\n",
    "    \"IMPALA\",\n",
    "    param_space=config,\n",
    "    run_config=tune.RunConfig(stop={\"training_iteration\": 1}),\n",
    ").fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42d4976",
   "metadata": {},
   "source": [
    "## 基于模型的强化学习\n",
    "## DreamerV3\n",
    "\n",
    "DreamerV3 架构：DreamerV3 使用从回放缓冲区采样的真实环境交互以监督方式训练一个循环 WORLD_MODEL。世界模型的目标是正确预测 RL 环境的过渡 dynamics：下一个 observation、reward 和一个布尔值 continuation flag。DreamerV3 仅在合成轨迹上训练 actor- 和 critic-网络，这些轨迹由世界模型“梦想”出来。DreamerV3 在两个方面进行扩展，支持多个 EnvRunners 进行样本收集以及多个基于 GPU 或 CPU 的 Learners 进行模型更新。它也可以用于不同的环境类型，包括基于图像或向量的 observation、连续或离散的 actions，以及稀疏或密集的 reward functions。\n",
    "\n",
    "## 离线强化学习和模仿学习\n",
    "## 行为克隆 (BC)\n",
    "\n",
    "BC 架构：RLlib 的行为克隆 (BC) 使用 Ray Data 利用其并行数据处理能力。在一个训练迭代中，BC 由 n 个 DataWorkers 并行读取离线文件（例如 parquet）中的 episode。然后 Connector pipelines 将这些 episode 预处理成训练批次，并将这些批次作为数据迭代器直接发送给 n 个 Learners 以更新模型。RLlib 的 BC 实现直接源自其 MARWIL 实现，唯一的区别是 beta 参数（设置为 0.0）。这使得 BC 试图匹配生成离线数据的行为策略，而忽略任何由此产生的奖励。\n",
    "\n",
    "ray.rllib.algorithms.bc.bc.BCConfig(algo_class=None) 定义一个配置类，可以从中构建一个新的 BC 算法。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3784ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.bc import BCConfig\n",
    "# Run this from the ray directory root.\n",
    "config = BCConfig().training(lr=0.00001, gamma=0.99)\n",
    "config = config.offline_data(\n",
    "    input_=\"./rllib/tests/data/cartpole/large.json\")\n",
    "\n",
    "# Build an Algorithm object from the config and run 1 training iteration.\n",
    "algo = config.build()\n",
    "algo.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1deb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.bc import BCConfig\n",
    "from ray import tune\n",
    "config = BCConfig()\n",
    "# Print out some default values.\n",
    "print(config.beta)\n",
    "# Update the config object.\n",
    "config.training(\n",
    "    lr=tune.grid_search([0.001, 0.0001]), beta=0.75\n",
    ")\n",
    "# Set the config object's data path.\n",
    "# Run this from the ray directory root.\n",
    "config.offline_data(\n",
    "    input_=\"./rllib/tests/data/cartpole/large.json\"\n",
    ")\n",
    "# Set the config object's env, used for evaluation.\n",
    "config.environment(env=\"CartPole-v1\")\n",
    "# Use to_dict() to get the old-style python config dict\n",
    "# when running with tune.\n",
    "tune.Tuner(\n",
    "    \"BC\",\n",
    "    param_space=config.to_dict(),\n",
    ").fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a0033a",
   "metadata": {},
   "source": [
    "## 保守 Q 学习 (CQL)\n",
    "CQL 架构：CQL (保守 Q 学习) 是一种离线强化学习算法，它通过保守的 critic 估计来减轻数据集分布外部 Q 值的过高估计。它在标准的 Bellman 更新损失中添加了一个简单的 Q 正则化损失，确保 critic 不会输出过于乐观的 Q 值。SACLearner 将此保守修正项添加到基于 TD 的 Q 学习损失中。\n",
    "\n",
    "ray.rllib.algorithms.cql.cql.CQLConfig(algo_class=None) 定义一个配置类，可以从中构建一个 CQL 算法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d1c6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.cql import CQLConfig\n",
    "config = CQLConfig().training(gamma=0.9, lr=0.01)\n",
    "config = config.resources(num_gpus=0)\n",
    "config = config.env_runners(num_env_runners=4)\n",
    "print(config.to_dict())\n",
    "# Build a Algorithm object from the config and run 1 training iteration.\n",
    "algo = config.build_algo(env=\"CartPole-v1\")\n",
    "algo.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f1f6ea",
   "metadata": {},
   "source": [
    "## 单调优势重加权模仿学习 (MARWIL)\n",
    "MARWIL 架构：MARWIL 是一种混合模仿学习和策略梯度算法，适用于在批处理的历史数据上进行训练。当 beta 超参数设置为零时，MARWIL objective 退化为简单的模仿学习（参见 BC）。MARWIL 使用 Ray.Data 利用其并行数据处理能力。在一个训练迭代中，MARWIL 由 n 个 DataWorkers 并行读取离线文件（例如 parquet）中的 episode。Connector pipelines 将这些 episode 预处理成训练批次，并将这些批次作为数据迭代器直接发送给 n 个 Learners 以更新模型。\n",
    "\n",
    "ray.rllib.algorithms.marwil.marwil.MARWILConfig(algo_class=None) 定义一个配置类，可以从中构建一个 MARWIL 算法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e11781ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgorithms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmarwil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MARWILConfig\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Get the base path (to ray/rllib)\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m base_path \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;18;43m__file__\u001b[39;49m)\u001b[38;5;241m.\u001b[39mparents[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Get the path to the data in rllib folder.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m data_path \u001b[38;5;241m=\u001b[39m base_path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtests/data/cartpole/cartpole-v1_large\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from ray.rllib.algorithms.marwil import MARWILConfig\n",
    "\n",
    "# Get the base path (to ray/rllib)\n",
    "base_path = Path(__file__).parents[2]\n",
    "# Get the path to the data in rllib folder.\n",
    "data_path = base_path / \"tests/data/cartpole/cartpole-v1_large\"\n",
    "\n",
    "config = MARWILConfig()\n",
    "# Enable the new API stack.\n",
    "config.api_stack(\n",
    "    enable_rl_module_and_learner=True,\n",
    "    enable_env_runner_and_connector_v2=True,\n",
    ")\n",
    "# Define the environment for which to learn a policy\n",
    "# from offline data.\n",
    "config.environment(\n",
    "    observation_space=gym.spaces.Box(\n",
    "        np.array([-4.8, -np.inf, -0.41887903, -np.inf]),\n",
    "        np.array([4.8, np.inf, 0.41887903, np.inf]),\n",
    "        shape=(4,),\n",
    "        dtype=np.float32,\n",
    "    ),\n",
    "    action_space=gym.spaces.Discrete(2),\n",
    ")\n",
    "# Set the training parameters.\n",
    "config.training(\n",
    "    beta=1.0,\n",
    "    lr=1e-5,\n",
    "    gamma=0.99,\n",
    "    # We must define a train batch size for each\n",
    "    # learner (here 1 local learner).\n",
    "    train_batch_size_per_learner=2000,\n",
    ")\n",
    "# Define the data source for offline data.\n",
    "config.offline_data(\n",
    "    input_=[data_path.as_posix()],\n",
    "    # Run exactly one update per training iteration.\n",
    "    dataset_num_iters_per_learner=1,\n",
    ")\n",
    "\n",
    "# Build an `Algorithm` object from the config and run 1 training\n",
    "# iteration.\n",
    "algo = config.build_algo()\n",
    "algo.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bbf00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from ray.rllib.algorithms.marwil import MARWILConfig\n",
    "from ray import tune\n",
    "\n",
    "# Get the base path (to ray/rllib)\n",
    "base_path = Path(__file__).parents[2]\n",
    "# Get the path to the data in rllib folder.\n",
    "data_path = base_path / \"tests/data/cartpole/cartpole-v1_large\"\n",
    "\n",
    "config = MARWILConfig()\n",
    "# Enable the new API stack.\n",
    "config.api_stack(\n",
    "    enable_rl_module_and_learner=True,\n",
    "    enable_env_runner_and_connector_v2=True,\n",
    ")\n",
    "# Print out some default values\n",
    "print(f\"beta: {config.beta}\")\n",
    "# Update the config object.\n",
    "config.training(\n",
    "    lr=tune.grid_search([1e-3, 1e-4]),\n",
    "    beta=0.75,\n",
    "    # We must define a train batch size for each\n",
    "    # learner (here 1 local learner).\n",
    "    train_batch_size_per_learner=2000,\n",
    ")\n",
    "# Set the config's data path.\n",
    "config.offline_data(\n",
    "    input_=[data_path.as_posix()],\n",
    "    # Set the number of updates to be run per learner\n",
    "    # per training step.\n",
    "    dataset_num_iters_per_learner=1,\n",
    ")\n",
    "# Set the config's environment for evalaution.\n",
    "config.environment(\n",
    "    observation_space=gym.spaces.Box(\n",
    "        np.array([-4.8, -np.inf, -0.41887903, -np.inf]),\n",
    "        np.array([4.8, np.inf, 0.41887903, np.inf]),\n",
    "        shape=(4,),\n",
    "        dtype=np.float32,\n",
    "    ),\n",
    "    action_space=gym.spaces.Discrete(2),\n",
    ")\n",
    "# Set up a tuner to run the experiment.\n",
    "tuner = tune.Tuner(\n",
    "    \"MARWIL\",\n",
    "    param_space=config,\n",
    "    run_config=tune.RunConfig(\n",
    "        stop={\"training_iteration\": 1},\n",
    "    ),\n",
    ")\n",
    "# Run the experiment.\n",
    "tuner.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ray_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
